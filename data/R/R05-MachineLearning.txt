R与机器学习


========================================
常用R脚本
----------------------------------------
GC含量统计：http://www.biotrainee.com/forum.php?mod=viewthread&tid=625&page=2#pid1904






========================================
ROC曲线的意义
----------------------------------------
ROC曲线的意义
http://www.cnblogs.com/emanlee/archive/2011/05/29/2062280.html

　　ROC曲线指受试者工作特征曲线(receiver operating characteristic curve), 是反映敏感性和特异性连续变量的综合指标,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界值，从而计算出一系列敏感性和特异性，再以敏感性为纵坐标、（1-特异性）为横坐标绘制成曲线，曲线下面积越大，诊断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为敏感性和特异性均较高的临界值. 　　
SPSS统计软件包的10.0版本有ROC曲线的统计功能。ROC曲线真阳性率为纵坐标,假阳性率为横坐标,在座标上由无数个临界值求出的无数对真阳性率和假阳性率作图构成,计算ROC曲线下面积AUCROC来评价诊断效率。


========================================
|-- 画ROC曲线(ROCR包和pROC包)
----------------------------------------
AUC为ROC曲线下方的面积。一般AUC大于0.75就能够说明模型是比较合理的了。


1.
(1)library(ROCR)
data(ROCR.simple)
pred <- prediction(ROCR.simple$predictions, ROCR.simple$labels)
perf <- performance(pred,"tpr","fpr")
str(perf)

##AUC值,ROC曲线下面积为AUC，用来评价分类器的综合性能，该数值取0-1之间，越大越好。
#https://blog.csdn.net/Hellolijunshy/article/details/79991385
auc <- performance(pred,'auc');auc_value=auc@y.values[[1]]
auc_value=round(auc_value,2)
auc_value #0.84

plot(perf,colorize=TRUE,main=paste0('AUC=',auc_value))
abline(a=c(0,0),b=c(1,1),col="grey")


(2)#用ggplot2画图
plotdata=data.frame(
  x=unlist(perf@x.values),
  y=unlist(perf@y.values)
)
g=ggplot(plotdata, aes(x,y, color=x))+
  geom_path(size=1)+
  labs(x="False positive rate",y="True positive rate",title="ROC Curve")+
  scale_color_gradient(name="False positive rate", low="blue", high="red")+
  theme(plot.title=element_text(face="bold",size=10))
g
#



2.
library(pROC)
roc_curve=roc(pre_test$Y~probability)
str(roc_curve)
plot(1-roc_curve$specificities, roc_curve$sensitivities,type="o",
     xlab="1-specificities",ylab="sensitivities",main="ROC Curve")
abline(a=0,b=1,col="gray")
auc2=roc_curve$auc
text(0.5,0.4,paste("AUC: ", round(auc2, digits=2)), col="blue")
#



========================================
R语言画生存期km plot
----------------------------------------
1.生存率：
    RFS: Relapse Free Survival; 无复发生存期
    OS: Overall Survival; 总生存期
    DMFS: Distant Metastasis Free Survival; 无远处转移生存期
    PPS: Post Progression Survival 进展后生存期

2.芯片数据该怎么做标准化：系统原理：两次标准化
第一次使用affy包MAS5算法标准化；第二次[文献14]把平均值调整为1000，消除批次效应。

1).After an initial quality control, redundant samples (n = 384) were excluded [12]. The raw CEL files were MAS5 normalized in the R statistical environment (www.r-project.org) using the affy Bioconductor library [13]. MAS5 can be applied to individual chips, making future extensions of the database uncomplicated.
2).At this stage, we performed a second scaling normalization to set the average expression on each chip to 1,000 to avoid batch effects [14].

13). Gautier L, Cope L, Bolstad BM et al (2004) affy—analysis of Affymetrix GeneChip data at the probe level. Bioinformatics 20:307–315
14). Sims AH, Smethurst GJ, Hey Y et al (2008) The removal of multiplicative, systematic bias allows integration of breast cancer gene expression datasets—improving meta-analysis and prediction of prognosis. BMC Med Genomics 1:42

探针序列：http://kmplot.com/analysis/index.php?p=download


3.使用网站画mRNA、miRNA画十年生存期的KM plot
http://kmplot.com/analysis/





4.R语言画km plot:
共三列数据有效：
# tx是分组
# status是是否删失（0：右删失，1：死亡）
# time表示存活时间

## 生存期曲线的画法
#原始数据 
# case	inst	tx	grade	cond	site	t.stage	n.stage	entry.dt	status	time
# 1	a1	1	1	1	1	1	1	1	1	1
# 2	a1	1	1	1	1	1	1	1	1	100
# 3	a1	1	1	1	1	1	1	1	1	12
# 4	a1	1	1	1	1	1	1	1	1	12
# 5	a1	1	1	1	1	1	1	1	1	13
# 6	a1	1	1	1	1	1	1	1	1	13
# 7	a1	1	1	1	1	1	1	1	1	13
# 8	a1	1	1	1	1	1	1	1	1	20
# 9	a1	1	1	1	1	1	1	1	1	30
# 10	a1	2	1	1	1	1	1	1	1	31
# 11	a1	2	1	1	1	1	1	1	1	32
# 12	a1	2	1	1	1	1	1	1	1	33
# 13	a1	2	1	1	1	1	1	1	1	34
# 14	a1	2	1	1	1	1	1	1	1	35
# 15	a1	2	1	1	1	1	1	1	1	35
# 16	a1	2	1	1	1	1	1	1	1	35
# 17	a1	2	1	1	1	1	1	1	1	40
# 18	a1	2	1	1	1	1	1	1	1	40
# 19	a1	2	1	1	1	1	1	1	1	60
# 20	a1	2	1	1	1	1	1	1	1	80
# 其中
# caese表示编号
# tx是分组
# status是是否删失（0：右删失，1：死亡）
# time表示存活时间

# 载入包
library('survival')

# 读取数据
setwd('D:/R_code')
my=read.csv('death.csv',header=T);my

# 按照tx分组对time和statuss拟合生存曲线，
sd=survfit(Surv(time,as.numeric(status))~tx,data=my,se.fit=FALSE, conf.int=.95)

# 画出生存曲线
plot(sd,lty=1,col=c("red","purple"),
      xlab="time(year)",ylab="Overall Survival",main="survival", #生存曲线
    ) 
#加上图例
legend(60, 1, c("Group1", "Group2"), col = c("red","purple"),
       text.col = c("red","purple"), lty = c(1, 1), pch = c(1, 2),
       merge = TRUE, bg ="#efeeef" );
#rgb(0.99,0.99,0.99) grey


# 这个没懂
with(lung, Surv(time, status))
Surv(heart$start, heart$stop, heart$event)

作图结果如下：





4.2.怎么计算P值（log rank p-value）呢？[解决]
[1]http://bcb.dfci.harvard.edu/~aedin/courses/Bioconductor/survival.pdf
[2]https://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_survival_analysis.pdf   Survival Analysis:
在pdf中搜索log-rank 等关键词。
[3]http://stats.stackexchange.com/questions/114304/log-rank-test-in-r


#p值怎么算？

# http://bbs.pinggu.org/thread-2178930-1-1.html


#请使用survdiff函数做log rank检验或建立Cox模型(coxph)来比较两条生存曲线,

#KM曲线只是一种可视化手段，不是正经的统计推断分析工具。

# 应该用 suvdiff 函数做 log-rank test

#
# 读取数据
setwd('D:/R_code/')
my=read.csv('suvive.csv',header=T);my

#############################
#计算p值：
#http://stats.stackexchange.com/questions/114304/log-rank-test-in-r
# install.packages("survival")
library("survival")
sdf=survdiff(Surv(time, as.numeric(status))~tx, data=my)
sdf
pvalue=1-pchisq(sdf$chisq, df=1)
#pvalue=round(pvalue,2)
# There is also an option for ‘rho’. Rho = 0 (default)
# gives the log-rank test, rho=1 gives the Wilcoxon test.

#install.packages("coin")
#library("coin")
#st=logrank_test(Surv(time, as.numeric(status)) ~ tx, data=my, distribution = "exact")
#st
#########################


# 按照tx分组对time和statuss拟合生存曲线，
km=survfit(Surv(time,as.numeric(status))~tx,data=my,se.fit=FALSE, conf.int=.95)
km

# 画出生存曲线
plot(km,lty=1,col=c("red","purple"), 
     xlab="OS MONTHS",ylab="Percent Survival",main="survival", #生存曲线
) 
#加上图例
legend(130, 1,  legend=c(paste("WT(n=",km$n[1],")"), paste("R132H(n=",km$n[2],")")), col = c("red","purple"),
       text.col = c("red","purple"), lty = c(1, 1), title = paste("logrank p = ",pvalue) )
#       merge = FALSE, bg ="#ffffff"





4.3.怎么标出来截尾数据？

http://www.biostatistic.net/thread-87214-1-1.html
mark.time=T  生存分析显示截尾数据点.

## S3 method for class 'survfit'
plot(x, conf.int=, mark.time=TRUE, 
mark=3, col=1, lty=1, lwd=1, cex=1, log=FALSE, xscale=1, yscale=1,  
firstx=0, firsty=1, xmax, ymin=0, fun, 
xlab="", ylab="", xaxs="S", ...)






4.4.从TCGA下载数据，怎么匹配样品名和生存时间？[半解决]
http://www.cbioportal.org/
（1）用excel的vookup函数[勉强可用，但是还是不自动化]：
VLOOKUP函数是Excel中的一个纵向查找函数，它与LOOKUP函数和HLOOKUP函数属于一类函数，在工作中都有广泛应用。VLOOKUP是按列查找，最终返回该列所需查询列序所对应的值；与之对应的HLOOKUP是按行查找的。
VLOOKUP(lookup_value,table_array,col_index_num,range_lookup)
当前页面样品name，寻找的表格范围，返回的列编号，不精确查找吗？
大概是这样： =VLOOKUP(A2,Sheet2!A:F,5,FALSE)

（2）使用R语言的sql模块【推荐】
...




4.5.怎么从marker选择最合适的cutoff值？
看文献是使用x-tile软件选择最优的cutoff值。
软件：http://medicine.yale.edu/lab/rimm/research/software.aspx
教程：http://wenku.baidu.com/link?url=GmVqIKZ41r1av7wzYDxZYquKk8Am6MuLyq5AD0NGicIa9t5bUvgokQqpgTGCHpIS-Oh-Vd24j1-wJpjY1b25UKXrpPLmZ2W9tlzUrJPqqV7

也有人说用ROC曲线的。





4.6 问题

How to calculate the HR and 95%CI using the log-rank test in R?
怎么用R的log-rank检验分析HR和95%置信区间

The R survival package is very useful to do survival analysis. And I know the survdiff function can be used to compare the difference of survival time in two or more groups. And the p-value number can also be calculated as below. However, how can I calculate the HR and 95% CI using the log-rank test.
R的生存期包对生存分析很有用。我知道survdiff函数可以被用于比较2组或多组生存时间差异。p值可以如下方法计算。然而，我不知道怎么使用log-rank检验计算HR和95%置信区间(CI)。

And I also know I can use the coxph() function to calculate the HR and 95% CI using the Cox regression. However, as the assumption of both the Cox model and log-rank test are that the hazard ratio stay constant over time, so I think I can also calculate the HR and 95% CI using the log-rank test.
我知道怎么使用Cox回归的coxph()函数计算HR和95%CI。然而，Cox模型和log-rank检验的假设都是风险比随着时间稳定，所以我认为我可以使用log-rank检验计算 HR and 95% CI。

According to the book Survival Analysis: A Practical Approach, I got two formulas on Page 62 and 66 to do this (as shown below). So I wrote the R code as below, is there anybody know whether I'm right?
按照《生存分析：实用方法》一书，我在P62和P66看到2个做这个的函数（如下）。所以，我写了如下R代码，有人知道我这么写对吗？

library(survival)
data.survdiff <- survdiff(Surv(time, status) ~ group)
p.val = 1 - pchisq(data.survdiff$chisq, length(data.survdiff$n) - 1)
HR = (data.survdiff$obs[2]/data.survdiff$exp[2])/(data.survdiff$obs[1]/data.survdiff$exp[1])
up95 = exp(log(HR) + qnorm(0.975)*sqrt(1/data.survdiff$exp[2]+1/data.survdiff$exp[1]))
low95 = exp(log(HR) - qnorm(0.975)*sqrt(1/data.survdiff$exp[2]+1/data.survdiff$exp[1]))


输出：
> data.survdiff
Call:
survdiff(formula = Surv(data[, "os_whw"], data[, "status_whw"] == 
    1) ~ data[, "pcascore"] >= median(data[, "pcascore"]))

                                                       N Observed Expected (O-E)^2/E (O-E)^2/V
data[, "pcascore"] >= median(data[, "pcascore"])=FALSE 4        3     4.33     0.411     0.974
data[, "pcascore"] >= median(data[, "pcascore"])=TRUE  5        5     3.67     0.486     0.974

 Chisq= 1  on 1 degrees of freedom, p= 0.324 
> p.val
[1] 0.3235935
> HR
[1] 1.970484
> up95
[1] 7.917248
> low95
[1] 0.4904239





4.6.1 回答1
#############
如果你的问题是两组生存是否有差异，不管其他特性，你应该使用KM估计并获得P值。这是用log-rank检验获得的。仅仅考虑组间生存期差异。如果你（1）分组时随机的（2）仅仅想知道组间生存时间差异（3）想看生存曲线，则这个方法很合适。

然而，如果分组不是随机的而是基本特性差异很大（比如年龄、性别差异），简单的生存分析用处就不大了（比如可能生存差异是年龄因素造成的）；这就是需要引入Cox回归。你使用Cox回归比较生存同时调整混杂因素。如果Cox回归没有包含“分组”之外的预测因素，得到的p值和KM 估计中的log rank检验结果是一样的。但是Cox回归能让你获得每个预测因素的HR和CI，以及p值。

使用Sruvival包中的coxph函数，或者rms包中的cph函数。

通常不用为此纠结。使用Rms包中的函数就很好，还有一本很棒的与包同名的书(Springer; FE Harrell)。包中有可视化和表格化的函数。

可能你有些理论我不理解，但是这时有一个专家会指导我们。其他场景中，你会感觉这个途径很好。

The package: http://cran.r-project.org/web/packages/rms/rms.pdf

The book: http://www.springer.com/mathematics/probability/book/978-0-387-95232-1

(Btw: you can obtain Cox adjusted survival curves; 
google it or check out RMS package documentation).





4.6.2 回答2

你的实现看样子是对的，你认为log-rank可以用于计算HR和95%CI，但我有点问题：
1.你重新计算p值有特殊的理由吗？（因为survdiff()函数已经提供了，重算就像是额外工作）
2.你函数中的HR和95%，和使用coxph()函数计算出来的一致吗？使用coxph()函数验证你的实现。

注意：coxph()函数已经计算过，你为什么用log-rank检验又算了一次HR和95%CI？

I agree with your idea that log-rank test is used to compare the survival of two groups.






refer
1.[推荐]R的生存期包
https://cran.r-project.org/web/packages/survival/survival.pdf
2. http://stats.stackexchange.com/questions/124489/how-to-calculate-the-hr-and-95ci-using-the-log-rank-test-in-r

3.Use Software R to do Survival Analysis and Simulation. A tutorial
http://www.ms.uky.edu/~mai/Rsurv.pdf

10.英文图书 https://www.amazon.com/dp/0470870400/?tag=stackoverfl08-20

R语言的with和by http://statmethods.net/stats/withby.html

生存分析完整论述：https://cran.r-project.org/web/views/Survival.html  太复杂了



http://wenku.baidu.com/link?url=Ld9FAX6pFUfLet5YtTToTtyzhQ2GA2mD9IkmhBqHjjDEb3e3S2BAI8C3hxZ3UxhGKUhWD4aT_uh8rWIJmEvY0fFJtdmU1VWwQzlF9LolNU7
如何用graphpad制作生存曲线？ http://www.dxy.cn/bbs/topic/25619320
http://blog.csdn.net/shmilyringpull/article/details/17529637










========================================
R语言k折交叉验证 k fold C.V.
----------------------------------------
# 原理分析 https://blog.csdn.net/Tanya_girl/article/details/50221797

library(caret)
#caret包中有createFolds函数，如
training=iris
set.seed(7)
folds<-createFolds(y=training$Species,k=10)
#这里参数y是训练数据集label,k是几折交叉验证
#我们可以看看这里输出是什么
folds
#可以看出来folds有10个元素

#可见folds里每个元素是原来label长度的十分之一，存的是label的id随机的十分之一
#可以写个for循环进行交叉验证
for(i in 1:10){
  train_cv<-training[-folds[[i]],]
  test_cv<-training[folds[[i]],]
  
  print(i)
  print(train_cv)
  print(test_cv)
}


# 完整代码这里：https://blog.csdn.net/tiaaaaa/article/details/58116346




========================================
|-- 用R对数据进行随机抽样
----------------------------------------
#去掉一类，变成两类
x=iris[which(iris$Species!="virginica"),];
x$Species=factor(x$Species)
str(x);dim(x)

#生成抽样序列
set.seed(100)
ind=sample(2,nrow(x),replace=TRUE,prob=c(0.8,0.2));ind

#取80%作为训练集，20为测试集
x_train=x[ind==1,];dim(x_train)
x_test=x[ind==2,];dim(x_test) #去掉标签





========================================
SVM 分类
----------------------------------------







========================================
SVR: 支持向量回归
----------------------------------------






========================================
主成分分析 (PCA, principal component analysis)
----------------------------------------
主成分分析 (PCA, principal component analysis)是一种数学降维方法, 利用正交变换 (orthogonal transformation)把一系列可能线性相关的变量转换为一组线性不相关的新变量，也称为主成分，从而利用新变量在更小的维度下展示数据的特征。


主成分是原有变量的线性组合，其数目不多于原始变量。组合之后，相当于我们获得了一批新的观测数据，这些数据的含义不同于原有数据，但包含了之前数据的大部分特征，并且有着较低的维度，便于进一步的分析。

在空间上，PCA可以理解为把原始数据投射到一个新的坐标系统，第一主成分为第一坐标轴，它的含义代表了原始数据中多个变量经过某种变换得到的新变量的变化区间；第二成分为第二坐标轴，代表了原始数据中多个变量经过某种变换得到的第二个新变量的变化区间。这样我们把利用原始数据解释样品的差异转变为利用新变量解释样品的差异。

这种投射方式会有很多，为了最大限度保留对原始数据的解释，一般会用最大方差理论或最小损失理论，使得第一主成分有着最大的方差或变异数 (就是说其能尽量多的解释原始数据的差异)；随后的每一个主成分都与前面的主成分正交，且有着仅次于前一主成分的最大方差 (正交简单的理解就是两个主成分空间夹角为90°，两者之间无线性关联，从而完成去冗余操作)。


1. 主成分分析的意义
(1) 简化运算。
假如某些基因如持家基因在所有样本中表达都一样，它们对于解释样本的差异也没有意义。

应用PCA就可以在尽量多的保持变量所包含的信息又能维持尽量少的变量数目，帮助简化运算和结果解释。


(2)去除数据噪音。

比如说我们在样品的制备过程中，由于不完全一致的操作，导致样品的状态有细微的改变，从而造成一些持家基因也发生了相应的变化，但变化幅度远小于核心基因 (一般认为噪音的方差小于信息的方差）。而PCA在降维的过程中滤去了这些变化幅度较小的噪音变化，增大了数据的信噪比。

(3)利用散点图实现多维数据可视化。

在上面的表达谱分析中，假如我们有1个基因，可以在线性层面对样本进行分类；如果我们有2个基因，可以在一个平面对样本进行分类；如果我们有3个基因，可以在一个立体空间对样本进行分类；如果有更多的基因，比如说n个，那么每个样品就是n维空间的一个点，则很难在图形上展示样品的分类关系。利用PCA分析，我们可以选取贡献最大的2个或3个主成分作为数据代表用以可视化。这比直接选取三个表达变化最大的基因更能反映样品之间的差异。（利用Pearson相关系数对样品进行聚类在样品数目比较少时是一个解决办法）


(4) 发现隐性相关变量。

我们在合并冗余原始变量得到主成分过程中，会发现某些原始变量对同一主成分有着相似的贡献，也就是说这些变量之间存在着某种相关性，为相关变量。同时也可以获得这些变量对主成分的贡献程度。对基因表达数据可以理解为发现了存在协同或拮抗关系的基因。






2. PCA 实现原理 及注意事项
(1)
PCA分析不是简单的选取2-3个变化最大的基因，而是先把原始的变量做一个评估，计算各个变量各自的变异度（方差）和两两变量的相关度（协方差），得到一个协方差矩阵。

在这个协方差矩阵中，对角线的值为每个变量的方差，其他值为每两个变量的协方差。随后对原变量的协方差矩阵对角化处理，即求解其特征值和特征向量。原变量与特征向量的乘积（对原始变量的线性组合）即为新变量（回顾下线性代数中的矩阵乘法）；新变量的协方差矩阵为对角协方差矩阵且对角线上的方差由大到小排列；然后从新变量中选择信息量最丰富的也就是方差最大的前2个或3个新变量，也即是主成分用于可视化。


(2) 不同预处理对PCA结果的影响
1) 不同标准化的本质在于研究者认为，是数值的量度本身重要，还是数值的变化重要。
2) 数值的量度重要，则选择原始数据或log转换；
3) 数值的变化重要，则选择scale。




3. PCA注意事项

(1) 一般说来，在PCA之前原始数据需要中心化（centering，数值减去平均值）。中心化的方法很多，除了平均值中心化（mean-centering）外，还包括其它更稳健的方法，比如中位数中心化等。

(2)除了中心化以外，定标 (Scale, 数值除以标准差) 也是数据前处理中需要考虑的一点。如果数据没有定标，则原始数据中方差大的变量对主成分的贡献会很大。数据的方差与其量级成指数关系，比如一组数据(1,2,3,4)的方差是1.67，而(10,20,30,40)的方差就是167,数据变大10倍，方差放大了100倍。

(3)但是定标(scale)可能会有一些负面效果，因为定标后变量之间的权重就是变得相同。如果我们的变量中有噪音的话，我们就在无形中把噪音和信息的权重变得相同，但PCA本身无法区分信号和噪音。在这样的情形下，我们就不必做定标。

(4)一般而言，对于度量单位不同的指标或是取值范围彼此差异非常大的指标不直接由其协方差矩阵出发进行主成分分析，而应该考虑对数据的标准化。比如度量单位不同，有万人、万吨、万元、亿元，而数据间的差异性也非常大，小则几十大则几万，因此在用协方差矩阵求解主成分时存在协方差矩阵中数据的差异性很大。

在后面提取主成分时发现，只提取了一个主成分，而此时并不能将所有的变量都解释到，这就没有真正起到降维的作用。此时就需要对数据进行定标(scale)，这样提取的主成分可以覆盖更多的变量，这就实现主成分分析的最终目的。

但是对原始数据进行标准化后更倾向于使得各个指标的作用在主成分分析构成中相等。对于数据取值范围不大或是度量单位相同的指标进行标准化处理后，其主成分分析的结果与仍由协方差矩阵出发求得的结果有较大区别。这是因为对数据标准化的过程实际上就是抹杀原有变量离散程度差异的过程，标准化后方差均为1，而实际上方差是对数据信息的重要概括形式，也就是说，对原始数据进行标准化后抹杀了一部分重要信息，因此才使得标准化后各变量在主成分构成中的作用趋于相等。因此，对同度量或是取值范围在同量级的数据还是直接使用非定标数据求解主成分为宜。


(5)中心化和定标都会受数据中离群值（outliers）或者数据不均匀（比如数据被分为若干个小组）的影响，应该用更稳健的中心化和定标方法。


(6)PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。








3. PCA 分析和可视化 示例

########### step1 PCA
library(caret)

# By default, prcomp will centralized the data using mean.
# Normalize data for PCA by dividing each data by column standard deviation.
# Often, we would normalize data.
# Only when we care about the real number changes other than the trends,
# `scale` can be set to TRUE. 
# We will show the differences of scaling and un-scaling effects.

iris.pca=prcomp(iris[,1:4])
iris.pca2=prcomp(iris[,1:4], scale=T)

iris.pca
#Standard deviations (1, .., p=4):
#[1] 2.0562689 0.4926162 0.2796596 0.1543862
#Rotation (n x k) = (4 x 4):
#                 PC1         PC2         PC3        PC4
#Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872
#Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231
#Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390
#Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574

print(str(iris.pca))





########### step2 PCA结果提取和可视化神器

library(factoextra)

#1. 碎石图展示每个主成分的贡献
## 如果需要保持，去掉下面第1,3行的注释
#pdf("1.pdf")
fviz_eig(iris.pca, addlabels = TRUE)
#dev.off()

# 2. PCA样品聚类信息展示
# repel=T，自动调整文本位置
fviz_pca_ind(iris.pca, repel=T) +coord_fixed(1) # 关键的增加


# 3. 根据样品分组上色
fviz_pca_ind( iris.pca, col.ind=iris[,5], 
             mean.point=F, addEllipses = T, legend.title="Groups")

# 4. 增加不同属性的椭圆
# “convex”: plot convex hull of a set o points.
# “confidence”: plot confidence ellipses around group mean points as 
#     the function coord.ellipse() [in FactoMineR].
# “t”: assumes a multivariate t-distribution.
# “norm”: assumes a multivariate normal distribution.
# “euclid”: draws a circle with the radius equal to level, 
#    representing the euclidean distance from the center. 
#    This ellipse probably won’t appear circular unless coord_fixed() is applied.

# 根据分组上色并绘制95%置信区间
fviz_pca_ind(iris.pca, col.ind=iris[,5], 
             mean.point=F, addEllipses = T, legend.title="Groups", 
             ellipse.type="confidence", ellipse.level=0.95)
#

#5. 展示贡献最大的变量 (基因)
# 1） 展示与主坐标轴的相关性大于0.99的变量 (具体数字自己调整)
# Visualize variable with cos2 >= 0.99
fviz_pca_var(iris.pca, select.var = list(cos2 = 0.60), 
             repel=T, 
             col.var = "cos2", 
             geom.var = c("arrow", "text") )

#2)展示与主坐标轴最相关的10个变量
# Top 10 active variables with the highest cos2
fviz_pca_var(iris.pca, select.var= list(cos2 = 10), repel=T, col.var = "contrib")

# 3）展示自己关心的变量（基因）与主坐标轴的相关性分布
# Select by names
# 这里选择的是MAD值最大的几个基因
#name <- list(name = c("FN1", "DCN", "CEMIP","CCDC80","IGFBP5","COL1A1","GREM1"))
#fviz_pca_var(pca, select.var = name)


#6. biPLot同时展示样本分组和关键基因
# top 5 contributing individuals and variable
fviz_pca_biplot(iris.pca,
                fill.ind=iris[,5],
                palette="joo",
                addEllipses = T, 
                ellipse.type="confidence",
                ellipse.level=0.95,
                mean.point=F,
                col.var="contrib",
                gradient.cols = "RdYlBu",
                select.var = list(contrib = 10),
                ggtheme = theme_minimal())
#




========================================
|-- PCA 分析与可视化 示例
----------------------------------------

1. 精简版
################
#PCA对列进行重组，重组成最多和原来一样多个PC成分。所以输入df竖着是基因，一行一个细胞。
test2=iris[,1:4] 
test=t(apply(test2, 1, function(x){x/sum(x)})) #按行标准化
test=as.data.frame(test)
head(test)

#做主成分分析
test.pr<-prcomp(test, center = F) 
test.pr
# 查看荷载
# 如果变量列数多于观察行数，则需要使用Q-PCA类型，函数时prcomp(), 载荷 test.pr$rotation
##  'princomp' can only be used with more units than variables

test.pr$rotation #PC1=-0.77sL-0.42sW-0.44pL-0.13pW, 后面以此类推
#princomp(test,cor=TRUE)时
#summary(test.pr,loadings=TRUE) 
#loading是逻辑变量，当 loading=TRUE 时表示显示 loading 的内容

biplot(test.pr)  #画出数据关于主成分的散点图和原坐标在主成分下的方向

#碎石图，显示每个PC占了多少百分比
screeplot(test.pr,type="lines")
#每个PC占了多少比例，另一种展示
library(factoextra)
fviz_eig(test.pr, addlabels = TRUE)

# 预测
p <- predict(test.pr) # 就是每行的PC维度上的值，可用于可视化观测值的位置
head(p)

# 查看每个PC的坐标
##var <- get_pca_var(test.pr)
ind <- get_pca_ind(test.pr) #这是看观察值，也就是行。
head(ind$coord) #和上文的head(p)结果一样

# 画出每个点的位置，使用前几个PC
library(ggplot2)

df=as.data.frame(ind$coord)
df$type=iris[,5]
head(df)

ggplot(df, aes(Dim.1, Dim.2, color=type) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.1, Dim.3, color=type) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.2, Dim.3, color=type) )+geom_point()+theme_bw()

# 三维可视化 https://www.jianshu.com/p/d248b7b54a4b


ref:
关于 R 中 princomp 和 prcomp 的 区别 https://blog.csdn.net/lfz_carlos/article/details/48442091






2.
http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/


library("FactoMineR")
res.pca <- PCA(iris[, 1:4], graph = FALSE)
res.pca

## **Results for the Principal Component Analysis (PCA)**
## The analysis was performed on 23 individuals, described by 10 variables
## *The results are available in the following objects:
## 
##    name               description                          
## 1  "$eig"             "eigenvalues"                        
## 2  "$var"             "results for the variables"          
## 3  "$var$coord"       "coord. for the variables"           
## 4  "$var$cor"         "correlations variables - dimensions"
## 5  "$var$cos2"        "cos2 for the variables"             
## 6  "$var$contrib"     "contributions of the variables"     
## 7  "$ind"             "results for the individuals"        
## 8  "$ind$coord"       "coord. for the individuals"         
## 9  "$ind$cos2"        "cos2 for the individuals"           
## 10 "$ind$contrib"     "contributions of the individuals"   
## 11 "$call"            "summary statistics"                 
## 12 "$call$centre"     "mean of the variables"              
## 13 "$call$ecart.type" "standard error of the variables"    
## 14 "$call$row.w"      "weights for the individuals"        
## 15 "$call$col.w"      "weights for the variables"


plot(res.pca)
summary(res.pca)



(2) PCA的可视化
No matter what function you decide to use [stats::prcomp(), FactoMiner::PCA(), ade4::dudi.pca(), ExPosition::epPCA()], you can easily extract and visualize the results of PCA using R functions provided in the factoextra R package.



These functions include:

- get_eigenvalue(res.pca): Extract the eigenvalues/variances of principal components
- fviz_eig(res.pca): Visualize the eigenvalues
- get_pca_ind(res.pca), get_pca_var(res.pca): Extract the results for individuals and variables, respectively.
- fviz_pca_ind(res.pca), fviz_pca_var(res.pca): Visualize the results individuals and variables, respectively.
- fviz_pca_biplot(res.pca): Make a biplot of individuals and variables.

In the next sections, we’ll illustrate each of these functions.


##1. Eigenvalues / Variances
library("factoextra")
eig.val <- get_eigenvalue(res.pca)
eig.val

#       eigenvalue variance.percent cumulative.variance.percent
# Dim.1 2.91849782       72.9624454                    72.96245
# Dim.2 0.91403047       22.8507618                    95.81321
# Dim.3 0.14675688        3.6689219                    99.48213
# Dim.4 0.02071484        0.5178709                   100.00000


## 2. 每个PC解释多少百分比，柱状图
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))



var <- get_pca_var(res.pca)
var

## Principal Component Analysis Results for variables
##  ======== ========== ========= ========= ======== =======
##   Name       Description                                    
## 1 "$coord"   "Coordinates for the variables"                
## 2 "$cor"     "Correlations between variables and dimensions"
## 3 "$cos2"    "Cos2 for the variables"                       
## 4 "$contrib" "contributions of the variables"



# Coordinates
head(var$coord)
# Cos2: quality on the factore map
head(var$cos2)
# Contributions to the principal components
head(var$contrib)


## Quality of representation
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)


# Total cos2 of variables on Dim.1 and Dim.2
fviz_cos2(res.pca, choice = "var", axes = 1:2)



## 
# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
#

# Change the transparency by cos2 values
fviz_pca_var(res.pca, alpha.var = "cos2")


##################
# PC 的贡献程度
library("corrplot")
corrplot(var$contrib, is.corr=FALSE)    



# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)


#The total contribution to PC1 and PC2 is obtained with the following R code:
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)



fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)
#

# Change the transparency by contrib values
fviz_pca_var(res.pca, alpha.var = "contrib")



############
# 可视化每个元素
fviz_pca_ind(res.pca)

#Like variables, it’s also possible to color individuals by their cos2 values:
fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
)



#You can also change the point size according the cos2 of the corresponding individuals:
fviz_pca_ind(res.pca, pointsize = "cos2", 
             pointshape = 21, fill = "#E7B800",
             repel = TRUE # Avoid text overlapping (slow if many points)
             )

#

#ind.p <- fviz_pca_ind(iris.pca, geom = "point", col.ind = iris$Species)
ggpubr::ggpar(ind.p,
              title = "Principal Component Analysis",
              subtitle = "Iris data set",
              caption = "Source: factoextra",
              xlab = "PC1", ylab = "PC2",
              legend.title = "Species", legend.position = "top",
              ggtheme = theme_gray(), palette = "jco"
)


##
ref:
https://www.jianshu.com/p/6e413420407a





========================================
|-- PCA 的原理和手工实现 //todo
----------------------------------------




ref:
https://www.cnblogs.com/leezx/p/6120302.html





========================================
一文读懂回归分析:概述Cox回归、岭回归、Lasso回归、ElasticNet 回归
----------------------------------------
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml

生物学中的基因很多是有共线性的，所以，后三种方法经常使用。
回归正则化方法（套索，岭和ElasticNet）在高维数据和数据集变量之间存在多重共线性的情况下运行良好。

3）Cox回归
Cox回归的因变量就有些特殊，它不经考虑结果而且考虑结果出现时间的回归模型。它用一个或多个自变量预测一个事件（死亡、失败或旧病复发）发生的时间。Cox回归的主要作用发现风险因素并用于探讨风险因素的强弱。但它的因变量必须同时有2个，一个代表状态，必须是分类变量，一个代表时间，应该是连续变量。只有同时具有这两个变量，才能用Cox回归分析。Cox回归主要用于生存资料的分析，生存资料至少有两个结局变量，一是死亡状态，是活着还是死亡；二是死亡时间，如果死亡，什么时间死亡？如果活着，从开始观察到结束时有多久了？所以有了这两个变量，就可以考虑用Cox回归分析。





9）岭回归
当数据之间存在多重共线性（自变量高度相关）时，就需要使用岭回归分析。在存在多重共线性时，尽管最小二乘法（OLS）测得的估计值不存在偏差，它们的方差也会很大，从而使得观测值与真实值相差甚远。岭回归通过给回归估计值添加一个偏差值，来降低标准误差。

岭回归要点：
1）除常数项以外，岭回归的假设与最小二乘回归相同；
2） 它收缩了相关系数的值，但没有达到零，这表明它不具有特征选择功能；
3）这是一个正则化方法，并且使用的是 L2 正则化。





13）套索回归 LASSO
与岭回归类似，套索也会对回归系数的绝对值添加一个罚值。此外，它能降低偏差并提高线性回归模型的精度。看看下面的等式：

套索回归要点：
1）除常数项以外，这种回归的假设与最小二乘回归类似；
2）它将收缩系数缩减至零（等于零），这确实有助于特征选择；
3）这是一个正则化方法，使用的是 L1 正则化；
4）如果一组预测因子是高度相关的，套索回归会选出其中一个因子并且将其它因子收缩为零。


lasso 回归就是这个意思，就是让回归系数不要太大，以免造成过度拟合（overfitting）。所以呢，lasso regression是个啥呢，就是一个回归，并且回归系数不要太大。

具体的实现方式是加了一个L1正则的惩罚项。





14）ElasticNet 回归
ElasticNet 回归是套索回归和岭回归的组合体。它会事先使用 L1 和 L2 作为正则化矩阵进行训练。当存在多个相关的特征时，Elastic-net 会很有用。岭回归一般会随机选择其中一个特征，而 Elastic-net 则会选择其中的两个。同时包含岭回归和套索回归的一个切实的优点是，ElasticNet 回归可以在循环状态下继承岭回归的一些稳定性。

ElasticNet 回归要点：
1）在高度相关变量的情况下，它会产生群体效应；
2）选择变量的数目没有限制；
3）它可以承受双重收缩。







ref:
https://www.bilibili.com/video/BV1LE411D7SW
Lasso回归（L1正则，MAP+拉普拉斯先验） https://blog.csdn.net/qq_32742009/article/details/81674021
Lasso回归总结 https://www.cnblogs.com/wmx24/p/9555219.html




========================================
|-- 实例: glmnet包做 Ridge回归、Lasso 回归、Elastic-Net Regression
----------------------------------------
1. 使用 glmnet 包

alpha=0时，只有岭回归;
alpha=1时，只有lasso回归;
lamda{ alpha(累加|variable1|) + (1-alpha)(累加|variable2|^2) }




2. 代码及详解
(1) 三种算法的结果与比较
##########
library(glmnet)
set.seed(42)

### step1 获取数据
#模拟数据
n=1000 #行
p=5000 #列
real_p=15 #真实有效的15列，其余都是背景噪音

x=matrix(rnorm(n*p), nrow=n, ncol=p)
y=apply(x[,1:real_p], 1, sum) + rnorm(n) #x的前15列，按行求和。加入噪音。
## 现在有一个向量y，需要用数据x预测向量y
# 预测方法: Ridge, Lasso, Elastic-Net Regression


### step2 分配训练集，测试集
train_rows=sample(1:n, 0.66*n) #选哪些行？随机的2/3行

x.train=x[train_rows,] #训练集
x.test=x[-train_rows,] #测试集
#
y.train=y[train_rows] #训练集结果
y.test=y[-train_rows] #测试集结果
#

### step3 开始岭归回
alpha0.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=0, family='gaussian')
### 参数解释:
#(1) 函数名字中的 cv 表示使用cross validation 获取最优的lambda; 默认使用 10-Fold Cross Validation;
# 和 lm(), glm() 不同，cv.glmnet()不接受 formula notation, x和y必须分别传入。
#(2) mse表示 mean squared error: 残差平方和，除以样本容量。
#  If we were applying Elastic_Net Regression to Logistic Regression, we would set this to deviance.
#(3) 因为使用Ridge Regression，所以设置 alpha=0;
#(4) family 设置为 gaussian; 如果使用Logistic Regression, 则使用 binomial;
#
## 整体解释：
# 该方程，应用岭回归罚分，使用10x交叉验证，找到最优化的lambda

### step4 开始预测
alpha0.predicted=predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx = x.test)
# 使用 predict() 函数，应用 alpha0.fit 到 测试数据集上。
# 参数解释:
# 第一个是模型
## 第二个参数s，可能是size，保存到alpha0.fit 中的最优化的lambda值对应的"the size of the penalty"
#   本例中，设置为 lambda.1se，保存在alpha0.fit中的，这导致最简单的模型(比如，模型有最少的非零参数)，
#   and within 1 standard error of the lambda that had the smallest sum.
## s也可以设置为 lambda.min，就是产生最小误差和的lambda值。
## 本例选择 lambda.1se, 因为统计学上，它和lambda.min没有差异，但是有更少的参数。
## Tips: 我认为只有 Lasso 和 Elastic Net Regression 可以去除参数... 然后呢？
## 本文为了比较的统一，都是用 lambda.1se 了。
# 第三个参数 newx 就是新数据。


## step5 计算损失函数: mean squared error
mean( (y.test - alpha0.predicted)^2 ) #14.88459
# (这里使用 预测与真实值差的平方均值)

################
# 尝试 Lasso Regression(alpha=1时)
# step3 B
alpha1.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=1, family='gaussian')
#step4 B
alpha1.predicted=predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx = x.test)
#step5 B
mean( (y.test - alpha1.predicted)^2 ) #1.184701
# 误差比Ridge小，模型预测效果好于Ridge。


################
# 尝试 Elastic-Net Regression(alpha=(0,1)时)
# step3 C
alpha0.5.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=0.5, family='gaussian')
#step4 C
alpha0.5.predicted=predict(alpha0.5.fit, s=alpha0.5.fit$lambda.1se, newx = x.test)
#step5 C
mean( (y.test - alpha0.5.predicted)^2 ) #1.23797
# 误差比Lasso大，这个比较中 Lasso 赢了。

################
# 可以尝试一系列的 alpha，
list.of.fits=list()
for(i in 0:10){
  print(i) #进度条
  fit.name=paste0('alpha', i/10)
  
  list.of.fits[[fit.name]]=cv.glmnet(x.train, y.train, type.measure = 'mse',
                                     alpha=i/10, family='gaussian')
}
# 计算 mean squared errors for each fit with the testing dataset
results=data.frame()
for(i in 0:10){
  print(i) #进度条
  fit.name=paste0('alpha', i/10)
  fit=list.of.fits[[fit.name]]
  #
  predicted=predict(fit, s=fit$lambda.1se, newx = x.test)
  mse=mean( (y.test - predicted)^2 )
  
  temp=data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
  results=rbind(results, temp)
}
dim(results) #11 3
head(results)
#   alpha       mse fit.name
#1    0.0 14.918840   alpha0
#2    0.1  2.256924 alpha0.1
#3    0.2  1.472927 alpha0.2
plot(mse~alpha, results, type="o")
#结论 对这组数据，alpha=1时，Lasso回归的结果还是最好的。




(2) 确定好拟合参数，接下来解决问题
#####
# part1
fit2<-glmnet(x.train, y.train, alpha=1,family='gaussian')
plot(fit2, xvar = "lambda", label = TRUE)
print(fit2)


# part2
cv.fit <- cv.glmnet( x.train, y.train, alpha=1);

# pic1 可视化拟合效果，看纵坐标MSE什么时候最小(红色虚线标出)
plot(cv.fit)
abline(v=log(c(cv.fit$lambda.min)),lty=2, col="red")

# pic2 看有几个截距，就有几个非0参数
plot(fit2, xvar = "lambda", label = TRUE)
abline(v=log(c(cv.fit$lambda.min,cv.fit$lambda.1se)),lty=2)
abline(v=log(c(cv.fit$lambda.min)),lty=2, col="red")

# 查看每个参数的系数
coef(fit2, s = cv.fit$lambda.1se )



# part 3 预测结果图
pfit = predict(fit2, x.test, s = cv.fit$lambda.1se, type = "response")
plot(pfit, y.test)




#如果取最小值时
cv.fit$lambda.min #0.0908066
Coefficients <- coef(fit2, s = cv.fit$lambda.min)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #69个非0参数

minlassopred <-predict(cv.fit,newx=x.test,s=cv.fit$lambda.min) #,type="response"
minlassopred


#如果取1倍标准误内(大小和min无统计学差异)，参数最少的lambda值。【建议使用这个】
cv.fit$lambda.1se #0.1257568
Coefficients <- coef(fit2, s = cv.fit$lambda.1se)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #27个非0参数

selassopred <-predict(cv.fit,type="response",newx=x.test,s=cv.fit$lambda.1se)
selassopred



(3) 怎么解释2张图? 
特别是顶部数字是什么意思？我推测，是非0变量的个数。

贴吧 https://tieba.baidu.com/p/6301813616?red_tag=2818960114











========================================
|-- 实例: glmnet包做 Cox 回归 筛选变量
----------------------------------------
1. 与预后有关的文章，传统的做法一般会选择多变量cox回归，高级做法自然就是我们今天的lasso分析。
https://www.dxy.cn/bbs/newweb/pc/post/42430476

线性回归采用一个高维的线性函数来尽可能的拟合所有的数据点，最简单的想法就是最小化函数值与真实值误差的平方。

Lasso回归则是在一般线性回归基础上加入了正则项，在保证最佳拟合误差的同时，使得参数尽可能的“简单”，使得模型的泛化能力强。正则项一般采用一，二范数，使得模型更具有泛化性，同时可以解决线性回归中不可逆情况。这个时候你可能不淡定了，你是魔鬼吗？什么是正则项？？？

正则项：正则化就是通过对模型参数进行调整（数量和大小），降低模型的复杂度，以达到可以避免过拟合的效果。正则化是机器学习中的一种叫法，其它领域内叫法各不相同，统计学领域叫惩罚项，数学领域叫范数。而正则项又包括两种，即一范数和二范数，就是L1和L2范数。

重点来了：采用L1范数则是lasso 回归，L2范数则是岭回归了。那么函数有啥区别呢？如下：
	公式略。本博客暂时不支持公式。
- L1范数是所有参数绝对值之和，对应的回归方法叫做Lasso回归。
- L2范数是所有参数的平方和，对应的回归方法叫做Ridge回归，岭回归需要注意的是，正则项中的回归系数为每个自变量对应的回归系数，不包含回归常数项。


L1和L2各有优劣，
- L1是基于特征选择的方式，有多种求解方法，更加具有鲁棒性；
- L2则鲁棒性稍差，只有一种求解方式，而且不是基于特征选择的方式。

在GWAS分析中，当用多个SNP位点作为自变量时，采用基于特征选择的L1范式，不仅可以解决过拟合的问题，还可以筛选重要的SNP位点，所以lasso回归在GWAS中应用的更多一点。

我们在大多数signature文章中主要是基因挑选，自然就是今天的主题lasso cox回归，接下来我们看一下，如何采用R语言glmnet来实现。



2. 代码
library(glmnet)
#载入数据。
#包含30个基因在1000个病人样本中的表达，另一个是每个患者的生存状态和生存时间，生存时间以年为单位，如下：
data('CoxExample')
# 1000行(病人)，30列(基因)
dim(x)
x[1:10,1:5]

# 1000行(病人) 2列(生存时间、终点)
dim(y)
head(y)



# step1 整理数据(30个基因在1000个病人)
x=data.frame(x)
y=data.frame(y)

row.names(x)=paste0("patient",1:1000)
colnames(x)=paste0('gene',1:30)

row.names(y)=paste0("patient",1:1000)
colnames(y)=c('time','status')



# step2 构建生存分析
library(survival)
fit_sur=Surv(y$time, y$status)
dim(fit_sur) #1000 2
head(fit_sur) #[1] 1.76877757  0.54528404  0.04485918+ 0.85032298+ 0.61488426  0.29860939+
str(fit_sur)


# step3 通过glmnet函数中的设置family参数定义采用的算法模型，比如设置cox
fit=glmnet(as.matrix(x), fit_sur, family='cox')
fit

## 画图: 各个基因的系数，大体能看出正负
plot(fit, label=T)

# step4 Lasso回归最重要的就是选择合适的λ值，可以通过cv.glmnet函数实现
cv.fit=cv.glmnet(as.matrix(x), fit_sur, 
                #nfold=10,
                family='cox')
cv.fit
# Call:  cv.glmnet(x = as.matrix(x), y = fit_sur, family = "cox") 
# 
# Measure: Partial Likelihood Deviance 
# 
#      Lambda Measure      SE Nonzero
# min 0.01750   13.07 0.07251      15
# 1se 0.04869   13.13 0.06238      10

plot(cv.fit) #画图，能找到误差最小的lambda值的位置
# 基于该图选择最佳的λ，一般可以采用两个内置函数实现cvfit$lambda.min和 cvfit$lambda.1se 。


# step5 基因筛选，采用coef函数即可，有相应参数的gene则被保留，采用λ使用的是lambda.min
# 我感觉采用1se更好。大小和min没有统计学差异，同时做到了参数最少化
coef.1se=coef(cv.fit, s="lambda.1se")
coef.1se
# 30 x 1 sparse Matrix of class "dgCMatrix"
#                  1
# gene1   0.38108115
# gene2  -0.09838545
# gene3  -0.13898708
# gene4   0.10107014
# gene5  -0.11703684
# gene6  -0.39278773
# gene7   0.24631270
# gene8   0.03861551
# gene9   0.35114295
# gene10  0.04167588
# gene11  .        
# 第二列有数值是非点号的则代表被选择的基因。

# step6 美化lasso图，怎么标出来每个基因？
plot(fit, label=T)



#如果取1倍标准误内(大小和min无统计学差异)，参数最少的lambda值。【建议使用这个】
cv.fit$lambda.1se
Coefficients <- coef(fit, s = cv.fit$lambda.1se)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #10个非0参数





========================================
****** 广告 ******
----------------------------------------




========================================
临床预测模型 速成班
----------------------------------------
#################
###day1
#################

一、快速掌握预测模型构建神器STATA\R软件
1、熟练应用SPSS,手把手让你快速学会STATA\R，无需相关统计软件的前期知识。
2、全程实战数据现场演示特征筛选、变量选择中三大法宝（逐步法、最优子集、Lasso）；模型评价中的三大法宝（AIC\BIC\Adjr2）。
3、发放STATA和R软件全套代码（形成2张Table，5张Figure），学员无需编程基础，对号入座，极大降低统计软件的学习成本。

二、反复练习掌握Liner\Logistic\Cox回归构建预测模型的策略
1、实战经验告知你不同类别预测模型构建的关键点、侧重点、闪光点、以问题为导向的案例式教学，帮你快速厘清建模时的哑变量、交互作用、调整作用、混杂因素、独立作用、阈值作用……




#################
### day2
#################
一、实战临床预测模型-验证部分实战课
1、快速实战体会什么是内部验证和外部验证？模型的可重复性Reproducibility，Cross-Validation、Bootstrap 验证等方法？如何体现模型的外推性（generalizability）？
2、2个以上模型比较的套路流程与模型改进思路

二、实战经验分析二分类结局（logistic回归）与time-to-event数据（COX回归）的similar与dissimilar
1、模型改进验证特点
2、快速理解NRI\IDI等最新评价指标




#################
### day3
#################
一、来自实战的数据，手把手的教你结构化的研究方式，利用STATA、R编程，每一个学员都能亲自独立完成基本的临床预测模型研究。（DCA分析结果、Nomogram图的制作代码等）

二、临床预测模型国际指南（TRIPOD）详细解读

三、既往学员的案例示范讨论，经典预测模型高分文章解析，回复审稿人问题的技巧，国际上喜欢刊登临床预测模型文章的期刊分析，开拓你的眼界……




ref:
https://mp.weixin.qq.com/s?__biz=MjM5MDc0NDU2NQ==&mid=2650376120&idx=1&sn=bac5fdce4914355189e2dadd4b239360




========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------




