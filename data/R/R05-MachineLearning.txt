R与机器学习


========================================
常用R脚本
----------------------------------------
GC含量统计：http://www.biotrainee.com/forum.php?mod=viewthread&tid=625&page=2#pid1904



========================================
ROC曲线的意义
----------------------------------------
ROC曲线的意义
http://www.cnblogs.com/emanlee/archive/2011/05/29/2062280.html

　　ROC曲线指受试者工作特征曲线(receiver operating characteristic curve), 是反映敏感性和特异性连续变量的综合指标,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界值，从而计算出一系列敏感性和特异性，再以敏感性为纵坐标、（1-特异性）为横坐标绘制成曲线，曲线下面积越大，诊断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为敏感性和特异性均较高的临界值. 　　
SPSS统计软件包的10.0版本有ROC曲线的统计功能。ROC曲线真阳性率为纵坐标,假阳性率为横坐标,在座标上由无数个临界值求出的无数对真阳性率和假阳性率作图构成,计算ROC曲线下面积AUCROC来评价诊断效率。


========================================
|-- 画ROC曲线(ROCR包和pROC包)
----------------------------------------
AUC为ROC曲线下方的面积。一般AUC大于0.75就能够说明模型是比较合理的了。


1.
(1)library(ROCR)
data(ROCR.simple)
pred <- prediction(ROCR.simple$predictions, ROCR.simple$labels)
perf <- performance(pred,"tpr","fpr")
str(perf)

##AUC值,ROC曲线下面积为AUC，用来评价分类器的综合性能，该数值取0-1之间，越大越好。
#https://blog.csdn.net/Hellolijunshy/article/details/79991385
auc <- performance(pred,'auc');auc_value=auc@y.values[[1]]
auc_value=round(auc_value,2)
auc_value #0.84

plot(perf,colorize=TRUE,main=paste0('AUC=',auc_value))
abline(a=c(0,0),b=c(1,1),col="grey")


(2)#用ggplot2画图
plotdata=data.frame(
  x=unlist(perf@x.values),
  y=unlist(perf@y.values)
)
g=ggplot(plotdata, aes(x,y, color=x))+
  geom_path(size=1)+
  labs(x="False positive rate",y="True positive rate",title="ROC Curve")+
  scale_color_gradient(name="False positive rate", low="blue", high="red")+
  theme(plot.title=element_text(face="bold",size=10))
g
#



2.
library(pROC)
roc_curve=roc(pre_test$Y~probability)
str(roc_curve)
plot(1-roc_curve$specificities, roc_curve$sensitivities,type="o",
     xlab="1-specificities",ylab="sensitivities",main="ROC Curve")
abline(a=0,b=1,col="gray")
auc2=roc_curve$auc
text(0.5,0.4,paste("AUC: ", round(auc2, digits=2)), col="blue")
#



========================================
R语言画生存期km plot
----------------------------------------
1.生存率：
    RFS: Relapse Free Survival; 无复发生存期
    OS: Overall Survival; 总生存期
    DMFS: Distant Metastasis Free Survival; 无远处转移生存期
    PPS: Post Progression Survival 进展后生存期

2.芯片数据该怎么做标准化：系统原理：两次标准化
第一次使用affy包MAS5算法标准化；第二次[文献14]把平均值调整为1000，消除批次效应。

1).After an initial quality control, redundant samples (n = 384) were excluded [12]. The raw CEL files were MAS5 normalized in the R statistical environment (www.r-project.org) using the affy Bioconductor library [13]. MAS5 can be applied to individual chips, making future extensions of the database uncomplicated.
2).At this stage, we performed a second scaling normalization to set the average expression on each chip to 1,000 to avoid batch effects [14].

13). Gautier L, Cope L, Bolstad BM et al (2004) affy—analysis of Affymetrix GeneChip data at the probe level. Bioinformatics 20:307–315
14). Sims AH, Smethurst GJ, Hey Y et al (2008) The removal of multiplicative, systematic bias allows integration of breast cancer gene expression datasets—improving meta-analysis and prediction of prognosis. BMC Med Genomics 1:42

探针序列：http://kmplot.com/analysis/index.php?p=download


3.使用网站画mRNA、miRNA画十年生存期的KM plot
http://kmplot.com/analysis/





4.R语言画km plot:
共三列数据有效：
# tx是分组
# status是是否删失（0：右删失，1：死亡）
# time表示存活时间

## 生存期曲线的画法
#原始数据 
# case	inst	tx	grade	cond	site	t.stage	n.stage	entry.dt	status	time
# 1	a1	1	1	1	1	1	1	1	1	1
# 2	a1	1	1	1	1	1	1	1	1	100
# 3	a1	1	1	1	1	1	1	1	1	12
# 4	a1	1	1	1	1	1	1	1	1	12
# 5	a1	1	1	1	1	1	1	1	1	13
# 6	a1	1	1	1	1	1	1	1	1	13
# 7	a1	1	1	1	1	1	1	1	1	13
# 8	a1	1	1	1	1	1	1	1	1	20
# 9	a1	1	1	1	1	1	1	1	1	30
# 10	a1	2	1	1	1	1	1	1	1	31
# 11	a1	2	1	1	1	1	1	1	1	32
# 12	a1	2	1	1	1	1	1	1	1	33
# 13	a1	2	1	1	1	1	1	1	1	34
# 14	a1	2	1	1	1	1	1	1	1	35
# 15	a1	2	1	1	1	1	1	1	1	35
# 16	a1	2	1	1	1	1	1	1	1	35
# 17	a1	2	1	1	1	1	1	1	1	40
# 18	a1	2	1	1	1	1	1	1	1	40
# 19	a1	2	1	1	1	1	1	1	1	60
# 20	a1	2	1	1	1	1	1	1	1	80
# 其中
# caese表示编号
# tx是分组
# status是是否删失（0：右删失，1：死亡）
# time表示存活时间

# 载入包
library('survival')

# 读取数据
setwd('D:/R_code')
my=read.csv('death.csv',header=T);my

# 按照tx分组对time和statuss拟合生存曲线，
sd=survfit(Surv(time,as.numeric(status))~tx,data=my,se.fit=FALSE, conf.int=.95)

# 画出生存曲线
plot(sd,lty=1,col=c("red","purple"),
      xlab="time(year)",ylab="Overall Survival",main="survival", #生存曲线
    ) 
#加上图例
legend(60, 1, c("Group1", "Group2"), col = c("red","purple"),
       text.col = c("red","purple"), lty = c(1, 1), pch = c(1, 2),
       merge = TRUE, bg ="#efeeef" );
#rgb(0.99,0.99,0.99) grey


# 这个没懂
with(lung, Surv(time, status))
Surv(heart$start, heart$stop, heart$event)

作图结果如下：





4.2.怎么计算P值（log rank p-value）呢？[解决]
[1]http://bcb.dfci.harvard.edu/~aedin/courses/Bioconductor/survival.pdf
[2]https://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_survival_analysis.pdf   Survival Analysis:
在pdf中搜索log-rank 等关键词。
[3]http://stats.stackexchange.com/questions/114304/log-rank-test-in-r


#p值怎么算？

# http://bbs.pinggu.org/thread-2178930-1-1.html


#请使用survdiff函数做log rank检验或建立Cox模型(coxph)来比较两条生存曲线,

#KM曲线只是一种可视化手段，不是正经的统计推断分析工具。

# 应该用 suvdiff 函数做 log-rank test

#
# 读取数据
setwd('D:/R_code/')
my=read.csv('suvive.csv',header=T);my

#############################
#计算p值：
#http://stats.stackexchange.com/questions/114304/log-rank-test-in-r
# install.packages("survival")
library("survival")
sdf=survdiff(Surv(time, as.numeric(status))~tx, data=my)
sdf
pvalue=1-pchisq(sdf$chisq, df=1)
#pvalue=round(pvalue,2)
# There is also an option for ‘rho’. Rho = 0 (default)
# gives the log-rank test, rho=1 gives the Wilcoxon test.

#install.packages("coin")
#library("coin")
#st=logrank_test(Surv(time, as.numeric(status)) ~ tx, data=my, distribution = "exact")
#st
#########################


# 按照tx分组对time和statuss拟合生存曲线，
km=survfit(Surv(time,as.numeric(status))~tx,data=my,se.fit=FALSE, conf.int=.95)
km

# 画出生存曲线
plot(km,lty=1,col=c("red","purple"), 
     xlab="OS MONTHS",ylab="Percent Survival",main="survival", #生存曲线
) 
#加上图例
legend(130, 1,  legend=c(paste("WT(n=",km$n[1],")"), paste("R132H(n=",km$n[2],")")), col = c("red","purple"),
       text.col = c("red","purple"), lty = c(1, 1), title = paste("logrank p = ",pvalue) )
#       merge = FALSE, bg ="#ffffff"





4.3.怎么标出来截尾数据？

http://www.biostatistic.net/thread-87214-1-1.html
mark.time=T  生存分析显示截尾数据点.

## S3 method for class 'survfit'
plot(x, conf.int=, mark.time=TRUE, 
mark=3, col=1, lty=1, lwd=1, cex=1, log=FALSE, xscale=1, yscale=1,  
firstx=0, firsty=1, xmax, ymin=0, fun, 
xlab="", ylab="", xaxs="S", ...)






4.4.从TCGA下载数据，怎么匹配样品名和生存时间？[半解决]
http://www.cbioportal.org/
（1）用excel的vookup函数[勉强可用，但是还是不自动化]：
VLOOKUP函数是Excel中的一个纵向查找函数，它与LOOKUP函数和HLOOKUP函数属于一类函数，在工作中都有广泛应用。VLOOKUP是按列查找，最终返回该列所需查询列序所对应的值；与之对应的HLOOKUP是按行查找的。
VLOOKUP(lookup_value,table_array,col_index_num,range_lookup)
当前页面样品name，寻找的表格范围，返回的列编号，不精确查找吗？
大概是这样： =VLOOKUP(A2,Sheet2!A:F,5,FALSE)

（2）使用R语言的sql模块【推荐】
...




4.5.怎么从marker选择最合适的cutoff值？
看文献是使用x-tile软件选择最优的cutoff值。
软件：http://medicine.yale.edu/lab/rimm/research/software.aspx
教程：http://wenku.baidu.com/link?url=GmVqIKZ41r1av7wzYDxZYquKk8Am6MuLyq5AD0NGicIa9t5bUvgokQqpgTGCHpIS-Oh-Vd24j1-wJpjY1b25UKXrpPLmZ2W9tlzUrJPqqV7

也有人说用ROC曲线的。





4.6 问题

How to calculate the HR and 95%CI using the log-rank test in R?
怎么用R的log-rank检验分析HR和95%置信区间

The R survival package is very useful to do survival analysis. And I know the survdiff function can be used to compare the difference of survival time in two or more groups. And the p-value number can also be calculated as below. However, how can I calculate the HR and 95% CI using the log-rank test.
R的生存期包对生存分析很有用。我知道survdiff函数可以被用于比较2组或多组生存时间差异。p值可以如下方法计算。然而，我不知道怎么使用log-rank检验计算HR和95%置信区间(CI)。

And I also know I can use the coxph() function to calculate the HR and 95% CI using the Cox regression. However, as the assumption of both the Cox model and log-rank test are that the hazard ratio stay constant over time, so I think I can also calculate the HR and 95% CI using the log-rank test.
我知道怎么使用Cox回归的coxph()函数计算HR和95%CI。然而，Cox模型和log-rank检验的假设都是风险比随着时间稳定，所以我认为我可以使用log-rank检验计算 HR and 95% CI。

According to the book Survival Analysis: A Practical Approach, I got two formulas on Page 62 and 66 to do this (as shown below). So I wrote the R code as below, is there anybody know whether I'm right?
按照《生存分析：实用方法》一书，我在P62和P66看到2个做这个的函数（如下）。所以，我写了如下R代码，有人知道我这么写对吗？

library(survival)
data.survdiff <- survdiff(Surv(time, status) ~ group)
p.val = 1 - pchisq(data.survdiff$chisq, length(data.survdiff$n) - 1)
HR = (data.survdiff$obs[2]/data.survdiff$exp[2])/(data.survdiff$obs[1]/data.survdiff$exp[1])
up95 = exp(log(HR) + qnorm(0.975)*sqrt(1/data.survdiff$exp[2]+1/data.survdiff$exp[1]))
low95 = exp(log(HR) - qnorm(0.975)*sqrt(1/data.survdiff$exp[2]+1/data.survdiff$exp[1]))


输出：
> data.survdiff
Call:
survdiff(formula = Surv(data[, "os_whw"], data[, "status_whw"] == 
    1) ~ data[, "pcascore"] >= median(data[, "pcascore"]))

                                                       N Observed Expected (O-E)^2/E (O-E)^2/V
data[, "pcascore"] >= median(data[, "pcascore"])=FALSE 4        3     4.33     0.411     0.974
data[, "pcascore"] >= median(data[, "pcascore"])=TRUE  5        5     3.67     0.486     0.974

 Chisq= 1  on 1 degrees of freedom, p= 0.324 
> p.val
[1] 0.3235935
> HR
[1] 1.970484
> up95
[1] 7.917248
> low95
[1] 0.4904239





4.6.1 回答1
#############
如果你的问题是两组生存是否有差异，不管其他特性，你应该使用KM估计并获得P值。这是用log-rank检验获得的。仅仅考虑组间生存期差异。如果你（1）分组时随机的（2）仅仅想知道组间生存时间差异（3）想看生存曲线，则这个方法很合适。

然而，如果分组不是随机的而是基本特性差异很大（比如年龄、性别差异），简单的生存分析用处就不大了（比如可能生存差异是年龄因素造成的）；这就是需要引入Cox回归。你使用Cox回归比较生存同时调整混杂因素。如果Cox回归没有包含“分组”之外的预测因素，得到的p值和KM 估计中的log rank检验结果是一样的。但是Cox回归能让你获得每个预测因素的HR和CI，以及p值。

使用Sruvival包中的coxph函数，或者rms包中的cph函数。

通常不用为此纠结。使用Rms包中的函数就很好，还有一本很棒的与包同名的书(Springer; FE Harrell)。包中有可视化和表格化的函数。

可能你有些理论我不理解，但是这时有一个专家会指导我们。其他场景中，你会感觉这个途径很好。

The package: http://cran.r-project.org/web/packages/rms/rms.pdf

The book: http://www.springer.com/mathematics/probability/book/978-0-387-95232-1

(Btw: you can obtain Cox adjusted survival curves; 
google it or check out RMS package documentation).





4.6.2 回答2

你的实现看样子是对的，你认为log-rank可以用于计算HR和95%CI，但我有点问题：
1.你重新计算p值有特殊的理由吗？（因为survdiff()函数已经提供了，重算就像是额外工作）
2.你函数中的HR和95%，和使用coxph()函数计算出来的一致吗？使用coxph()函数验证你的实现。

注意：coxph()函数已经计算过，你为什么用log-rank检验又算了一次HR和95%CI？

I agree with your idea that log-rank test is used to compare the survival of two groups.






refer
1.[推荐]R的生存期包
https://cran.r-project.org/web/packages/survival/survival.pdf
2. http://stats.stackexchange.com/questions/124489/how-to-calculate-the-hr-and-95ci-using-the-log-rank-test-in-r

3.Use Software R to do Survival Analysis and Simulation. A tutorial
http://www.ms.uky.edu/~mai/Rsurv.pdf

10.英文图书 https://www.amazon.com/dp/0470870400/?tag=stackoverfl08-20

R语言的with和by http://statmethods.net/stats/withby.html

生存分析完整论述：https://cran.r-project.org/web/views/Survival.html  太复杂了



http://wenku.baidu.com/link?url=Ld9FAX6pFUfLet5YtTToTtyzhQ2GA2mD9IkmhBqHjjDEb3e3S2BAI8C3hxZ3UxhGKUhWD4aT_uh8rWIJmEvY0fFJtdmU1VWwQzlF9LolNU7
如何用graphpad制作生存曲线？ http://www.dxy.cn/bbs/topic/25619320
http://blog.csdn.net/shmilyringpull/article/details/17529637










========================================
R语言k折交叉验证 k fold C.V.
----------------------------------------
# 原理分析 https://blog.csdn.net/Tanya_girl/article/details/50221797

library(caret)
#caret包中有createFolds函数，如
training=iris
set.seed(7)
folds<-createFolds(y=training$Species,k=10)
#这里参数y是训练数据集label,k是几折交叉验证
#我们可以看看这里输出是什么
folds
#可以看出来folds有10个元素

#可见folds里每个元素是原来label长度的十分之一，存的是label的id随机的十分之一
#可以写个for循环进行交叉验证
for(i in 1:10){
  train_cv<-training[-folds[[i]],]
  test_cv<-training[folds[[i]],]
  
  print(i)
  print(train_cv)
  print(test_cv)
}


# 完整代码这里：https://blog.csdn.net/tiaaaaa/article/details/58116346







========================================
SVM 分类
----------------------------------------







========================================
SVR: 支持向量回归
----------------------------------------






========================================
主成分分析 (PCA, principal component analysis)
----------------------------------------
主成分分析 (PCA, principal component analysis)是一种数学降维方法, 利用正交变换 (orthogonal transformation)把一系列可能线性相关的变量转换为一组线性不相关的新变量，也称为主成分，从而利用新变量在更小的维度下展示数据的特征。


主成分是原有变量的线性组合，其数目不多于原始变量。组合之后，相当于我们获得了一批新的观测数据，这些数据的含义不同于原有数据，但包含了之前数据的大部分特征，并且有着较低的维度，便于进一步的分析。

在空间上，PCA可以理解为把原始数据投射到一个新的坐标系统，第一主成分为第一坐标轴，它的含义代表了原始数据中多个变量经过某种变换得到的新变量的变化区间；第二成分为第二坐标轴，代表了原始数据中多个变量经过某种变换得到的第二个新变量的变化区间。这样我们把利用原始数据解释样品的差异转变为利用新变量解释样品的差异。

这种投射方式会有很多，为了最大限度保留对原始数据的解释，一般会用最大方差理论或最小损失理论，使得第一主成分有着最大的方差或变异数 (就是说其能尽量多的解释原始数据的差异)；随后的每一个主成分都与前面的主成分正交，且有着仅次于前一主成分的最大方差 (正交简单的理解就是两个主成分空间夹角为90°，两者之间无线性关联，从而完成去冗余操作)。


1. 主成分分析的意义
(1) 简化运算。
假如某些基因如持家基因在所有样本中表达都一样，它们对于解释样本的差异也没有意义。

应用PCA就可以在尽量多的保持变量所包含的信息又能维持尽量少的变量数目，帮助简化运算和结果解释。


(2)去除数据噪音。

比如说我们在样品的制备过程中，由于不完全一致的操作，导致样品的状态有细微的改变，从而造成一些持家基因也发生了相应的变化，但变化幅度远小于核心基因 (一般认为噪音的方差小于信息的方差）。而PCA在降维的过程中滤去了这些变化幅度较小的噪音变化，增大了数据的信噪比。

(3)利用散点图实现多维数据可视化。

在上面的表达谱分析中，假如我们有1个基因，可以在线性层面对样本进行分类；如果我们有2个基因，可以在一个平面对样本进行分类；如果我们有3个基因，可以在一个立体空间对样本进行分类；如果有更多的基因，比如说n个，那么每个样品就是n维空间的一个点，则很难在图形上展示样品的分类关系。利用PCA分析，我们可以选取贡献最大的2个或3个主成分作为数据代表用以可视化。这比直接选取三个表达变化最大的基因更能反映样品之间的差异。（利用Pearson相关系数对样品进行聚类在样品数目比较少时是一个解决办法）


(4) 发现隐性相关变量。

我们在合并冗余原始变量得到主成分过程中，会发现某些原始变量对同一主成分有着相似的贡献，也就是说这些变量之间存在着某种相关性，为相关变量。同时也可以获得这些变量对主成分的贡献程度。对基因表达数据可以理解为发现了存在协同或拮抗关系的基因。






2. PCA 实现原理 及注意事项
(1)
PCA分析不是简单的选取2-3个变化最大的基因，而是先把原始的变量做一个评估，计算各个变量各自的变异度（方差）和两两变量的相关度（协方差），得到一个协方差矩阵。

在这个协方差矩阵中，对角线的值为每个变量的方差，其他值为每两个变量的协方差。随后对原变量的协方差矩阵对角化处理，即求解其特征值和特征向量。原变量与特征向量的乘积（对原始变量的线性组合）即为新变量（回顾下线性代数中的矩阵乘法）；新变量的协方差矩阵为对角协方差矩阵且对角线上的方差由大到小排列；然后从新变量中选择信息量最丰富的也就是方差最大的前2个或3个新变量，也即是主成分用于可视化。


(2) 不同预处理对PCA结果的影响
1) 不同标准化的本质在于研究者认为，是数值的量度本身重要，还是数值的变化重要。
2) 数值的量度重要，则选择原始数据或log转换；
3) 数值的变化重要，则选择scale。




3. PCA注意事项

(1) 一般说来，在PCA之前原始数据需要中心化（centering，数值减去平均值）。中心化的方法很多，除了平均值中心化（mean-centering）外，还包括其它更稳健的方法，比如中位数中心化等。

(2)除了中心化以外，定标 (Scale, 数值除以标准差) 也是数据前处理中需要考虑的一点。如果数据没有定标，则原始数据中方差大的变量对主成分的贡献会很大。数据的方差与其量级成指数关系，比如一组数据(1,2,3,4)的方差是1.67，而(10,20,30,40)的方差就是167,数据变大10倍，方差放大了100倍。

(3)但是定标(scale)可能会有一些负面效果，因为定标后变量之间的权重就是变得相同。如果我们的变量中有噪音的话，我们就在无形中把噪音和信息的权重变得相同，但PCA本身无法区分信号和噪音。在这样的情形下，我们就不必做定标。

(4)一般而言，对于度量单位不同的指标或是取值范围彼此差异非常大的指标不直接由其协方差矩阵出发进行主成分分析，而应该考虑对数据的标准化。比如度量单位不同，有万人、万吨、万元、亿元，而数据间的差异性也非常大，小则几十大则几万，因此在用协方差矩阵求解主成分时存在协方差矩阵中数据的差异性很大。

在后面提取主成分时发现，只提取了一个主成分，而此时并不能将所有的变量都解释到，这就没有真正起到降维的作用。此时就需要对数据进行定标(scale)，这样提取的主成分可以覆盖更多的变量，这就实现主成分分析的最终目的。

但是对原始数据进行标准化后更倾向于使得各个指标的作用在主成分分析构成中相等。对于数据取值范围不大或是度量单位相同的指标进行标准化处理后，其主成分分析的结果与仍由协方差矩阵出发求得的结果有较大区别。这是因为对数据标准化的过程实际上就是抹杀原有变量离散程度差异的过程，标准化后方差均为1，而实际上方差是对数据信息的重要概括形式，也就是说，对原始数据进行标准化后抹杀了一部分重要信息，因此才使得标准化后各变量在主成分构成中的作用趋于相等。因此，对同度量或是取值范围在同量级的数据还是直接使用非定标数据求解主成分为宜。


(5)中心化和定标都会受数据中离群值（outliers）或者数据不均匀（比如数据被分为若干个小组）的影响，应该用更稳健的中心化和定标方法。


(6)PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。








3. PCA 分析和可视化 示例

########### step1 PCA
library(caret)

# By default, prcomp will centralized the data using mean.
# Normalize data for PCA by dividing each data by column standard deviation.
# Often, we would normalize data.
# Only when we care about the real number changes other than the trends,
# `scale` can be set to TRUE. 
# We will show the differences of scaling and un-scaling effects.

iris.pca=prcomp(iris[,1:4])
iris.pca2=prcomp(iris[,1:4], scale=T)

iris.pca
#Standard deviations (1, .., p=4):
#[1] 2.0562689 0.4926162 0.2796596 0.1543862
#Rotation (n x k) = (4 x 4):
#                 PC1         PC2         PC3        PC4
#Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872
#Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231
#Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390
#Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574

print(str(iris.pca))





########### step2 PCA结果提取和可视化神器

library(factoextra)

#1. 碎石图展示每个主成分的贡献
## 如果需要保持，去掉下面第1,3行的注释
#pdf("1.pdf")
fviz_eig(iris.pca, addlabels = TRUE)
#dev.off()

# 2. PCA样品聚类信息展示
# repel=T，自动调整文本位置
fviz_pca_ind(iris.pca, repel=T) +coord_fixed(1) # 关键的增加


# 3. 根据样品分组上色
fviz_pca_ind( iris.pca, col.ind=iris[,5], 
             mean.point=F, addEllipses = T, legend.title="Groups")

# 4. 增加不同属性的椭圆
# “convex”: plot convex hull of a set o points.
# “confidence”: plot confidence ellipses around group mean points as 
#     the function coord.ellipse() [in FactoMineR].
# “t”: assumes a multivariate t-distribution.
# “norm”: assumes a multivariate normal distribution.
# “euclid”: draws a circle with the radius equal to level, 
#    representing the euclidean distance from the center. 
#    This ellipse probably won’t appear circular unless coord_fixed() is applied.

# 根据分组上色并绘制95%置信区间
fviz_pca_ind(iris.pca, col.ind=iris[,5], 
             mean.point=F, addEllipses = T, legend.title="Groups", 
             ellipse.type="confidence", ellipse.level=0.95)
#

#5. 展示贡献最大的变量 (基因)
# 1） 展示与主坐标轴的相关性大于0.99的变量 (具体数字自己调整)
# Visualize variable with cos2 >= 0.99
fviz_pca_var(iris.pca, select.var = list(cos2 = 0.60), 
             repel=T, 
             col.var = "cos2", 
             geom.var = c("arrow", "text") )

#2)展示与主坐标轴最相关的10个变量
# Top 10 active variables with the highest cos2
fviz_pca_var(iris.pca, select.var= list(cos2 = 10), repel=T, col.var = "contrib")

# 3）展示自己关心的变量（基因）与主坐标轴的相关性分布
# Select by names
# 这里选择的是MAD值最大的几个基因
#name <- list(name = c("FN1", "DCN", "CEMIP","CCDC80","IGFBP5","COL1A1","GREM1"))
#fviz_pca_var(pca, select.var = name)


#6. biPLot同时展示样本分组和关键基因
# top 5 contributing individuals and variable
fviz_pca_biplot(iris.pca,
                fill.ind=iris[,5],
                palette="joo",
                addEllipses = T, 
                ellipse.type="confidence",
                ellipse.level=0.95,
                mean.point=F,
                col.var="contrib",
                gradient.cols = "RdYlBu",
                select.var = list(contrib = 10),
                ggtheme = theme_minimal())
#




========================================
|-- PCA 分析与可视化 示例
----------------------------------------

1. 精简版
test=iris[,1:4]
test.pr<-princomp(test,cor=TRUE)
test.pr

#
summary(test.pr,loadings=TRUE)  #loading是逻辑变量，当 loading=TRUE 时表示显示 loading 的内容

#
screeplot(test.pr,type="lines")
p <- predict(test.pr) 
#biplot(test.pr)  #画出数据关于主成分的散点图和原坐标在主成分下的方向

# 每个PC占了多少比例
library(factoextra)
fviz_eig(test.pr, addlabels = TRUE)

# 查看每个PC的坐标
res.pca=test.pr
var <- get_pca_var(res.pca)
head(var$coord) 

# 画出每个点的位置，使用前几个PC
library(ggplot2)
ggplot(as.data.frame(var$coord), aes(Dim.1, Dim.2) )+geom_point()
ggplot(as.data.frame(var$coord), aes(Dim.1, Dim.3) )+geom_point()
ggplot(as.data.frame(var$coord), aes(Dim.2, Dim.3) )+geom_point()








2.
http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/


library("FactoMineR")
res.pca <- PCA(iris[, 1:4], graph = FALSE)
res.pca

## **Results for the Principal Component Analysis (PCA)**
## The analysis was performed on 23 individuals, described by 10 variables
## *The results are available in the following objects:
## 
##    name               description                          
## 1  "$eig"             "eigenvalues"                        
## 2  "$var"             "results for the variables"          
## 3  "$var$coord"       "coord. for the variables"           
## 4  "$var$cor"         "correlations variables - dimensions"
## 5  "$var$cos2"        "cos2 for the variables"             
## 6  "$var$contrib"     "contributions of the variables"     
## 7  "$ind"             "results for the individuals"        
## 8  "$ind$coord"       "coord. for the individuals"         
## 9  "$ind$cos2"        "cos2 for the individuals"           
## 10 "$ind$contrib"     "contributions of the individuals"   
## 11 "$call"            "summary statistics"                 
## 12 "$call$centre"     "mean of the variables"              
## 13 "$call$ecart.type" "standard error of the variables"    
## 14 "$call$row.w"      "weights for the individuals"        
## 15 "$call$col.w"      "weights for the variables"


plot(res.pca)
summary(res.pca)



(2) PCA的可视化
No matter what function you decide to use [stats::prcomp(), FactoMiner::PCA(), ade4::dudi.pca(), ExPosition::epPCA()], you can easily extract and visualize the results of PCA using R functions provided in the factoextra R package.



These functions include:

- get_eigenvalue(res.pca): Extract the eigenvalues/variances of principal components
- fviz_eig(res.pca): Visualize the eigenvalues
- get_pca_ind(res.pca), get_pca_var(res.pca): Extract the results for individuals and variables, respectively.
- fviz_pca_ind(res.pca), fviz_pca_var(res.pca): Visualize the results individuals and variables, respectively.
- fviz_pca_biplot(res.pca): Make a biplot of individuals and variables.

In the next sections, we’ll illustrate each of these functions.


##1. Eigenvalues / Variances
library("factoextra")
eig.val <- get_eigenvalue(res.pca)
eig.val

#       eigenvalue variance.percent cumulative.variance.percent
# Dim.1 2.91849782       72.9624454                    72.96245
# Dim.2 0.91403047       22.8507618                    95.81321
# Dim.3 0.14675688        3.6689219                    99.48213
# Dim.4 0.02071484        0.5178709                   100.00000


## 2. 每个PC解释多少百分比，柱状图
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))



var <- get_pca_var(res.pca)
var

## Principal Component Analysis Results for variables
##  ======== ========== ========= ========= ======== =======
##   Name       Description                                    
## 1 "$coord"   "Coordinates for the variables"                
## 2 "$cor"     "Correlations between variables and dimensions"
## 3 "$cos2"    "Cos2 for the variables"                       
## 4 "$contrib" "contributions of the variables"



# Coordinates
head(var$coord)
# Cos2: quality on the factore map
head(var$cos2)
# Contributions to the principal components
head(var$contrib)


## Quality of representation
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)


# Total cos2 of variables on Dim.1 and Dim.2
fviz_cos2(res.pca, choice = "var", axes = 1:2)



## 
# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
#

# Change the transparency by cos2 values
fviz_pca_var(res.pca, alpha.var = "cos2")


##################
# PC 的贡献程度
library("corrplot")
corrplot(var$contrib, is.corr=FALSE)    



# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)


#The total contribution to PC1 and PC2 is obtained with the following R code:
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)



fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)
#

# Change the transparency by contrib values
fviz_pca_var(res.pca, alpha.var = "contrib")



############
# 可视化每个元素
fviz_pca_ind(res.pca)

#Like variables, it’s also possible to color individuals by their cos2 values:
fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
)



#You can also change the point size according the cos2 of the corresponding individuals:
fviz_pca_ind(res.pca, pointsize = "cos2", 
             pointshape = 21, fill = "#E7B800",
             repel = TRUE # Avoid text overlapping (slow if many points)
             )

#

#ind.p <- fviz_pca_ind(iris.pca, geom = "point", col.ind = iris$Species)
ggpubr::ggpar(ind.p,
              title = "Principal Component Analysis",
              subtitle = "Iris data set",
              caption = "Source: factoextra",
              xlab = "PC1", ylab = "PC2",
              legend.title = "Species", legend.position = "top",
              ggtheme = theme_gray(), palette = "jco"
)


##
ref:
https://www.jianshu.com/p/6e413420407a





========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------




