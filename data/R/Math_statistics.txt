R03 - 统计学

罗马数字符合

累加 大sigma
正态分布 X~N(mu, sigma^2)



先学习负二项分布


本文记录R和数理统计学。
还有一个paper中遇到的统计学: NGS/NGS_statistics.txt 


薛毅和陈立萍的《统计建模与R软件》（清华大学出版社，2007年）中


========================================
统计学资源汇总 及书单
----------------------------------------
G:\baiduDisk\考研数学2014基础班\概率统计

1.StatQuest系列笔记汇总
https://mp.weixin.qq.com/s?__biz=MzAxMzkzNDUyOQ==&mid=2247484185&idx=1&sn=59944cbe7a078148287707e25be70467&scene=21


2.  概率论与数理统计学习笔记
https://blog.csdn.net/hpdlzu80100

G:\baiduDisk\考研数学2014基础班\概率统计





3. 教科书《R语言与统计分析》 汤银才 主编 447页pdf





4.教科书《统计建模与R软件》上下册 共643页 薛毅陈立萍 编著
目录在PDF7页，


5.统计之都： http://cos.name/tag/%E5%8A%A8%E7%94%BB/



6.《R Programming for Data Science》
https://bookdown.org/rdpeng/rprogdatascience/
podcast: 《Not So Standard Deviations》 http://nssdeviations.com/






###################
统计学书单

一、统计学基础部分
1、《统计学》 David Freedman等著，魏宗舒，施锡铨等译中国统计出版社
    据说是统计思想讲得最好的一本书，读了部分章节，受益很多。整本书几乎没有公式，但是讲到了统计思想的精髓。

2、《Mind on statistics(英文版）》机械工业出版社
    只需要高中的数学水平，统计的扫盲书。有一句话影响很深：Mathematics as to statistics is something like hammer, nails, wood as to a house, it's just the material and tools but not the house itself。

3、《Mathematical Statistics and Data Analysis（英文版.第二版）》 机械工业出版社
    看了就发现和国内的数理统计树有明显的不同。这本书理念很好，讲了很多新的东西，把很热门的Bootstrap方法和传统统计在一起讲了。Amazon上有书评。

4、《Business Statistics a decision making approach（影印版）》中国统计出版社
    在实务中很实用的东西，虽然往往为数理统计的老师所不屑

5、《Understanding Statistics in the behavioral science（影印版）》 中国统计出版社
    和上面那本是一个系列的。老外的书都挺有意思的

6、《探索性数据分析》中国统计出版社和第一本是一个系列的。
       大家好好看看陈希儒老先生做的序，可以说是对中国数理统计的一种反思。

二、回归部分
1、《应用线性回归》 中国统计出版社
    还是著名的蓝皮书系列，有一定的深度，道理讲得挺透的。看看里面对于偏回归系数的说明，绝对是大开眼界啊！非常精彩的书

2、《Regression Analysis by example (3rd Ed影印版)》
    这是偶第一本从头到底读完的原版统计书，太好看了。那张虚拟变量写得比小说都吸引人。没什么推导，甚至说“假定你有统计软件可以算出结果”，主要就是将分-析，怎么看图，怎么看结果。看完才觉得回归真得很好玩

3、《Logistics回归模型——方法与应用》王济川郭志刚高等教育出版社
    不多的国内的经典统计教材。两位都是社会学出身，不重推导重应用。每章都有详细的SAS和SPSS程序和输出的分析。两位估计洋墨水喝得比较多，中文写的书，但是明显老外写书的风格


三、多元

1、《应用多元分析（第二版）》王学民上海财经大学出版社
    现在好像就是用的这本书，但是请注意，这本书的亮点不是推导，而是后面和SAS结合的部分，以及其中的一些想法（比如P99 n对假设检验的影响，绝对是统计的感觉，不是推推公式就能感觉到的）。这是一本国内很好的多元统计教材。

2、《Analyzing Multivariate Data（英文版）》 Lattin等著机械工业出版社
    这本书有很多直观的感觉和解释，非常有意思。对数学要求不高，证明也不够好，但的确是“统计书”，不是数学书。

3、《Applied Multivariate Statistical Analysis (5th Ed影印版)》 Johnson & Wichem 著中国统计出版社
    个人认为是国内能买到的最好的多元统计书了。Amazon 上有人评论，评价很高的。不过据王学民老师说，这本书的证明还是有不太清楚，老外实务可以，证明实在不咋的，呵呵


四、时间序列

1、《商务和经济预测中的时间序列模型》弗朗西斯著
    Amazon 上五星推荐的书，讲了很多很新的东西也非常实用。我看完才知道，原来时间序列不知有AR(1) MA(1)啊，哈

2、《Forecasting and Time Series an applied approach(third edition)》 Bowerman & Connell 著
    本书的主讲Box-Jenkins(ARIMA)方法，附上了SAS和Minitab程序


五、抽样


1、《抽样技术》 科克伦著 张尧庭译
    绝对是该领域最权威，最经典的书了。王学民老师说：这本书不是那么好懂的，数学系的人，就算看得懂每个公式，未必能懂它的意思（不是数学系的人，还是别看了吧）。

2、《Sampling: Design and Analysis（影印版)》 Lohr著中国统计出版社
    讲了很多很新的方法，无应答，非抽样误差，再抽样，都有讨论。也很不好懂，当时偶是和《Advance Microeconomic Theory》一起看的，后者被许多人认为是梦魇，但是和前者一比，好懂多了。主要还是理念上的差距。我们的统计思想和数据感觉有待加强啊


六、软件及其他

1、《SAS软件与应用统计分析》王吉利张尧庭主编    好书啊！！！！

2、《SAS V8基础教程》汪嘉冈编中国统计出版社
    主要讲编程，没怎么讲统计。如果想加强SAS编程可以考虑。

3、《SPSS11统计分析教程（基础篇）（高级篇）》张文彤北京希望出版社
    当初第一次看这本书，发现怎么几乎都看不懂，尤其是高级篇，现在终于搞清楚了：）

4、《金融市场的统计分析》张尧庭著广西师范大学出版社
    张老师到底是大家，薄薄的一本书，言简意言简意赅，把主要的金融模型都讲清楚了。看完会发现，分析金融单单数学模型还是纸上谈兵，必须加上统计模型和统计方法才能真正应用。本书用的多元统计（代数知识）比较深
#


========================================
偏差信息准则 deviance information criterion (DIC),与 bayesian information criterion (BIC), AIC
----------------------------------------
The deviance information criterion (DIC) was introduced in 2002 by Spiegelhalter et al to compare the relative fit of a set of Bayesian hierarchical models.

It is similar to Akaike's information criterion (AIC) in combining a measure of goodness-of-fit and measure of complexity, both based on the deviance. 

While AIC uses the maximum likelihood estimate, DIC's plug-in estimate is based on the posterior mean. 

As the number of independent parameters in a Bayesian hierarchical model is not clearly defined, DIC estimates the effective number of parameters by the difference of the posterior mean of the deviance and the deviance at the posterior mean. 

This coincides with the number of independent parameters in fixed effect models with flat priors, thus the DIC is a generalization of AIC. 

It can be justified as an estimate of the posterior predictive model performance within a decision-theoretic framework and it is asymptotically equivalent to leave-one-out cross-validation. 

The DIC has been used extensively for practical model comparison in many disciplines and works well for exponential family models but due to its dependence on the parametrization and focus of a model, its application to mixture models is problematic. 

Keywords: Bayesian model comparison; posterior predictive accuracy; BIC; AIC; WAIC; scoring rule; cross-validation; decision theory; Kullback–Leibler information










========================================
标准化与归一化，scale()函数、sweep()函数
----------------------------------------

一、
#数据集
x<-cbind(c(1,2,3,4),c(5,5,10,20),c(3,6,9,12))

#自己写标准化
x_min_temp<-apply(x,2,min) 
x_min<-matrix(rep(x_min_temp,4),byrow=TRUE,ncol=3)#需要输入行数和列数
abs(x-x_min)#当前值减去均值
x_extreme_temp<-apply(x,2,max)-apply(x,2,min)
x_extreme<-matrix(rep(x_extreme_temp,4),byrow=TRUE,ncol=3)#需要输入行数和列数
abs(x-x_min)/x_extreme


2. sweep函数
center <- sweep(x, 2, apply(x, 2, min),'-') #在列的方向上减去最小值，不加‘-’也行
R <- apply(x, 2, max) - apply(x,2,min)   #算出极差，即列上的最大值-最小值
x_star<- sweep(center, 2, R, "/")        #把减去均值后的矩阵在列的方向上除以极差向量

#sweep函数更简洁、易懂，且不需要输入行数和列数，二者性能也差不多

#sweep再举一个例子哈
m<-matrix(c(1:9),byrow=TRUE,nrow=3)
#第一行都加1，第二行都加4，第三行都加7
sweep(m, 1, c(1,4,7), "+")  



3. scale函数，这个比较简单，不多说
scale(x, center = TRUE, scale = TRUE)







二、归一化和标准化的区别？
1. 归一化（Normalization）
1).把数据变为（0，1）之间的小数。主要是为了方便数据处理，因为将数据映射到0～1范围之内，可以使处理过程更加便捷、快速。 
2).把有量纲表达式变换为无量纲表达式，成为纯量。经过归一化处理的数据，处于同一数量级，可以消除指标之间的量纲和量纲单位的影响，提高不同数据指标之间的可比性。 

主要算法： 
1).线性转换，即min-max归一化（常用方法） y=(x-min)/(max-min) 
2). 对数函数转换 y=log10(x) 
3).反余切函数转换 y=atan(x)*2/PI 



2.标准化（Standardization） 
数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。 

主要方法： 
1).z-score标准化，即零-均值标准化（常用方法） y=(x-μ)/σ 是一种统计的处理，基于正态分布的假设，将数据变换为均值为0、标准差为1的标准正态分布。但即使数据不服从正态分布，也可以用此法。特别适用于数据的最大值和最小值未知，或存在孤立点。 
2).小数定标标准化 y=x/10^j （j确保max(|y|)&lt;1） 通过移动x的小数位置进行标准化 
3).对数Logistic模式 y=1/(1+e^(-x))


refer:
http://blog.csdn.net/yitianguxingjian/article/details/51820758



========================================
2.1 随机抽样: 模拟——随机数、抽样、线性模型
----------------------------------------
为了可重复，设置随机数种子为一个具体值
set.seed(1)


1.
(1)Simulation
今天学习的内容是模拟 —— Simulation，在统计和一些其他的应用中很重要，所以在这里介绍一些R语言中可以做模拟的函数。

用于模拟已知概率分布的数字和变量
有些函数可以直接生成符合某种概率分布的随机数字或变量，例如：
rnorm()：指定一个均值和标准差，即可生成符合正态分布的随机数字变量
rpois()：从已知平均发生率（rate）的泊松分布中生成泊松随机变量

一共有四类基本的函数和概率分布函数相关，它们的前缀分别是d, r, p以及q：
d：用来估计密度（density）
r：用来产生随机数字（random）
p：估计累计分布（cumulative distribution）
q：估计分位数（quantile）
每种分布都有以上这四种前缀构成的函数，比如rpois(), dpois(), ppois(), qpois()等。

每种函数都有不同的参数，拿正态分布的四个函数举例：
dnorm(x, mean = 0, sd = 1, log = FALSE)
## 可以求密度的对数值

pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
## 可以对概率求对数；计算该分布的左尾，如果填FALSE就是计算右尾

qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
## 可以对概率求对数；计算该分布的左尾，如果填FALSE就是计算右尾

rnorm(n, mean = 0, sd = 1)
## n是你想生成的随机变量的个数

所有函数都需要我们给定均值和标准差，以此确定实际的概率分布，如果不指定，那么默认属于标准正态分布（均值为0，标准差为1）
生成最简单的服从标准正态分布的随机数：
> ## Simulate standard Normal random numbers
> x <- rnorm(10)   
> x
 [1]  0.01874617 -0.18425254 -1.37133055 -0.59916772  0.29454513
 [6]  0.38979430 -1.20807618 -0.36367602 -1.62667268 -0.25647839

修改参数：
> x <- rnorm(10, 20, 2) 
> x
 [1] 22.20356 21.51156 19.52353 21.97489 21.48278 20.17869 18.09011
 [8] 19.60970 21.85104 20.96596
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  18.09   19.75   21.22   20.74   21.77   22.20
#



(2)设置随机数字生成种子
set.seed()函数可以用来设置随机数字生成种子（seed）。
这种方法可产生相同的随机数，也叫“伪随机数”。它怎么用？

举例
我们可以设置种子为任意整数，然后生成随机数字如下：
> set.seed(1)
> rnorm(5)
[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078
> set.seed(2)
> rnorm(5)
[1] -0.89691455  0.18484918  1.58784533 -1.13037567 -0.08025176
> set.seed(5)
> rnorm(5)
[1] -0.84085548  1.38435934 -1.25549186  0.07014277  1.71144087

我们看到种子设定值不同时随机数字也不同，但是再设定相同种子就会出现完全一样的随机数字：
> set.seed(1)
> rnorm(5)
[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078

这个操作能让你重复得到之前生成的随机数字，可以让我们的模拟过程可重复，所以在生成随机数字之前最好设定一个种子。

再拿泊松分布举例：
生成不同发生率的10个随机变量：
> rpois(10, 1)    ## Counts with a mean of 1
 [1] 0 0 1 1 2 1 1 4 1 2
> rpois(10, 2)    ## Counts with a mean of 2
 [1] 4 1 2 0 1 1 0 1 4 1
> rpois(10, 20)   ## Counts with a mean of 20
 [1] 19 19 24 23 22 24 23 20 11 22
#

估计泊松分布的累积分布函数：

## 在平均发生率为2的泊松分布中，出现小于等于2的随机变量的概率是多少
> ppois(2,2) 
[1] 0.6766764
## 在平均发生率为2的泊松分布中，出现小于等于4的随机变量的概率是多少
> ppois(4,2)
[1] 0.947347
## 在平均发生率为2的泊松分布中，出现小于等于6的随机变量的概率是多少
> ppois(6,2)
[1] 0.9954662




(3)Simulating a Linear Model
我们再来看怎样从简单的线性模型中提取随机值

如果x是单一自变量
举例：
## 首先记得先设定一个种子
> set.seed(20)

## 设置自变量x取值，属于标准正态分布
> x <- rnorm(100)

## 随机噪声e是属于标准差为2的正态分布
> e <- rnorm(100, 0, 2)

## 设置线性方程的回归系数和截距
> y <- 0.5 + 2 * x + e

## 输出概要结果
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-6.4084 -1.5402  0.6789  0.6893  2.9303  6.5052 

## 画出图表
> plot(x,y)
这就是通过回归模型模拟出的x和y的图表，可以看出明显的线性关系。





(4)如果x是二元变量
x如果是二元变量，可以代表两种性别、两种实验处理（可以是实验组和对照组）这一类情况

使用二项分布函数rbinom()

## 重新设定种子
> set.seed(10)

## 随机取100个二元变量数据，并设置参数
> x <- rbinom(100, 1, 0.5)

## 取符合正态分布的随机变量
> e <- rnorm(100, 0, 2)

## 生成线性模型
> y <- 0.5 + 2 *x + e

## 查看结果和作图
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-3.4936 -0.1409  1.5767  1.4322  2.8397  6.9410 
> plot(x, y)

可以看到，x是二元变量，y依旧是连续的，符合正态分布






(5)从复杂模型中模拟取值
假设结果y服从于一个平均值为μ的泊松分布，log(μ)服从一个截距为β0，斜率β1的线性函数，x是其中的自变量

> set.seed(1)
> x <- rnorm(100)    

## 模拟线型变量log(μ)
> log.mu <- 0.5 + 0.3 * x

## 使用rpois函数来计算y，取幂
> y <- rpois(100, exp(log.mu))

## 查看结果
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.00    1.00    1.00    1.55    2.00    6.00 
> plot(x, y)

可以看到x越大y越大，呈线性关系






(6)Random Sampling
这里使用的函数是sample()，可以从你给定的一组对象中，随机抽取样本

假如我们提供一个数值向量，sample()可以从中随机抽取样本。因此我们可以人为确定一个分布，指定给一个向量并从中取样

举例：
从整数1到10中取样，取出的数字不放回，如果重复去取样两次，得到的数字是不重复的，对字母取样也是同理：

> set.seed(1)
> sample(1:10, 4)
[1] 3 4 5 7
> sample(1:10, 4)
[1] 3 9 8 5

> ## Doesn't have to be numbers
> sample(letters, 5)    
[1] "q" "b" "e" "x" "p"



如果只提供向量，不指定任何条件，返回结果就是这些整数重新排列，数字内部无重复：
> ## Do a random permutation
> sample(1:10)          
 [1]  4  7 10  6  9  2  8  3  1  5
> sample(1:10)
 [1]  2  3  4  1  9  5 10  8  6  7
#


有放回的取样，设置参数replace = TRUE，因为有放回，所以可以看到数字内部有重复：
> ## Sample w/replacement
> sample(1:10, replace = TRUE)  
 [1] 2 9 7 8 2 8 5 9 7 8
#







2.Simulation 小结
- 使用R中的函数从指定的概率分布中取样
使用rnorm(), rpois(), rbinom()等。

常用分布包括正态分布（normal）、泊松分布（poission）、二项分布（binomial）、指数分布（exponential）、伽马分布（gamma）等。

- sample函数可用来从指定向量中随机取样
- 无论何时记得用seed生成种子



ref:
https://mp.weixin.qq.com/s/6VrRjH4tx6VSqNIJ8qw7Kg

视频课程 R Programming by Johns Hopkins University: https://www.coursera.org/learn/r-programming/home/welcome

讲义 Programming for Data Science: https://bookdown.org/rdpeng/rprogdatascience/



========================================
2.2 常见的统计函数
----------------------------------------
1.统计函数   作用
max(x) 返回向量x中最大的元素
min(x) 返回向量x中最小的元素
which.max(x) 返回向量x中最大元素的下标
which.min(x) 返回向量x中最小元素的下标
mean(x) 计算样本(向量)x的均值
median(x) 计算样本(向量)x的中位数
mad(x) 计算中位绝对离差
var(x) 计算样本(向量)x的方差
sd(x) 计算向量x的标准差
...More(汤银才 P41)


(1)Median Absolute Deviation
#绝对中位差实际求法是用原数据减去中位数后得到的新数据的绝对值的中位数。
#但绝对中位差常用来估计标准差，估计标准差=1.4826*绝对中位差。

#R语言中返回的是估计的标准差。
#例如：原数据{2，3，4，5，6}中位数4，新数据{2，1，0，1，2}，即{0，1，1，2，2}。
#所以中位数是1。也就是绝对中位差是1.

#而R返回的是1*1.4826=1.4826

#mad(c(2,3,4,5,6))[1] 1.4826
#R语言里的绝对中位差还要乘上一个比例因子：constant=1.4826
#也可以不乘
mad(c(2,3,4,5,6), constant=1) #1
mad(c(2,3,4,5,6,100), constant=1) #[1] 1.5
#也就是相对于标准差，mad对少量异常值不太敏感
#https://www.zhihu.com/question/56537218/answer/163638714






========================================
R常见用途: 探索性数据分析、统计推断、回归分析、机器学习、可视化报告 //todo
----------------------------------------
https://www.imooc.com/video/8778


1. 发布平台
(1)github: 发布各种源代码，写作编程等；

(2)RPubs: https://www.rpubs.com/ 貌似是RStudio的产品。
Easy web publishing from R
Write R Markdown documents in RStudio.
Share them here on RPubs. (It’s free, and couldn’t be simpler!)


Prerequisites
You'll need R itself, RStudio (v0.96.230 or later), and the knitr package (v0.5 or later).

Instructions
1)In RStudio, create a new R Markdown document by choosing File | New | R Markdown.
选择R markdown和notebook没区别，个人倾向于使用后者。

2)Click the Knit HTML button in the doc toolbar to preview your document.
注意，保存时不要输入后缀名，会自动生成.Rmd后缀名。

3)In the preview window, click the Publish button.
打开一个新网页，需要输入用户名和密码。
不能更新！！只能删掉旧的，重新上传。
删除方法是打开web页，左下角可以删除。



(3)申请用户名:
https://www.rpubs.com/dawnEve
测试页:
http://rpubs.com/dawnEve/test001
http://rpubs.com/dawnEve/test002



(4)搜索方法：
google搜索“??+RPubs”即可。(如: heatmap rpubs)
找到: https://www.rpubs.com/tskam/heatmap




2. 探索性数据分析 - 其实就是作图
https://github.com/angelayuan/Exploratory-Data-Analysis

散点图
柱状图
折线图


3. 统计推断
基于数据得到正式结论的过程，从不确定和抽样中，得到有一定可信度(错误率控制在5%以内)的结论。
例子: 药效


4. 回归分析
线性模型拟合数据
- 预测变量
- 结果变量

父母的身高，预测孩子的身高。


5. 机器学习
训练模型 + 预测
例子: 分类模型。

机器学习是一门高深的学问，没有深入研究，不可能实现。
不过R包可以大大降低使用机器学习的门槛。

# BiocManager::install("caret")
library(caret)

# 训练集建立模型，测试集测试效果
inTrain=createDataPartition(y=training$classe, p=0.7, list=F)
training_set=training[inTrain,]
cv_set=training[-inTrain,]

# 使用训练集，建立随机森林
rf_fit=randomForest(class~., data=training_set)

#

#


#


========================================
1.事件与概率，及概率曲线
----------------------------------------
1. 概念的定义

(1)随机事件
 
条件相同可以重复进行
结果多样
实验前不知道结果

比如，丢色子；


(2)样本空间
所有事件的基本结果，用 欧米伽Ω 表示。

比如：丢6面色子有6个结果。
但是偶数面朝上，包含{2,4, 6}三个基本结果。





========================================
|-- f(x)=F'(x): 概率密度函数; F(x)=P(X<x)概率分布函数
----------------------------------------
对于密度函数f(x)，F(x)不可导的地方都取0；
对于F(x)，分界线等于值都给到大于，因为是右连续的。



1. f(x)=F'(x): 概率密度曲线(probability density curve) pdf曲线 /density
钟形曲线，根据情况带偏斜程度。


在数学中，连续型随机变量的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。

随机数据的概率密度函数：表示瞬时幅值落在某指定范围内的概率，因此是幅值的函数。它随所取范围的幅值而变化。


https://www.jianshu.com/p/70b188d512aa
图中曲线称为概率密度函数, 简称为pdf
pdf的曲线下面积(AUC)总和为1
曲线末端不会接触到x轴(换句话说, 我们不可能100%的确定某件事)

 







############
2. F(x)=P(X<=x): 概率分布函数; 概率累积曲线(probability cumulative curve) CDF曲线 /stat_ecdf

从0逐步上升到1的曲线。
对于每个观测值，一个经验累计分布函数（empirical cumulative distribution function，ECDF）显示小于该值的点的百分比。由于点的个数是有限的，经验累计分布函数是一个阶梯函数。

#概率累积曲线(probability cumulative curve)亦称粒度概率图，是一种在概率坐标纸上作出的累积曲线。
#概率纸上的纵座标是概率分度的百分数值，横座标是(等差的)算术分度的φ值，它通常是由若干直线段组成。


作用：累积分布函数是概率密度函数的积分，能完整描述一个实随机变量X的概率分布。
累积分布函数一般以大写CDF标记，与概率密度函数 probability density function（小写pdf）相对。
互补累积分布函数（complementary cumulative distribution function、CCDF），是对连续函数，所有大于a的值，其出现概率的和。


累计分布函数（The Cumulative Distribution Function, CDF）:在x点左侧事件发生的总和。
累积分布函数表示：
F(a)=P(x<=a)
对离散变量而言，所有小于等于a的值出现概率的和



实例
(1)准备数据
#https://bbs.pinggu.org/thread-3732035-1-1.html
data1=data.frame(
  id=seq(1,14),
  value=c(10,15,20,22,25,27,26,28,31,31.9,32,32.5,40,45),
  group=c(rep( c('A','B'),7))
)
data1


(2)
# https://blog.csdn.net/vv_eve/article/details/96457415
library("latticeExtra")
ecdfplot(~value|group, data=data1)
ecdfplot(~value, data=data1,group=group) #增得快，表示x轴这个位置数据多
#斜率最大的地方表示，概率密度曲线的峰值，也就是p值最大的点(看x坐标值)。

ref:
##原例
ecdfplot(~ gcsescore | factor(score), data = Chem97, 
         groups = gender, auto.key = list(columns = 2),
         subset = gcsescore > 0, xlab = "Average GCSE Score",
         main=" Figure 3.8 ")
#ecdfplot-->累积概率密度图; 
#subset函数,从某一个数据框中选择出符合某条件的数据或是相关的列



(3)#ggplot()+stat_ecdf()
library(ggplot2)
ggplot(data1,aes(x=value,color=group))+stat_ecdf()




ref:
https://baike.baidu.com/item/%E7%B4%AF%E7%A7%AF%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0/7763383?fr=aladdin
https://blog.csdn.net/wangyj705/article/details/81976455





========================================
2.一维随机变量及其分布
----------------------------------------

常见的一维随机变量的分布：
离散型：二项分布-负二项分布、泊松分布、几何分布；
连续型：均匀分布、正态分布、指数分布；






========================================
|-- 离散型 之 泊松分布 Poisson distribution
----------------------------------------
关于泊松分布，不算特别好理解。可以参考资料：
http://www.ruanyifeng.com/blog/2013/01/poisson_distribution.html
http://maider.blog.sohu.com/304621504.html

1. 泊松分布主要满足3个条件：
（一）A为小概率事件
（二）A发生概率是稳定的
（三）A与下一次A事件的发生，是相互独立的

#泊松分布适合于描述单位时间（或空间）内随机事件发生的次数（事件发生的次数只能是离散的整数）。
#如某一服务设施在一定时间内到达的人数，电话交换机接到呼叫的次数，汽车站台的候客人数，
#机器出现的故障数，自然灾害发生的次数，一块产品上的缺陷数，显微镜下单位分区内的细菌分布数等等。

#P(X=k)=lambda^k/k!*e^lamda, k=0,1,2
#对于泊松分布而言，其均值和方差是相等的。而我们的测序数据，通常方差比均值还大，怎么办？

#λ是波松分布所依赖的唯一参数。 λ值愈小分布愈偏倚， 随着λ的增大 ， 分布趋于对称。 
#当λ=20时分布接近于正态分布；当λ=50时， 可以认为波松分布呈正态分布。



(2)Density, distribution function, quantile function and random generation for the Poisson distribution with parameter lambda.
dpois(x, lambda, log = FALSE)
ppois(q, lambda, lower.tail = TRUE, log.p = FALSE)
qpois(p, lambda, lower.tail = TRUE, log.p = FALSE)
rpois(n, lambda)
lower.tail = FALSE允许在默认情况下获得更精确的结果，lower.tail = TRUE将返回1


#产生泊松分布的点
points=rpois(300, lambda=4) #points
plot(density(points))



(3)#泊松分布 (poisson)
library(RColorBrewer)
display.brewer.all()
#
par(mar=c(5, 4, 4, 5))
n=6
color2=brewer.pal(n,"Dark2") #11个
#
for(i in seq(1,n) ){
  print(i)
  if(i==1){
    plot(dpois(0:15, lambda = 1), col = color2[i], xlim = c(-1,15), type="o",
         pch=i,
         xlab = "n", ylab = "Probability", main = "Poisson Distribution")
  }else{
    lines(0:15, y=dpois(0:15, lambda = i),col =color2[i], type="o",pch=i)
  }
}
#
legend("topright", legend=seq(1,n), 
       #inset=-0.15,xpd=TRUE,
       box.col="white",
       lty=1, pch=seq(1,6), col=color2)
box()
###end





2.一家医院，统计下来平均每分钟接待2个客人，问假设某次一分钟接待4个客人的概率是：
dpois(4, lambda = 2) #[1] 0.09022352
其中，参数2为泊松分布公式中的λ * t

泊松分布概率分布律图：
plot(dpois(0:30, lambda = 2), col = "red", xlim = c(-1,30), xlab = "发生次数", ylab = "概率", main = "泊松分布图")





3. 假设检验
poisson.test(x, T = 1, r = 1, alternative = c(“two.sided”, “less”, “greater”), conf.level = 0.95)

例: 
poisson.test(137, 24.19893)

##
## 		Exact Poisson test
## 
## data:  137 time base: 24.19893
## number of events = 137, time base = 24.199, p-value < 2.2e-16
## alternative hypothesis: true event rate is not equal to 1
## 95 percent confidence interval:
##  4.753125 6.692709
## sample estimates:
## event rate 
##   5.661407 





4. 基因表达量的分布不符合泊松分布。

横坐标为基因在所有样本中的均值，纵坐标为基因在所有样本中的方差，直线的斜率为1，代表泊松分布的均值和方差的分布。
可以看到，真实数据的分布是偏离了泊松分布的，方差明显比均值要大。

如果假定总体分布为泊松分布， 根据我们的定量数据是无法估计出一个合理的参数，能够符合上图中所示分布的，这样的现象就称之为over dispersion。

由于真实数据与泊松分布之间的overdispersion，选择泊松分布分布作为总体的分布是不合理。

以上只证明了泊松分布是个不太恰当的分布估计，那怎么证明负二项分布就是合适的分布估计呢？



refer:
https://blog.csdn.net/qq_33335484/article/details/80389807



========================================
|-- 离散型 之 二项分布与负二项分布 (negative binomial)
----------------------------------------
1. 二项分布

dbinom(x, size, prob, log = FALSE)
pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
rbinom(n, size, prob)

二项分布描述的是n重伯努利实验，在n重贝努利试验中，事件A恰好发生x(0≤x≤n)次的概率为：
Pn(x)=C(n,x)*p^x*(1-p)^(n-x)
参数n和p

#pic1
library(RColorBrewer)
display.brewer.all()
color2=brewer.pal(n,"Dark2")
i=2
plot(dbinom(0:100, size= i*20, prob=0.3), col = color2[i], xlim = c(0,25), type="o",
     pch=i,
     xlab = "n", ylab = "Probability", main = "Binomial Distribution")
text(20,0.12,"size=40,prob=0.3",col=color2[i])



#pic2
data=rbinom(n=10000,size=40,prob=0.3)
plot(density(data))









2.负二项分布

dnbinom(x, size, prob, mu, log = FALSE)
pnbinom(q, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
qnbinom(p, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
rnbinom(n, size, prob, mu)

负二项分布描述的也是伯努利实验，不过它的目标事件变成了：
对于Bernoulli过程，我们设定，当某个结果出现固定次数的时候，整个过程的数量，比如我们生产某个零件，假设每个零件的合格与否都是相互独立的，且分布相同，那么当我们生产出了r=5个不合格零件时，一共生产了多少合格的零件，这个数量就是一个负二项分布，公式如下：
f(k;r;p)=P(x=k)=C(r+k-1, k)*p^k*(1-p)^r

该公式描述的是，在合格率为p的一堆产品中，进行连续有放回的抽样，当抽到r个次品时，停止抽样，此时抽到的正品正好为k个的概率
它的概率分布如下：

#pic1
library(RColorBrewer)
display.brewer.all()
color2=brewer.pal(n,"Dark2")
i=2
plot(dnbinom(0:100, size= i*1, prob=0.1), col = color2[i], xlim = c(0,65), type="o",
     pch=i,
     xlab = "n", ylab = "Probability", main = "negative binomial distribution")
text(30,0.03,"size=2,prob=0.3",col=color2[i])

#pic2
data=rnbinom(n=10000,size=2,prob=0.1)
plot(density(data))



负二项回归。同泊松计数回归不同是，在负二项回归中假设方差大于均值，这种情形为通常称为过度离散(over dispersion)，相反，若方差小于均值，称之为低扩散(under dispersion)。负二项回归可以有效地对过离散数据建模，

负二项分布的均值和方差分别为:
mu=p*r/(1-p)
Sigma的平方  σ^2=p*r/(1-p)^2

将p用mu表示，得到：
p=mu/(mu+r)
1-p=r/(mu+r)

将上一步推出的p和1-p带入到方差的表达式中，得到
σ^2=p*r/(1-p)^2=mu^2/r + mu

记1/r=alpha，则
σ^2 = mu + alpha*mu^2 > mu

可见，方差是均值的二次函数，方差随着均值的增加而进行二次函数形式的递增，正好符合上文。
其中alpha和r称为dispersion parameter


负二项分布与泊松分布的关系，可以用α或r推出：
当 r -> ∞ 时，α -> 0，此时 σ2= μ，为泊松分布；
当 r -> 0 时，α -> ∞，此时 overdispersion



(2)
同泊松计数回归不同是，在负二项回归中假设方差大于均值，这种情形为通常称为过度离散(overdispersion)，相反，若方差小于均值，称之为低扩散(underdispersion)。

易见，这个负二项回归模型的方差为均值的二次函数，因此也被称为NB2模型。当时，NB2模型退化为泊松回归模型。负二项回归的系数可通过牛顿迭代法获得，在R中利用MASS包的glm.nb函数求取。最后一个问题，对于计数型数据集，如何判断它是否过度离散。因为在NB2中

σ^2=mu =alpha*mu^2

而在泊松回归中均值和方差相等，因此只需判断是否等于0，可通过R中的AER::dispersiontest函数实现。
具体的推导过程可参见Cameron和Trivedi在1990对这一问题的论述。


案例:
MASS包中的 quine 数据集描述了新南威尔士农村学校旷课情况，该数据集记录146个学生的宗教信仰(Eth), 性别(Sex),年龄(Age) ,学习状态(Lrn),旷课天数(Days)五个指标。我们想通过这一数据集合研究旷课天数与其他因素之间的关系。

library(MASS)
library(AER)
library(COUNT)

首先检查数据是否过度离散，先进行泊松回归，然后使用dispersiontest函数对泊松回归结果进行检测，看alpha是否大于0，注意需设置dispersiontest函数中的trafo=1，否则认为alpha大于1过离散。

po <- glm(Days ~ Sex+Age + Eth +Lrn, data = quine,family=poisson)
dispersiontest(po,trafo=1)
## 	Overdispersion test
## 
## data:  po
## z = 5.469, p-value = 2.263e-08
## alternative hypothesis: true alpha is greater than 0
## sample estimates:
##    alpha 
## 11.53013

过离散测试中alpha=11.53013,因此泊松回归不能有效反映这组数据真实特征，下面采用负二项回归研究这组数据

nb <- glm.nb(Days ~ Sex+Age + Eth +Lrn, data = quine)
#
list(nb=unlist(modelfit(nb)), po=unlist(modelfit(po)))
## $nb
##         AIC        AICn         BIC       BICqh 
## 1109.151018    7.596925 1130.036265    7.687628 
## 
## $po
##        AIC       AICn        BIC      BICqh 
## 2299.18363   15.74783 2320.06888   15.83854 

结果表明，对于quie数据集，在各项指标下，负二项回归都远比泊松回归好。

> nb
## 
## Call:  glm.nb(formula = Days ~ Sex + Age + Eth + Lrn, data = quine, 
##     init.theta = 1.274892646, link = log)
## 
## Coefficients:
## (Intercept)         SexM        AgeF1        AgeF2        AgeF3         EthN        LrnSL  
##     2.89458      0.08232     -0.44843      0.08808      0.35690     -0.56937      0.29211  
## 
## Degrees of Freedom: 145 Total (i.e. Null);  139 Residual
## Null Deviance:	    195.3 
## Residual Deviance: 168 	AIC: 1109






3. 方差估计
在生物学重复很少时，我们是很难准确计算每个基因表达的标准差的（相当于这个数据集的离散程度）。我们很可能会低估数据的离散程度。

被逼无奈的科学家提出了一个假设：表达丰度相似的基因，在总体上标准差应该也是相似的。我们把不同生物学重复中表达丰度相同的基因的总标准差取个平均值，低于这个值的都用这个值，高于这个值的就用算出来的值。

（图来自 H. J. Pimentel, et al. Differential analysis of RNA-Seq incorporating quantification uncertainty. bioRxiv, 2016）



(2)
然后为了建模，本来需要估计负二项分布的两个值，均值和方差，但是发现方差其实可以用均值和dispersion表示，于是就只要求均值和dispersion就行了。有了这两个值，就开以进行广义线性模型建模，就可以搞p值，找差异基因对吧。

另外，还有一个比较重要的值叫做Log Fold Change,这个受count数影响很大，原因是count数据是方差不齐性的，方差跟他的count均值有很大关系。

如果你的样本没有重复，那么你只要自己给每个基因都来一个dispersion就能算你最可爱的p值了，然后在低count，高LFC的基因留个心眼就行了。













refer: 
1.https://www.jianshu.com/p/ad24bb90b972
2.https://mp.weixin.qq.com/s/UTmSzCgDIFYbG2WByzaqQQ / http://www.jintiankansha.me/t/QicEQRzHra
3.转录组差异表达筛选的真相
https://mp.weixin.qq.com/s/VcjnvI5FqwOFEC9wSUfdSw

4.数萃大数据 2018-09-16 数据分析师应该知道的16种回归方法：负二项回归





========================================
|-- 离散型 之 几何分布 X~G(p)
----------------------------------------
几何分布：事件成功的概率为p，尝试第k次时才第一次成功，则k的分布符合几何分布。
P(X=k)=p*(1-p)^(k-1); (k=1,2,3,...)

1.产生几何分布随机变量(R中的几何分布k是成功前的实验次数，从0开始，要转换成一般定义的几何分布的k)
k=rgeom(1000,0.5)+1;k
hist(k)



2.




========================================
|-- 连续型 之 均匀分布 X~U(a,b)
----------------------------------------




========================================
|-- 连续型 之 指数分布 X~E(lambda)
----------------------------------------
1.
密度函数
f(x)={ lambda*e^(-lambda*x), x>=0; 0, x<0

分布函数:
F(x)={0, x<0;  1-e^(-lambda*x), x>=0;


##
2. 等价转换




========================================
|-- 连续型 之 正态分布( normal distribution /Gaussian又叫高斯分布 X~N(mu,sigma^2) ) 与 over dispersion(过度离散) 
----------------------------------------
1. 正态分布

dnorm(x, mean = 0, sd = 1, log = FALSE)
pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
rnorm(n, mean = 0, sd = 1)


#pic0
x <- seq(-4, 4, length.out = 100)
p <- dnorm(x, mean = 0, sd = 1)
plot(x, p, type = 'l', lwd = 2, col = "red",
     xlab = "Residual", ylab = "Density")
#


#pic1
library(RColorBrewer)
display.brewer.all()
color2=brewer.pal(n,"Dark2")
i=2
plot(dnorm(0:100, mean= i*10, sd=4), col = color2[i], xlim = c(0,100), type="o",
     pch=i,
     xlab = "n", ylab = "Probability", main = "Normal Distribution")
text(40,0.095,"mean=20,sd=4",col=color2[i])
#


#pic1.2
x <- seq(0, 40, length.out = 100)
p=dnorm(x, mean= i*10, sd=4)
plot(x,p, col = color2[i], xlim = c(0,40), type="l", lwd=2,
     xlab = "n", ylab = "Probability", main = "Normal Distribution")
text(28,0.095,"mean=20,sd=4",col=color2[i])
#


#pic2
data=rnorm(n=10000,mean=20,sd=4)
plot(density(data))
#






2. What does under or over-dispersion look like?
https://www.r-bloggers.com/what-does-under-or-over-dispersion-look-like/

(1) 正态分布
x <- seq(-4, 4, length.out = 100)
p <- dnorm(x, mean = 0, sd = 1)
plot(x, p, type = 'l', lwd = 2, col = "red",
    xlab = "Residual", ylab = "Density")
#

(2)更离散的分布， add an over-dispersed curve using the Student t distribution
p_t <- dt(x, df = 1.4, ncp = 0)

plot(x, p, type = 'l', lwd = 2, col = "red",
    xlab = "Residual", ylab = "Density")
lines(x, p_t, col = "darkblue", lwd = 2)

(3)更不离散的分布draw an under-dispersed distribution, using the Laplace distribution
library(rmutil)
p_l <- dlaplace(x, m = 0, s = 0.4)


(4) 以上三个图合起来
plot(x, p_l, xlim = c(-4, 4), lwd = 2, xlab = "Residuals", main = "", type = "l")
lines(x, p, col = "red", lwd = 2, lty = 2)
lines(x, p_t, col = "darkblue", lwd = 2)
legend("topright", legend = c("normal", "under-dispersed", "over-dispersed"),
      lty = c(2,1,1), col = c("red", "black", "darkblue"), lwd = 2)
#

(5) 画QQ图是检查是否符合正态分布的重要标准(注意：怎么理解QQ图？)
plot them on a normal QQ plot, which you may be familiar with. It is
one of the standard checks of model residuals:

set.seed(1997)
par(mfrow = c(1,3))
qqnorm(rnorm(1000, mean = 0, sd = 1), main = "normal")
abline(0,1)
qqnorm(rt(1000, df = 1.4, ncp = 0), main = "over-dispersed")
abline(0,1)
qqnorm(rlaplace(1000, m = 0, s = 0.4), main = "under-dispersed")
abline(0,1)

# 我的理解：QQ图取的是样品和理论值的百分位数字。
第一个值1%位置就是最小的值，过度离散的样品由于扁平分布，很小的地方(-60)还有值，而正态分布在-3位置已经是最小值了。
在100%分位数是最大值，过度离散样本最大值很大(250)，而正态分布最大值在3左右。
中间的部分则比较接近，在0的位置是相等的。


#改变参数，能加深对结果的理解。

#####
# 增大Laplace分布的scale值，它会变成过离散的。因为两侧尾巴相对正态分布越来越胖(虽然中位数还有峰)。
scale for the Laplace distribution to a larger number it will become over-dispersed, 
because it gets fatter tails than the normal (despite being more peaked at its mode).


#####
# 低离散比你想象的更常见。只要有审核过程，就有可能发生。
Under-dispersion is more common than you might think. It can occur when you have a censoring process. 
比如你的机器只能测试某个精度的值，更小的值四舍五入为0了。
For instance, perhaps your machine can only measure length’s to a certain precision and any distance that is to small gets rounded down to zero.





3. Overdispersion
https://en.wikipedia.org/wiki/Overdispersion

When the observed variance is higher than the variance of a theoretical model, overdispersion has occurred. 
当观察到的变异高于理论模型时，过离散就出现了。

Conversely, underdispersion means that there was less variation in the data than predicted.
相反，低离散意味着观察到的变异比期望的更少。

在应用数据分析中，过度离散是一个非常常见的特征，因为在实践中，总体常常是异质性的(非均匀的)，这与广泛使用的简单参数模型中隐含的假设相反。








========================================
|-- z-标准化 / Z-normalization
----------------------------------------

https://jmotif.github.io/sax-vsm_site/morea/algorithm/znorm.html

公式
x′i=(xi−μ)/σ, where i∈N


R代码：
z-normalization can be coded as a simple R function:

znorm <- function(ts){
  ts.mean <- mean(ts)
  ts.dev <- sd(ts)
  (ts - ts.mean)/ts.dev
}

s1_znorm=znorm(series1)
s2_znorm=znorm(series2)








========================================
|-- qqplot和qqnorm有什么区别
----------------------------------------
1.QQ图的主要作用是判断样本是否近似于某种类型的分布，这里的“QQ”是两个Quantiles的大写字母，即两个分位数，一个是样本分位数（Sample Quantiles），一般画在纵轴，一个是理论分位数（Theoretical Quantiles），一般画在横轴。

n <-rnorm(1000, 0, 1) #生成1000个均值为0，方差为1的正态分布样本
qqnorm(n) #做变量n的QQ图
abline(0,1) #画一个截距是0，斜率是1的直线

这个（x,y）形式的二维的散点图的坐标是如何确定的?
要回答这个问题需要先从qqnorm()入手，把QQ图的结果保存到变量中，再对这个变量进行分析。

# q是一个列表变量
q<- qqnorm(n) # 将变量n的QQ图的结果保存到变量q中
str(q)
q$x[1:5]
q$y[1:5]

变量q$y是样本分位数，也是刚才随机产生的1000个样本的值，即和变量n是完全一样的，为了验证，如果运行下面的代码，会得到1000个“TRUE”

table(n == q$y)

接下来讨论这个q$x，q$x是理论分位数，之所以是“理论”，指的是如果按照正态分布的假设（qqnorm对应的是正态分布假设，qqpois对应的是泊松分布的假设），这个分位数的理论值应该是多少。

例如如果有5个样本，那么理论分位数应该给出的是20%，40%，60%，80%，100%的分位数，但是100%的分位数一般会无限大，因此在这里需要进行一下数据处理，找一个“近似”的理论样本，来替代“真正”的理论样本。在这里问题就在这个“近似”的理论样本的数据处理方法。举个例子，假设P()为正态分布函数，是正态分布函数的反函数，假设一共有N个样本，则第n个样本的“真正”的理论样本分位数应该是 


直线由四分之一分位点和四分之三分位点这两点确定的，四分之一分位点的坐标中横坐标为实际数据的四分之一分位点(quantile(data,0.25)),纵坐标为理论分布的四分之一分位点(qF(0.25)),四分之三分位点类似，这两点就刚好确定了QQ图中的直线。

##理论x1=qnorm(0.25,0,1)
##实际y1=quantile(n,0.25)
qqnorm(n)
# points(qnorm(0.25,0,1), quantile(n,0.25), col='red',type="p",lwd=5)
x2=c(qnorm(0.25,0,1),qnorm(0.75,0,1))
y2=c(quantile(n,0.25),quantile(n,0.75))
reg=lm(y2~x2)
reg
abline(reg,col='blue')


//todo?? 公式不清楚




## N=100#设定样本数量，可以将100改为别的数进行验证
d <- matrix(nrow= N ,ncol= 3)#生成一个N *3的矩阵
d[,1]<-1:N #给每一次N的值编上序号
for(i in 2:N){ #从i=2开始计算，一直计算到i=N
  x <- rnorm(i,0,1) #生成均值为0,、方差为1的i个数的样本，保存到x
  y <- qqnorm(x) #生成样本x的QQ图，保存到变量y
  yy <- sort(y$x) #将QQ图中的理论样本值保存到变量yy中
  yyy<- pnorm(yy,0,1) # 计算分位数yy的概率，保存到yyy中
  a <- 1:i
  ayyy<-lm(a~yyy) #计算公式“ayyy=a*系数+截距”中的参数
  b <-summary(ayyy)$coefficients
  d[i,2]<-b[1,1] #将截距保存到第二列
  d[i,3]<-b[2,1] #将系数保存到第三类
}
d






########

qqplot应该是两样本的正态性对比，而qqnorm应该是样本与样本期望的正态性对比。
在R里面如果单是qqplot(x)，其中x为一组样本，运行时会出错的，但qqnorm(x)就不会……

其实关于这个问题我也不是很确定，只是之前尝试得到的一些经验。具体你可以查R里面的help()，那里对于两者的参数设置会比较详细


refer:
http://blog.sina.com.cn/s/blog_5eb5c9370102vyim.html







========================================
|-- lognormal-like ditribution 及其平均数(几何平均值)的计算
----------------------------------------

1.画分布图
(1)
# 分布符合lognormal-like ditribution, 则要求几何平均数
hist(rlnorm(1000, meanlog = 0.1, sdlog = 0.500), n=300)
hist(rlnorm(1000, meanlog = 0.1, sdlog = 1.00), n=300)
hist(rlnorm(1000, meanlog = 0.1, sdlog = 2.00), n=300)

#
hist(rlnorm(1000, meanlog = 0.1305, sdlog = 0.018), n=300)



(2)
to create a lognormal distribution, you can use the property that if X follows a log-normal distribution and Y = ln(X), then Y follows a normal distribution.

(A positive random variable X is log-normally distributed if the logarithm of X is normally distributed.)
https://en.wikipedia.org/wiki/Log-normal_distribution

plot_lnormal=function(sd=0.5){
  set.seed(1234)
  dist <- rnorm(1000, 1, sd)
  ldist <- exp(dist)
  hist(ldist)
}
plot_lnormal(sd=0.1)
plot_lnormal(sd=0.5)
plot_lnormal(sd=1)
plot_lnormal(sd=1.5)
#





2.几何平均数的计算公式

(1) 几何平均数分为简单几何平均数和加权几何平均数两种。
1)简单几何平均数
G = (X1*X2*...*Xn)^(1/n);

2)加权几何平均数
G = (X1^f1 * X2^f2 * ... *Xn^fn)^(1/(f1+f2+...+fn));


(2) 使用R语言计算
G = (X1*X2*...*Xn)^(1/n);
取对数后
ln(G)=1/n * ln(X1*X2*...*Xn)= mean(ln(X))
相当于求原变量的对数的算术平均值。

也就是 G=e^ mean(ln(X))
这在R中很容易实现。


例子:
已知某市2010~2014年国内生产总值的增长率（以上1年为1）分别为12%、8%、14%、16%和13%，试计算该市5年的平均增长率

编写R程序：
x <- c(12,8,14,16,13)/100
tmp <- mean(log(1+x)) #防止遇到0吗？
re <- exp(tmp) - 1
print(re) #0.1256843

# 测试增加后来又减去的的这个值，对最终结果有啥影响
testG=function(x,plusN=1){
  tmp <- mean(log(plusN+x))
  exp(tmp) - plusN
}
testG(x,0) #0.1228266 这个才是什么都不加的真实值
testG(x) #0.1256843
testG(x,10) #0.1259652
testG(x,100) #0.1259965
testG(x,1000) #0.1259996
testG(x,1000000) #0.126 极限值

# 倾向于直接算，如果有0的，跳过去。


例2：
geometry.mean <- exp(mean(log(x)))

几何平均数实现算法，要考虑到NA或负值
geo_mean <- function(data) {
    log_data <- log(data)
    gm <- exp(mean(log_data[is.finite(log_data)]))
    return(gm)
}
# https://stackoverflow.com/questions/2602583/geometric-mean-is-there-a-built-in






ref:
https://stackoverflow.com/questions/54768902/lognormal-stock-price-distribution-in-r










========================================
3.二维随机变量及其分布
----------------------------------------



三、 常见的二维随机变量
(1) 均匀分布
(X,Y) ~ f(x,y)={ 1/A, (x,y)属于D;  0, (x,y)不属于D; }

(2) 二维正态分布
(X,Y) ~ N(mu1,mu2,sigma1^2, sigma2^2, rou)
则，
X ~ N(mu1, sigma1^2)
Y ~ N(mu2, sigma2^2)
反之不成立。



(3)推论:
如果
X ~ N(mu1, sigma1^2)
Y ~ N(mu2, sigma2^2)
且X和Y独立，
则线性组合 aX+bY ~ N(a*mu1+b*mu2, a^2*sigma1^2+b^2*sigma2^2)





四、
联合概率分布，边缘概率分布：边缘就是不管另一个变量(相当于另一个变量从-无穷大到+无穷大)。


求密度函数的方法：可以先求分布函数，再求导数(不可导的地方都取0)。


见微积分Gama函数性质
(2)gama 函数
Gama(alpha)=积分 0 到+无穷大 x^(alpha-1) * e^(-x) dx
性质:
Gama(alpha+1)=alpha*Gama(alpha);
Gama(n+1)=n!
Gama(1/2)=根号 Pi

常用：积分0到+无穷大 e^(-x) dx= 积分0到+无穷大 x^0 * e^(-x) dx= Gama(1)=0!=1




五、 等价转换
1. P{min(x,2)<=y}=1-P{min(x,2)>y}=1-P{x>y, 2>y}
因为按照2分界点进行讨论时，2>y不是必然事件，就是不可能事件。
又因为必然事件和不可能事件和任何事件都独立
	- 独立的判定标准就是P(AB)=P(A)P(B)
所以：
P{min(x,2)<y}=1-P(x>y)P(2>y)
(1) y<=2时，P(2>y)=1
P{min(x,2)<y}=1-P(x>y)*1=P(x<=y) ### 习惯上往F(x)和f(x)上靠近。
=Fx(y) #就是Fx(x)在x=y点的分布函数值
(2) y>2时， p(2>y)=0
P{min(x,2)<y}=1-0=1;




========================================
4.随机变量的数字特征(常见的4个): 数学期望、方差、协方差与相关系数
----------------------------------------
1. 数学期望 


(1)定义：
1)一维离散型 P(X=xi)=pi, E(X)=累加(Xi*pi)

2)一维连续型 X~f(x), E(X)=积分(-无穷大, +无穷大) x*f(x) dx;
如果 Y=T(X),则 E(Y)=E(T(x))=积分(-无穷大, +无穷大) T(x)*f(x) dx;

应用: E(X^2)=积分(-无穷大, +无穷大) x^2*f(x) dx;

3) 二维离散：
E(X+Y)=累加(i=1,m) 累加(j=1,n) (xi+yj)*pij;
E(X^2+Y^2)=累加(i=1,m) 累加(j=1,n) (xi^2+yj^2)*pij;

4) 二维连续：
(X,Y)~f(x,y),
如果 Z=T(X,Y)
则 E(Z)=积分(-无穷大,+无穷大) dx 积分(-无穷大,+无穷大) T(x,y)*f(x,y) dy




(2)数学期望的性质
1) E(C) = C。 
2) E(aX + bY) = aEX + bEY 。
   E(X +Y) = EX + EY
   E(kX ) = kEX 。 
3) 若随机变量X,Y 相互独立，则E(XY) = EX * EY 。反之不成立。





2. 方差：反应变异程度
为了反应波动，求出变量和期望的差，为了防止这些差值正负抵消，可以求绝对值或者平方，因为绝对值会产生分段函数，不可导，所以使用平方更好。


(1)方差的定义
方差的定义 DX = E(X − EX )^2
方差的计算公式 DX = E(X^2) − (EX)^2。

(2)方差的性质
1) D(C) = 0。 
2) D(k*X ) = k^2 * DX 。
3) 设随机变量X,Y 相互独立，则
D(aX + bY) = a^2*DX + b^2*DY;

D(X + Y) = DX + DY
D(X - Y) = DX + DY





3. 常见随机变量r.v. 的EX和DX
(1) X~B(n,p) 二项分布，n重伯努利实验，某结果出现的次数k=0,1,2,...,n;
P{X=k} = C(n,k) * p^k * (1-p)^(n-k)
EX=np;
DX=npq;

(2) X~Pi(lambda) (lambda>0) 泊松分布
P{X=k}=lambda^k * e^(-lambda) / k! (k=0,1,2,...)
EX=lambda;
DX=lambda;
## 好奇怪！泊松分布的数学期望和方差都是lambda!

推导过程：
高数 1+x+ x^2/2! + ... +x^n/n!+...=e^x
EX=累加(k=0, 无穷大) k*L^k*e^(-L) /k! 
  =累加(k=1, 无穷大) L^k*e^(-L) /(k-1)!
  =L*e^(-L) * 累加(k=1, 无穷大) L^(k-1)/(k-1)!
  =L*e^(-L) * e^L
  =L
#
EX^2 = 累加(k=0, 无穷大) k^2 * L^k*e^(-L) /k! #k=0时是0，去掉。
  = 累加(k=1, 无穷大) k^2 * L^k*e^(-L) /k!
  = e^L * 累加(k=1, 无穷大) k*L^k / (k-1)!
  = e^L * 累加(k=1, 无穷大) (k-1 +1)*L^k / (k-1)!
  = e^L * [累加(k=2, 无穷大) (k-1)*L^k / (k-1)! + 累加(k=1, 无穷大) L^k / (k-1)!] #因为第一个累加k=1时为0，所以从k=2开始累加不变。
  = e^L * [ L^2 *累加(k=2, 无穷大) L^(k-2) / (k-2)! + L*累加(k=1, 无穷大) L^(k-1) / (k-1)!]
  = e^(-L)*[ L^2*e^L + L*e^L]
  = L^2 + L
#
DX=EX^2-(EX)^2=L;


(3)均匀分布 X~U*(a,b)
密度函数 f(x)={ 1/(b-a), a<x<b;  0, 其他;
分布函数 F(x)=P{X<=x}=
	0, x<a
	(x-a)/(b-a), a<=x<b
	1, x>=b
#
EX=(a+b)/2;
DX=(b-a)^2 /12;

简单积分就能推导出来；



(4) 指数分布 X~E(lambda) (lambda>0)
密度函数 f(x)={ L*e(-L*x), x>0;  0, x<=0;
分布函数 F(x)={1-e^(-L*x),x>=0;  0,x<0;

EX=1/lambda;
DX=1/lambda^2;

推导过程： 
EX=积分(-无穷大,+无穷大) x*f(x) dx 
  =积分(0,+无穷大) x*lambda*e^(-lambda*x) dx 
  =1/L * 积分(0,+无穷大) x*L*e^(-L*x) d(L*x)
  = 1/L * 积分(0,+无穷大) t*e^(-t) dt 
  = 1/L * Gama(2)
  = 1/L * 1!
  = 1/L
EX^2=积分(0,+无穷大) x^2*lambda*e^(-lambda*x) dx 
  = 1/L^2 * 积分(0,+无穷大) (L*x)^2*e^(-L*x) d(L*x)
  = 1/L^2 * 积分(0,+无穷大) t^2*e^(-t) dt
  = 1/L^2 * Gama(3)
  = 1/L^2 * 2!
  = 2/L^2
DX=EX^2-(EX)^2=2/L^2 - (1/L)^2=1/L^2;

例题: X~E(lambda), P{X>根号DX}=?
= P{X>1/L}=1-P{X<=1/L}=1-F(1/L)
= 1- [ 1-e^(-L*x) ], x=1/L>=0
= e^(-L*1/L)=e^(-1)
=1/e



(5) 正态分布 X~N(mu, sigma^2)
密度函数 f(x)=1/sqrt(2*pi) *e^[ -(x-mu)^2/(2*sigma^2) ]
(X-mu)/sigma~N(0,1)

EX=mu;
DX=sigma^2;







####
4. 协方差与相关系数

(1)cooperation variation 协方差 描述两者之间的变异关系。

DX=E(X-EX)^2 =E(X-EX)(X-EX) 就是每个变量和均值的差，求平方，再求期望。
把第二个改为Y，就是
协方差 Cov(X,Y)=E(X-EX)(Y-EY)

性质：
1) Cov(X,X)=DX;


(2) 相关系数 记为Row(X,Y) 或Cor(X,Y)
Cor(X,Y)=Cov(X,Y)/sqrt(DX*DY)

性质：
Row(X,Y)=0, 则称X和Y不相关;
Row(X,Y)=1, 则称X和Y正相关;
Row(X,Y)=-1, 则称X和Y负相关;


(3)协方差的性质

2) Cov(X,Y)=E(X-EX)(Y-EY)=E(XY-X*EY-Y*EX+EX*EY)
  =E(XY)-EY*EX-EX*EY+EX*EY 
  =E(XY)-EX*EY;
# 这个就是协方差计算公式：2个随机变量的协方差=乘积的数学期望-各自数学期望的乘积。
比定义还好用。

3)Cov(X, aY1+bY2)=a*Cov(X,Y1)+b*Cov(X,Y2)
协方差的结合律。

4)D(X+Y)= Cov(X+Y, X+Y)=Cov(X,X)+Cov(X,Y)+Cov(Y,X)+Cov(Y,Y)
=DX+DY+2*Cov(X,Y)
该公式不要求X和Y互相独立。


更一般的：
D(a*X+b*Y)=a^2*DX+b^2*DY+2abCov(X,Y)


(4) 相关系数的性质
1)abs(Row(X,Y))<=1;
2)Row(X,Y)=0 <=> Cov(X,Y)=0 <=> E(XY)=EX*EY
3)Row(X,Y)=1 <=> P{Y=a*X+b}=1 (a>0) 就是X和Y是线性相关的。
4)Row(X,Y)=-1 <=> P{Y=a*X+b}=1 (a<0) 就是X和Y是线性相关的。

例题：抛硬币n次，正反面分别为X和Y，则Row(X,Y)=?
X+Y=n, Y=-X+n 
所以 P{Y=-X+n}=1
所以 Row(X,Y)=-1;


例1: 射击命中率1/5，命中为止，射击次数x，求EX;
解: k=1,2,3,...
P{X=k}=0.8^(k-1)*0.2
EX=累加(1,无穷) k*0.8^(k-1)*0.2
  =1/5 * 累加(1,无穷) k*0.8^(k-1)
#
S(x)=Sigma(n=1,无穷) k*x^(k-1)
  =Sigma(n=1,无穷) (x^k)' #求导后求和，可以先求和再求导
  =( Sigma(n=1,无穷) x^k )' #几何幂级数
  =( x/(1-x) )'
  =1/(1-x)^2
#
EX=1/5 * S(4/5)=0.2*1/0.2^2=5;

更一般的，几何分布X~G(p)，则EX=p;



例2：从学校到火车站有3个红绿灯，信号灯之间互相独立，遇到红灯的概率是2/5，遇到红灯的次数是X，求EX？
X~B(3,2/5)
EX=np=1.2



例3: X~N(1,4), Y~N(1,9),X和Y独立，Z=|X-Y|，求EZ, DZ？
解: X-Y~N(0,13);
U=(X-Y)/sqrt(13)~N(0,1)

EZ=E(|X-Y|)
  =sqrt(13)*积分(-无穷大,+无穷大) |U|*1/sqrt(2*pi) *e^(-u^2/2) du 
  =2*sqrt(13)*积分(0,+无穷大) U*1/sqrt(2*pi) *e^(-u^2/2) du #偶函数积分的性质
  =2*2*sqrt(13/(2*pi))*积分(0,+无穷大) e^(-u^2/2) d(u^2/2) #积分变换
  =2*sqrt(13/(2*pi))*Gama(1) #Gama函数的性质
  =2*sqrt(13/(2*pi))
#
EZ^2=13*E( (X-Y)/sqrt(13) )^2=13*EU^2
因为U~N(0,1), EU=0,DU=1, 所以EU^2=(EU)^2+DU=1; 所以 EZ^2=13;
#
DZ=EZ^2-(EZ)^2=13- 4*13/(2*pi)=13*(1-2/pi)



问题: (X,Y)~f(x,y), X和Y独立，则X和Y不相关，反之不成立。(不相关推不出独立)
X和Y的独立性，一般强于其不相关性。
也有等价的情况： (X,Y)~N(mu1,mu2,sigma1^2,sigma2^2,Row)，则X和Y 独立<=>不相关。

证明：
1) =>
因为X和Y独立，所以EXY=EX*EY
所以Cov(X,Y)=EXY-EX*EY=0;
Row(X,Y)=Cov(X,Y)/sqrt(DX * DY)=0;

2) <= 推不出来，反例即可
X~U(-1,1), Y=X^2;
EX=积分(-无穷大,+无穷大) x*1/2 dx=0; #奇函数
EXY=EX3=0; #还是奇函数
Cov(X,Y)=EXY-EX*EY=0;
Row(X,Y)=0; 所以X和Y不相关。
#
再看独立性：若F(x,y)=Fx(x)*Fy(y)处处成立，则称X和Y独立。
F(1/2, 1/4)=P{X<=1/2, Y<=1/4}=P{X<=1/2, -1/2<=X<=1/2} #逗号表示且
  =P{-1/2 <=x<=1/2}
  =积分(-1/2, 1/2) 1/2 dx=1/2
#
Fx(1/2)=P{X<=1/2}=积分(1/2, -1) 1/2 dx=3/8
Fy(1/4)=P{Y<=1/4}=P{-1/2<=X<=1/2}=1/2
因为 F(1/2, 1/4)!=Fx(1/2)*Fy(1/4)
所以，不能保证 F(x,y)=Fx(x)*Fy(y)处处成立,
所以，X和Y不独立。



==> 并不是所有的随机变量都有数学期望和方差。
反例: X~f(x)=1/[Pi * (1+x^2)]
满足 f(x)>=0; 积分(-,+无穷大) f(x) dx=1;
但是 EX= 积分(-,+无穷大) x/[Pi * (1+x^2)] dx 不收敛。
也就是没有数学期望，更没有方差。




========================================
5.大数定律和中心极限定理
----------------------------------------

大数定律，就是发生很多次实验的时候，频率趋近于其概率。
抛硬币，10次，正面朝上不一定是0.5，但是实验1万次时，就很接近0.5了。


一、车比雪夫不等式
设随机变量 X 的方差存在，则对任意的ε > 0，有
P{|X-EX|>=ε} <= DX/ε^2 或者 P{|X-EX|<ε} >= 1-DX/ε^2

# 在数学期望周围t范围内，曲线下面积的概率占优势，比DX/t^2大或相等。

#例: X~N(1,4), Y~N(0,16),X和Y独立，用车比雪夫不等式估计P{|X-Y-1|<10}>=___
解: X+Y~N(1,20);
E(X+Y)=1, D(X+Y)=20;
P{|X-Y-1|<10}=P{|X+Y-E(X+Y)|<=10} >= 1- D(X+Y)/10^2=1-20/100=0.8;
#



二、大数定律(3个，注意它们的条件差异)

1、（车比雪夫大数定律）设随机变量X1,X2,...,Xn,... 相互独立，DXi 存在且 DXi≤M0 (i=1,2,...)，则对任意的 ε>0，有 lim(n->无穷大) P{|E(Xi) - E(EX)|<ε} =1。

通俗的说： 无数个随机变量，独立；都有方差，存在公共的方差上限；则均值概率收敛于期望的均值。
不太常用。

2、（独立同分布）设X1,X2,...,Xn,... 独立同分布，且EXi = mu, DXi = σ^2(i =1,2,...)，则对任意的 ε>0，有 lim(n->无穷大) P{|1/n*累加(i=1,n) Xi-mu|<ε}=1

通俗：一组随机变量，独立，同分布，有期望和方差(同分布，则相同)，则期望无限趋近于期望。


3、（辛钦大数定律）设X1,X2,...,Xn,... 独立同分布，且EXi= μ，则对任意的ε > 0，有 lim(n->无穷大) P{ |1/n*累加(i=1,n) Xi-mu|<ε}=1

通俗: 一组随机变量，独立，同分布，有共同期望mu，(不强调有方差)，则 均值概率收敛于期望mu。


4.（贝努利大数定律）设Xi,X2,...,Xn,... 独立同分布于参数为p 的0 −1分布，则对任意的ε > 0，有 lim(n->无穷大) P{ |1/n*累加(i=1,n) Xi-mu|<ε}=1




三、中心极限定理

1.（ Levy-Lindberg 中心极限定理） 设随机变量序列 Xi, X2,..., Xn,... 独立同分布， 且 EX =mu, DX = sigma^2 (i =1,2,...)，则对任意实数x，有
lim(n->无穷大) P{ (累加(i=1,n, Xi-n*mu) -n*mu )/sqrt(n)*sigma <= x}=1/sqrt(2*pi) * 积分(-无穷大,x) e^(-t^2/2) dt;



就是说不管n个随机变量原来什么分布，但是每个变量和，近似符合正态分布
累加(i=1,n) Xi ~ N(n*mu, n*sigma^2)



2.（拉普拉斯中心极限定理）设X ~ B(n, p)(0 < p <1)(n =1,2,...) ，则对任意实数x ，有Xn~N(np, npq)
(Xn-np)/[np(1-p)]~N(0,1)

例: X~B(100, 1/10), 用中心极限定理，估计P{X<=16} 约等于__?
解: X 近似 N(np,npq) 也即是N(10,9), 
所以 (X-10)/3~N(0,1);
P{X<=16}=P{(x-10)/3<=2}=Fai(2)


========================================
6.数理统计基础
----------------------------------------
一、 定义
1.总体 - 被研究对象的所有可能结果称为总体X。
2.样本 - 来自总体X的n个互相独立且与总体X同分布的随机变量X1,X2,...,Xn称为简单随机样本，样本X1,X2,...,Xn的观察值x1,x2,...,xn称为样本观察值。
抽样的几个原则:  
 1) X1,X2,...,Xi互相独立
 2) X1,X2,...,Xi与X同分布； 就是说抽样要有一般性：调查全国身高，要排除卫生年人、80岁以上老人、父子关系有一个抽过了(不独立了)

样本观察值: x1,x2,x3,...,xn


3.统计量 - 样本的无参函数称为统计量。

比如 (x1+x2+x2)/3, x1^2+x2*2+x3^2 都是统计量，
而 a*x1+b*x2+c*x3 不是统计量，因为包含未知参数a/b/c。

这样来看，样本有无数个统计量。


二、样本常用数字特征。

设X1,X2,...,Xn为来自总体X的简单样本，则 
1. 样本均值 X bar=1/n*累加(i=1,n, Xi)
2. 样本方差 S^2=1/(n-1) * 累加(i=1,n, Xi-X bar)^2
3. 样本的k阶原点矩 Ak=1/n * 累加(i=1,n, Xi^k), k=1,2,...
样本均值就是k=1阶原点矩。
4. 样本的k阶中心矩 Bk=1/n * 累加(i=1,n,Xi-X bar)^k, k=1,2,...
样本方差不是k=2的样本中心矩，注意1/n和1/(n+1)的区别。


三、常用的抽样分布(3个重要的)

1.卡方分布 χ^2
(1) 定义：随机变量X1,X2,...,Xn互相独立且都服从标准正态分布，
则称随机变量χ^2=X1^2 + X2^2+...+Xn^2 为服从自由度为n的χ^2分布，记为 χ^2~χ^2(n)。

例1: 总体Xi~N(mu, sigma^2), 1<=i<=n，取样本(X1,X2,...,Xn)

标准化 (Xi-mu)/sigma ~ N(0,1) (1<= x <=n)
则n个相互独立的标准正态分布的随机变量的和 ~N(n*mu, n^2*sigma^2) 
[(X1-mu)/sigma]^2 + [(X2-mu)/sigma]^2+...+[(Xn-mu)/sigma]^2
= 1/sigma^2 * 累加(i=1,n,(Xi-mu)^2 ) ~ χ^2(n)

例2: 总体 X~N(0,4)，则取样四个(X1,X2,X3,X4)，互相独立，
且 a*(X1-2*X2)^2 +b*(3*X3-4*X4)^2 ~χ^2(n)
求a、b、n的值。
解：正态分布标准化。
X1-2*X2~N(0,20), 则 (X1-2*X2)/sqrt(20)~N(0,1),
3*X3-4*X4~N(0, 100), 则 (3*X3-4*X4)/sqrt(100)~N(0,1),

## 如果随机事件A B C D E两两独立，则不重合的组合，如bar(A),B,E 和 C,D，也独立。

则X1-2*X2 和 3*X3-4*X4独立。
[(X1-2*X2)/sqrt(20)]^2 + [(3*X3-4*X4)/sqrt(100)]^2 ~X^2(2)

则 a=1/20,b=1/100,n=2

#
### 卡方分布的性质
1) X~N(0,1) => X^2 ~X^2(1)
2) X~X^2(n) => EX=n, DX=2*n;
3) X~X^2(m), Y~X^2(n),且X和Y独立 => X+Y ~X^2(m+n)


(2)卡方分布的概率密度函数：钟形曲线，从0开始升起，之后下降到无限接近0
a=rchisq(n=1000,df=10); hist(a,n=100)
df自由度越大，越对称。

1)下方积分面积是1。
2)如果右侧面积是a/2，则记x上该点为a/2分为点: X^2(下标 a/2)(n)

> qchisq(n=1000,df=10,p=0.05)
[1] 907.4394
> qchisq(n=1000,df=10,p=0.95)
[1] 1115.967

3) 某点左侧面积是a/2，则记该点为左分位数 X^2(下标 1-a/2)(n)

#卡方分布概率密度曲线
x=seq(0,30,1)
plot(x,dchisq(x,df=1), type="o", main="卡方分布")
points(x,dchisq(x,df=5), type="o", col="orange")
points(x,dchisq(x,df=10), type="o", col="red")
points(x,dchisq(x,df=20), type="o", col="green")

#c6=terrain.colors(10)[1:6]
c2=c('black','orange','red','green');
legend('topright', legend=c("df=1",'df=5',"df=10",'df=20'),col=c2, lty=c(2,2))

###画出0.05/2分位数
abline(v=qchisq(0.05/2, df=10), col='red')
abline(v=qchisq( 1-0.05/2, df=10), col='#FFaaaa')
#
abline(v=qchisq(0.2, df=10), col='lightgreen') #左侧曲线下面积为0.2, 曲线下面积总共为1




2. t分布
(1)定义：
总体X~N(0,1)，Y~X^2(n),X和Y独立， 
Z=X/sqrt[Y/n] #分子标准正态分布，分母是卡方分布除以自由度n(本质上也是正态分布的和再除以自由度)
则 Z~t(n)


例: 总体X~N(0,a^2), 总体取容量为15的简单样本(X1,X2,...,X15),两两独立，
记U=(X1-X2+X3-X4+...-X10)/sqrt(X11^2+...+X15^2) *(1/sqrt(2))，则U服从什么分布?

解:
正态分布的线性组合 X1-X2+X3-X4+...-X10 ~N(0,10a^2)
则标准化后 (X1-X2+X3-X4+...-X10)/sqrt(10a^2) ~N(0,1)

后几个Xi~N(0,a^2), i=11,12,13,14,15;
Xi/sigma~N(0,1), i=11,...15;
则 (X11/sigma)^2+...+(X15/sigma)^2~X^2(5);
也就是 1/sigma^2 * (X11^2+...+X15^2)~X^2(5);

分子分母没有重复变量，则独立。
(X1-X2+X3-X4+...-X10)/sqrt(10a^2) / sqrt[1/sigma^2 * (X11^2+...+X15^2) /5] ~t(5);
左边系数整理为 sqrt(sigma^2*5)/sqrt(10a^2)=1/sqrt(2)，左边就是U，所以 U~t(5).


例3: X和Y独立，X~N(0,9),Y~N(0,9),
从X抽样得到9个随机变量(X1,X2,...,X9),
从Y抽样得到9个随机变量(Y1,Y2,...,Y9).
则 U=(X1+...+X9)/sqrt(Y1^2+...+Y9^2)~?
解:
(1) X1+...+X9~N(0,81), 则(X1+...+X9)/9~N(0,1)
(2) Yi~N(0,9), 则 Yi/3~N(0,1), 1<=i<=9;
1/9(Y1^2+...+Y9^2)~X^2(9);
(3) (X1+...+X9)/9 / sqrt[1/9(Y1^2+...+Y9^2) /9] ~t(9);
左边就是 U=(X1+...+X9)/sqrt(Y1^2+...+Y9^2)


(2) 记号
t分布类似于标准正态分布。
右分位点 t(下标 a/2)(n) 表示该点右侧区线下面积为a/2.
左分位点 t(下标 -a/2)(n) 表示该点左侧区线下面积为a/2.


# t分布
x=seq(-5,5,0.1)
plot(x, dt(x,df=100), type="l",main='t分布')
points(x,dt(x,df=10), type="l", col="orange")
points(x,dt(x,df=3), type="l", col="red")
points(x,dt(x,df=1), type="l", col="green")
#
c2=c('black','orange','red','green');
legend('topright', legend=c("df=100",'df=10',"df=3",'df=1'),col=c2, lty=c(2,2))
#df越大越接近正态分布。

#t分布的0.05分位数
abline(v=qt(0.05/2, df=3),col='red',lty=2)
abline(v=qt( 1-0.05/2, df=3), col='#FFaaaa',lty=2)



3. F分布
X~X^2(m), Y~X^2(n), 且X和Y独立，记做F=(X/m)/(Y/n), 则F服从自由度为(m,n)的F分布，记做 F~F(m,n);
分别叫m和n为第一自由度，第二自由度。

(1)性质: 
如果F~F(m,n)，则 1/F~F(n,m);

(2)密度函数图形：钟形曲线，左侧从0开始，右侧无穷大。
右a/2分位数：F(下标 a/2)(m,n);
左a/2分位数: F(下标 1-a/2)(m,n);

例4: X~t(n), y=1/X^2 ~?
解: X~t(n) 则存在U~N(0,1), V~X^2(n),且U和V独立，使得X=U/sqrt(V/n)~t(n);
Y=1/X^2= V/n/(U^2/1)=F(n,1)
(V服从卡方分布，U服从正态分布，U^2/1~卡方(1))





###
4. 正态总体的统计量
很多书会给出一个表格。

总体 X~N(mu, sigma^2)，取样本(X1,X2,...,Xn),

(1)样本均值 
bar(X)=(X1+...+Xn)/n ~ N(mu,sigma^2/n);

# 推导
E(X bar)=mu/n+...+mu/n=mu;
D(X bar)=1/n^2 *D(X1+...+Xn)=1/n^2 *n*DX=sigma^2/n
#
所以
(X-mu)/[sigma*sqrt(n)]~N(0,1)

(2) 样本均值的标准化，符合t分布
(X bar -mu) /[S/sqrt(n)]~t(n-1);

# 和(1)类似

(3) 
总体X~N(mu, sigma^2) =>(X1,..., Xn);
则 (Xi-mu)/sigma ~N(0,1) (1<=i<=n);
标准正态分布求和后符合卡方分布:
1/sigma^2 * 累加(i=1,n, [Xi-mu]^2)~X^2(n);

(4) 把上述中的总体均值mu替换为样本均值x bar后，也符合卡方分布，不过自由度少1:
1/sigma^2 * 累加(i=1,n, [Xi-X bar]^2)=(n-1)*S^2/sigma^2 ~X^2(n-1);

样本方差 S^2=1/(n-1) * 累加(i=1,n, [Xi-X bar]^2);

(5)ES^2=sigma^2;

S^2=1/(n-1) * 累加(i=1,n, [Xi-X bar]^2)样本方差(本质上是随机变量)的期望是总体方差。

只有样本方差的值，才是具体数字: S^2=1/(n-1) * 累加(i=1,n, [xi-X bar]^2)

# 若X~X^2(n)，则EX=n,
因为 (n-1)*S^2/sigma^2 ~X^2(n-1);
所以 E[(n-1)*S^2/sigma^2]=n-1;
左边提取常数项 (n-1)/sigma^2 * E(S^2)=n-1;
约去n-1得到 E(S^2)=sigma^2;
#

(6) X bar 与 S^2 独立。
独立就是检测 P(AB)=P(A)*P(B);



例: X~E(mu, sigma^2), 定义 T=累加(i=1,n, [Xi-X bar]^2), 求 E(X1*T)=?
解: T=(n-1)*1/(n-1)*累加(i=1,n, [Xi-X bar]^2)
  = (n-1)*S^2
#
E(X1*T)=(n-1)*E(X1*S^2)
#
因为 E(X1*S^2)=E(X2*S^2)=...=E(Xn*S^2) # 不懂??
所以 E(X1*S^2)=1/n*(EX1*S^2+...+EXn*S^2)=E[(X1+...+Xn)/n *S^2]
=E[X bar * S^2]

因为 X bar和S^2独立，
所以 E(X1*S^2)=E(X bar)*ES^2=mu*sigma^2;

所以 E(X1*T)=(n-1)*mu*sigma^2;



========================================
7.参数估计
----------------------------------------
统计学几大问题总结：
1.概念 
总体 X: r.v.
样本 (X1,X2,...,Xn), 独立且和总体同分布，又称简单样本；
  样本观察值(x1,...,xn): 抽样的个体的某个指标的具体值；
#
统计量: 没有参数的函数，无穷多个，比较有价值的有样本均值和样本方差等。

X bar:
Ak:
S^2:

2. 三个抽样分布
(1)卡方分布: n个相对独立的标准正态分布的平方和;
Xi~N(0,1), i属于[1,n]

(2) t分布 (最接近标准正态分布)
X~N(0,1), Y~X^2(n), X和Y独立，
则 X / sqrt(Y/n) ~t(n)

(3) X~X^2(m), Y~X^2(n), X和Y独立， 
(X/m) / (Y/n) ~F(m,n)


3.正态总体X~N(mu,sigma^2),则抽样(X1,X2,...,Xn) 有一些列统计量服从什么分布?
常见的6个，详见上文。


###
参数检验，假设检验(只需要20min即可入门到精通)。
傅里叶级数，也是一个模块。




1. 估计量的定义：
估计量: 用统计量 θˆ=ϕ( X1, X2, ..., Xn) 来估计未知参数θ，称该统计量为参数的估计量。

使用场景： 总体分布已知，但是含未知的参数，想根据样本的参数，来估计总体的该未知参数的值。
比如学生的英语成绩X~N(mu,sigma^2), 但是mu或者sigma未知。

(2).
点估计：估计一个具体值。theta=?
区间估计：要估计的是一个区间；P{X属于[?,?]}=1-alpha;

(3) 点估计的方法

## 方法1: 矩估计(使用大数定理)
辛钦大数定理: 1/n * 累加(n=1,n, Xi)概率收敛于mu;
更一般的 
EXi^k 存在，则 1/n * 累加(n=1,n, ri^k)概率收敛于 EX^k;

例1: X~G(theta), theta>0，有样本(X1,X2,...,Xn),估计 EX.
解:
EX=bar(X);
EX^2=A2=1/n * 累加(i=1,n, Xi^2);
#
p=theta;
几何分布 EX=累加(k=1,无穷大, k* (1-p)^(k-1)*p )
  = p* 累加(k=1, 无穷大, k*(1-p)^(k-1) )
#
令 S(x)=累加(n=1, 无穷大, n*x^(n-1) )=(累加[n=1,无穷大, x^n ])'
  = (x/1-x)'=1/(1-x)^2
则 EX=p*S(1-p)=p* (1/p^2)=1/p =1/theta= X bar;
theta^ =1/X bar;
这就是theta这个参数的矩估计量。




例2: 已知从总体X~N(mu, sigma^2)取样(X1,X2,...,Xn),求mu和sigma^2矩估计量。
解: EX=mu, DX=sigma^2;
则 EX^2=(EX)^2+DX=mu^2+sigma^2;

根据大数定理, 令 EX=X bar, EX^2=A2;
则 mu=X bar; mu^2+sigma^2=A1;

矩估计量 
mu^ =X bar;
sigma^2 ^=A2-X bar^2



小结: 求矩估计量的方法很简单，就是令 总体的原点矩= 样本的原点矩。
1/n * 累加(i=1, n, Xi^k) 概率P收敛于 EX^k;






## 方法2: 最大似然估计法
case1: 离散型
整个套路:
问题: 总体X-离散型(含参数theta)，取样本(X1,X2,...,Xn)，其观察值为(x1,x2,...,xn)
解: 
求似然函数 L(theta)=P{X1=x1}*...*P{Xn=xn}
两边取对数: ln(L(theta))=ln(P{X1=x1})+...+ln(P{Xn=xn})
求导数: 1/L(theta)=1/P{X1=x1}+...+1/P{Xn=xn}=0
导数等于0的点是极值点。
通过上式可求出theta的估计值theta^。

# 例1: 已知离散型总体X的分布如下(0<t<1/2),
X 0 1 2
P t t 1-2t
已知(X1,X2,...,X5)为其样本，0,0,1,2,2为观察值，
1)求t的最大似然估计值; 
2)求t的矩估计值;
3)如果有n个观察值，N1个0，N2个1，(n-N1-N2)个2，求t的最大似然估计量?
解:
1) L(t)=P{X1=0}*P{X2=0}*P{X3=1}*P{X4=2}*P{X5=2}
=P{X1=0}*P{X2=0}*P{X3=1}*P{X4=2}*P{X5=2} #简单随机样本，样本分布和总体一致
=t^3*(1-2*t)^2
两边同时取对数 ln(L(t))=3*ln(t)+2*ln(1-2*t)
两边求导数 d(ln(L(t))/dt=3/t-4/(1-2*t)=(3-10t)/[t*(1-2t)]=0
因为0<t<1/2，所以 t=3/10;
这既是t的最大似然估计值;
2)
EX=0*t+1*t+2*(1-2t)=2-3t
x bar=(0+0+1+1+2+2)/5=1;
根据大数定理，令EX=x bar, 
则 2-3*t=1, t=1/3;
这就是t的矩估计值。

两种方法估计的参数值不一致，哪种更好呢？后面会介绍怎么评价。

3)
估计值，是一个数字，记号 x bar, xi;
估计量，是一个随机变量，记号 X bar, Xi;

L(t)=t^(N1+N2) * (1-2*t)^(n-N1-N2)
取对数 ln(L(t))=(N1+N2)*ln(t) + (n-N1-N2)*ln(1-2*t)
求导 d(ln(L(t)))/dt=(N1+N2)/t-2*(n-N1-N2)/(1-2*t)=0
(可求出t的值；





case2: 连续型
总体X~f(x; theta)，其中theta未知，已知抽样(X1,X2,...,Xn)的观察值为(x1,x2,...,xn),求theta的估计值。

L(theta)=f(x1,t)*f(x2,t)*...*f(xn,t) #因为Xi的分布和X一样
取对数 ln(L(t))=
求导数 d(ln(L(t)))/dt=...=0;


例1:总体 X~f(x)={(t+1)*x^t, 0<t<; 0,其他; t>-1;
抽样(X1,X2,...,Xn)，求t的矩估计量和最大似然估计量；
解:
1) 矩估计量 
EX=积分(0,1, x*f(x)dx )=(t+1)*积分(0,1, x^(t+1)*dx )=







========================================
统计学: x2检验（chi-square test）或称卡方检验
----------------------------------------

http://www.cnblogs.com/emanlee/archive/2008/10/25/1319569.html




========================================
拟合
----------------------------------------




========================================
|-- 非线性拟合 nls()函数
----------------------------------------

1.非线性拟合 nls()函数
x=1:12;x
y1=c(7098.00, 7498,7848,8254,8761,8801.12,8951.32,9325.03,9680.90,10200,11000,12360.74)
plot(y1~x, col="red")

fit1=nls(y1~a*b^x, start=list(a=1,b=2))
lines(seq(1,12,by=0.1), predict(fit1, data.frame(x=seq(1,12,by=0.1))), col="#0096ff", lty="dotted" )
#end






========================================
回归
----------------------------------------




========================================
|-- lowess和loess方法: 局部多项式回归 Local Polynomial Regression Fitting(loess), Scatter Plot Smoothing(lowess)
----------------------------------------

1. 二维变量之间的关系研究是很多统计方法的基础，例如回归分析通常会从一元回归讲起，然后再扩展到多元情况。局部加权回归散点平滑法（locally weighted scatterplot smoothing，LOWESS或LOESS）是查看二维变量之间关系的一种有力工具。

LOWESS主要思想是取一定比例的局部数据，在这部分子集中拟合多项式回归曲线，这样我们便可以观察到数据在局部展现出来的规律和趋势；而通常的回归分析往往是根据全体数据建模，这样可以描述整体趋势，但现实生活中规律不总是（或者很少是）教科书上告诉我们的一条直线。我们将局部范围从左往右依次推进，最终一条连续的曲线就被计算出来了。显然，曲线的光滑程度与我们选取数据比例有关：比例越少，拟合越不光滑（因为过于看重局部性质），反之越光滑。

LOESS and LOWESS (locally weighted scatterplot smoothing) are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model. "LOESS" is a later generalization of LOWESS; 


Lowess和Loess都是非参数回归方法，Loess相比Lowess更加灵活和有用。Lowess通过窗口来考虑周边数据的影响，其预测值由窗口中的数据决定，窗口外的数据其贡献为0。但这种方法对野值非常敏感，Loess方法相比Lowess是一种更加robust的方法，其不仅仅考虑的局部的权重，还提出了robust权重，此权重主要是对野值进行加权，当某数据点被判断是野值后，其robust权重被设置为0，无贡献。

LOWESS本质上就是（加权）局部回归，所以理论上只要回归能做，LOWESS就能做，但我没见过你说的这种情况。LOWESS的初衷是为了检查散点图中的趋势（它具有较好的耐抗性，离群点的影响不大），而散点图通常是连续变量对连续变量的图。若因变量是离散变量，那么散点图本身的意义就不大了，仅仅在一些非常特殊的情况下可能有用，例如因变量为二分类。

Lowess and Loess的详细解释:
http://streaming.stat.iastate.edu/~stat416/LectureNotes/handout_LOWESS.pdf

Lowess and Loess in WiKi:
http://en.wikipedia.org/wiki/Local_regression



loess(): Fit a polynomial surface determined by one or more numerical predictors, using local fitting.

LOWESS(): This function performs the computations for the LOWESS smoother which uses locally-weighted polynomial regression (see the references).










2.
局部多项式回归拟合是对两维散点图进行平滑的常用方法，它结合了传统线性回归的简洁性和非线性回归的灵活性。当要估计某个响应变量值时，先从其预测变量附近取一个数据子集，然后对该子集进行线性回归或二次回归，回归时采用加权最小二乘法，即越靠近估计点的值其权重越大，最后利用得到的局部回归模型来估计响应变量的值。用这种方法进行逐点运算得到整条拟合曲线。 

在R语言中进行局部多项式回归拟合是利用loess函数，我们以cars数据集做为例子来看下使用方法。该数据中speed表示行驶速度，dist表示刹车距离。用loess来建立模型时重要的两个参数是span和degree，
- span表示数据子集的获取范围，取值越大则数据子集越多，曲线越为平滑。
span: the parameter α which controls the degree of smoothing.

- degree表示局部回归中的阶数，1表示线性回归，2表示二次回归，也可以取0，此时曲线退化为简单移动平均线。
degree: the degree of the polynomials to be used, normally 1 or 2. (Degree 0 is also allowed, but see the ‘Note’.)



(1)
# code 1
# 这里我们设span取0.4和0.8，从下图可见取值0.8的蓝色线条较为平滑。
plot(cars,pch=19)
model1=loess(dist~speed,data=cars,span=0.4)
lines(cars$speed,model1$fit,col='red',lty=2,lwd=2)
model2=loess(dist~speed,data=cars,span=0.8)
lines(cars$speed,model2$fit,col='blue',lty=2,lwd=2)


# code 2
# 当模型建立后，也可以类似线性回归那样进行预测和残差分析 
x=5:25
predict(model2,data.frame(speed=x))
plot(model2$resid~model2$fit)


# code 3
# 查看degree的影响
plot(cars,pch=19)
model0=loess(dist~speed,data=cars,degree=0)
lines(cars$speed,model0$fit,col='green',lty=1,lwd=2)

model1=loess(dist~speed,data=cars,degree=1)
lines(cars$speed,model1$fit,col='red',lty=3,lwd=2)

model2=loess(dist~speed,data=cars,degree=2)
lines(cars$speed,model2$fit,col='blue',lty=2,lwd=2)




## 还看到一个 poly 函数拟合曲线的，使用ggplot2绘制。不知道有啥区别。 //todo
data1 <- cars
for (i in 1:3) {
  mdl <- lm(dist ~ poly(speed, degree=i), data = data1)
  data1[,2+i] <- predict(mdl,data1)
}

# 作图
library(ggplot2)
ggplot(data1)+
  geom_point(aes(x=speed,y=dist))+
  geom_line(data=data1, aes(x=speed,y=V3),color="red")+
  geom_line(data=data1, aes(x=speed,y=V4),color="blue")+
  geom_line(data=data1, aes(x=speed,y=V5),color="green")+
  scale_fill_discrete(breaks=c("trt1","ctrl","trt2"))+
  guides(fill=guide_legend(title=NULL))+
  theme_bw()
#


(2)
# R语言中另一个类似的函数是lowess，它在绘图上比较方便，但在功能上不如loess强大和灵活。 
plot(cars,pch=19) 
lines(lowess(cars),lty=2,lwd=2) 



LOESS的优势是并不需要确定具体的函数形式，而是让数据自己来说话，其缺点在于需要大量的数据和运算能力。LOESS作为一种平滑技术，其目的是为了探寻响应变量和预测变量之间的关系，所以LOESS更被看作一种数据探索方法，而不是作为最终的结论。



ref:
https://blog.csdn.net/bbbeoy/article/details/72124019





========================================
|-- 使用Lasso（套索算法） 缩减基因组变量（去除多重共线性、选择变量）
----------------------------------------

http://agetouch.blog.163.com/blog/static/228535090201712103844146/

文献中看到：
The ‘‘glmnet’’ R package was used to perform the LASSO Cox regression model analysis. Complete details are provided in the Supplementary Materials, http://links.lww.com/SLA/B161.
lars包[4]也实现了改进的lasso。

1.
Construction of ISGC using LASSO Cox Regression Model

LASSO is a popular method for regression of high-dimensional predictors.3-5 The method uses an L1 penalty to shrink some regression coefficients to exactly zero. The penalty parameter l, called the tuning parameter, controls the amount of shrinkage. 

Lasso是高维度回归预测的常用方法。该方法使用L1罚分来把一些回归因子缩减到精确的0。这个罚分参数lamada叫做转换参数，控制着缩减的程度。



The larger the l value, the fewer the number of predictors selected. 

这个lamada值越大，预测选择的参数越少。



LASSO has been extended and broadly applied to the Cox proportional hazard regression model for survival analysis with high-dimensional data. LASSO can also be used for optimal selection of markers in high-dimensional data with a strong prognostic value and low correlation among each other to prevent overfitting. 

Lasso被扩展，并广泛应用到高维度数据生存分析的Cox比例风险回归模型中。Lasso也被用于在高纬度数据中选择有最强预后值、低关联度的最优marker，防止过渡拟合。



We adopted the penalized Cox regression model with LASSO penalty to simultaneously achieve shrinkage and variable selection. 

我们采用Lasso罚分式的Cox回归模型来模拟同时实现缩减和变量筛选。



Five-time cross validations were used to determine the optimal values of l. We selected l via 1-SE (standard error) criteria, i.e., the optimal l is the largest value for which the partial likelihood deviance is within one SE of the smallest value of partial likelihood deviance. 

5倍交叉验证被用于选择最优的lamada。我们通过1-SE(standard error)来选择lamada，最优的lamada是最小偏似然离差的1倍SE内最大lamada值。



Thus, we plotted the partial likelihood deviance versus log (l), where l is the tuning parameter. A value l = 0.176 with log (l) = -1.738 was chosen by cross-validation via the 1-SE criteria. A vertical line was drawn at log (l) = -1.738, which corresponds to the optimal value l = 0.176 (Figure S5). The optimal tuning parameter resulted in five non-zero coefficients. Five features, CD3IM, CD3CT, CD8IM, CD45ROCT, and CD66bIM, with coefficients 0.14855447, 0.02054805, 0.04325494, 0.09574467, and 0.17309582, respectively, were selected in the LASSO Cox regression model (Figure 1D).

所以，我们画了偏似然离差vs log[lamda]，这里的lamda就是调整参数。通过1SE标准的交叉验证，我们选择了lamda=0.176，log[lamda]=-1.738。对应于lamda=0.176，也就是log[lamda]=-1.738位置画一条竖线。最优调整参数产生了5个非零系数。通过Lasso cox回归模型，选择了5个特征CD3IM, CD3CT, CD8IM, CD45ROCT, 和 CD66bIM, 系数分别是 0.14855447, 0.02054805, 0.04325494, 0.09574467, 和 0.17309582。


We investigated the prognostic or predictive accuracy of the ISGC using time-dependent ROC analysis. The AUC at different cutoff times was used to measure prognostic or predictive accuracy. The “survival ROC” package were used to perform the time-dependent ROC curve analysis.

我们研究了预后、使用时间依赖的ROC分析预测了ISgc的精度。 在不同时间截断点的AUC被用于衡量预后或者预测精度。“生存ROC”包被用于进行时间依赖的ROC曲线分析。




2.
X-tile plots offer a single and intuitive method to evaluate the association between variables and survival. The X-tile program can automatically choose the optimum data cutoff on the basis of the highest χ? value (minimum p value) formed by Kaplan–Meier survival analysis and log-rank test. We selected the optimum cutoff score for the density of each feature using X-tile software (version 3.6.1) based on the association with the patients’ DFS.

X-tile点图提供了单个的、直观的方法来评估变量二号生存率的相关性。X-tile程序可以根据最高卡方值自动选择最优的数据cutoff值（最低p-value），这是KM-生存分析和log-rank检验中的一部分。我们使用X-tile软件（3.6.1版本），选择了每个特征的最优cutoff值，依据是与病人DFS的关联。



3.
主要思路：
1.Lasso通过最小Cp值选择变量；
2.使用coef()获取变量的系数； 这个系数是标准化过的吗？
3.怎么求回归方程的截距？
用predict把零向量代进去，就有截距值了。predict(laa,t(c(0,0,0,0)))$fit

用R做Lasso如果使用lars，那么得到的系数是非标准化的，即原先变量的系数。（不信你把标准化的预处理后做下回归，会发现不一样的）。要求截距，确实用predict比较方便，当然根据数理推导代入公式也可以的。

http://f.dataguru.cn/forum.php?mod=viewthread&tid=265747&extra=&highlight=lasso&page=2
关于Fitting the Penalized Cox Model：https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf
X-tile画图： http://medicine.yale.edu/lab/rimm/research/software.aspx






一、Lasso概述
lasso estimate的提出是Robert Tibshirani在1996年JRSSB上的一篇文章Regression shrinkage and selection via lasso[2]。全称是least absolute shrinkage and selection operator。该方法是一种压缩估计。它通过构造一个罚函数得到一个较为精炼的模型，使得它压缩一些系数，同时设定一些系数为零。因此保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。Lasso算法是一种能够实现指标集合精简的估计方法。LASSO的主要作用是降维[1,4]，排除多重共线性，进行变量选择，是有偏估计[5]。


基本思想是在回归系数的绝对值之和小于一个常数的约束条件下，使残差平方和最小，从而能够产生某些严格等于0的回归系数，得到可以解释的模型。其想法可以用如下的最优化问题来表述：


其他降维方法，如逐步回归有可能遗漏最优方程，而Lasso在错误率方面有着无可替代的优势[3]，可用于初级基因组学中经常出现的高纬度数据。传统的回归方法（最小二乘、逐步回归）用来处理业务背景相对熟悉、影响因素比较明确且不容易出现异常点的实际问题较为合适，而对于业务背景未知特别是海量自变量的问题（如基因、文本挖掘、语音识别等），甚至自变量超过观测数的问题，基于收缩机制的Lasso方法是更好的选择。



 lasso estimate具有shrinkage和selection两种功能，shrinkage这个不用多讲，本科期间学过回归分析的同学应该都知道岭估计会有shrinkage的功效，lasso也同样。关于selection功能，Tibshirani提出，当t" role="presentation" style="box-sizing: border-box; display: inline; line-height: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px;position:inherit;" >

//todo 




========================================
|-- R语言做岭回归
----------------------------------------
ridge regression可以用来处理下面两类问题：一是数据点少于变量个数；二是变量间存在共线性。

当变量间存在共线性的时候，最小二乘回归得到的系数不稳定，方差很大。这是因为系数矩阵X与它的转置矩阵相乘得到的矩阵不能求得其逆矩阵，而ridge regression通过引入参数lambda，使得该问题得到解决。在R语言中，MASS包中的函数lm.ridge()可以很方便的完成。它的输入矩阵X始终为n x p 维，不管是否包含常数项。

Usage
lm.ridge(formula, data, subset, na.action, lambda = 0, model = FALSE,
         x = FALSE, y = FALSE, contrasts = NULL, ...)
#
lambda: A scalar or vector of ridge constants.



ridge3.3<-lm.ridge(y~.-1,longley,lambda=seq(0,0.02,0.0001))
#标准化数据要-1没截距项




##############
# code 1
#install.packages("MASS")
library('MASS')
longley 
names(longley)[1] <- "y"
lm.ridge(y ~ ., longley)
##                   GNP          Unemployed    Armed.Forces     Population      Year          Employed 
## 2946.85636017    0.26352725    0.03648291    0.01116105       -1.73702984   -1.41879853    0.23128785 
plot(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.001)))

select(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.0001)))
# modified HKB estimator is 0.006836982 
# modified L-W estimator is 0.05267247 
# smallest value of GCV  at 0.0057 





##############
# code 2
# 做岭回归，对于标准化后的数据模型不包含截距项，其中lambda为岭参数k的所有取值
ridge3.3<-lm.ridge(y~.,longley,lambda=seq(0,0.02,0.0001)) #seq(0,0.2,0.001)
str(ridge3.3)
plot(ridge3.3)

#
ridge.sol=ridge3.3
matplot(x=ridge3.3$lambda, y=t(ridge3.3$coef), 
        xlab = expression(lamdba), ylab= "Cofficients", type = "l", lty = 1:20) # lty = 1:20可加可不加，设置线的形状.
#作出lambdaGCV取最小值时的那条竖直线
abline(v = ridge3.3$lambda[which.min(ridge3.3$GCV)]) #GCV是什么？
#上图在lambad在0.006左右，自变量的系数值趋于稳定。
#下面的语句绘出lambda同GCV之间关系的图形：
plot(ridge.sol$lambda, ridge.sol$GCV, type = "l", xlab = expression(lambda), ylab = expression(beta))
abline(v = ridge.sol$lambda[which.min(ridge.sol$GCV)]) 
#语句ridge.sol$coef[which.min(ridge.sol$GCV)]  为找到GCV最小时对应的系数
ridge.sol$lambda[which.min(ridge.sol$GCV)] #[1] 0.0057

#在上面的代码中，还可以调整lambda = seq(0, 1, length = 2000)的范围及大小，
#如果lambad只是一个值，则得到的图像为空，只有在lambad变化的时候，才能得到岭迹曲线。







##############
# code 3 https://blog.csdn.net/li603060971/article/details/49508279
library('MASS')
head(longley)
#先标准化数据
data.norm=as.data.frame(apply(longley,2,scale) )
names(data.norm)[1] <- "y"
head(data.norm)
# y   GNP Unemployed Armed.Forces Population       Year   Employed
#
la=seq(0,1,0.01)
ridge.rs=lm.ridge(y~.-1, data=data.norm, lambda=la) #标准化数据要-1没截距项
head(ridge.rs)
plot(ridge.rs)
coef(ridge.rs)
#
#删除Population 由-变+
ridge.rs2<-lm.ridge(y~.-Population-1, data=data.norm,
                    lambda=seq(0,1,0.001))#岭回归
plot(ridge.rs2)
coef(ridge.rs2)
#
#删除year 由-变+
ridge.rs3<-lm.ridge(y~.-Population-Year-1, data=data.norm,
                    lambda=seq(0,1,0.001))#岭回归
plot(ridge.rs3)
coef(ridge.rs3) #全为正了
#
select(ridge.rs3)
#modified HKB estimator is 0.05941355 
#modified L-W estimator is 0.03466392 
#smallest value of GCV  at 0.479
lm.ridge(y~.-Population-Year-1, data=data.norm,lambda=0.479)
#     GNP      Unemployed   Armed.Forces  Employed 
# 0.4546171    0.1769713    0.1210725    0.3684340


#
# 下面利用ridge包中的linearRidge()函数进行自动选择岭回归参数
#install.packages('ridge')
library(ridge)
mod <- linearRidge(y ~ ., data = data.norm)
summary(mod)
## Call:
## linearRidge(formula = y ~ ., data = data.norm)
## 
## Coefficients:
##                Estimate Scaled estimate Std. Error (scaled) t value (scaled) Pr(>|t|)    
## (Intercept)  -3.106e-18              NA                  NA               NA       NA    
## GNP           3.995e-01       1.547e+00           3.419e-01            4.526  6.0e-06 ***
## Unemployed    1.026e-01       3.972e-01           2.323e-01            1.710   0.0873 .  
## Armed.Forces  8.903e-02       3.448e-01           1.765e-01            1.953   0.0508 .  
## Population   -1.825e-02      -7.068e-02           4.898e-01            0.144   0.8853    
## Year          2.897e-01       1.122e+00           2.493e-01            4.500  6.8e-06 ***
## Employed      2.195e-01       8.502e-01           4.630e-01            1.836   0.0663 .  
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
## 
## Ridge parameter: 0.01046912, chosen automatically, computed using 2 PCs
## 
## Degrees of freedom: model 3.67 , variance 3.218 , residual 4.123 

#从模型运行结果看，测岭回归参数值为0.01046912，2个变量的系数显著（GNP，Year）
#最后，利用Lasso回归解决共线性问题






##############
## code 4
library(lars)
## Loaded lars 1.2
x = as.matrix(longley[, 2:7])
y = as.matrix(longley[, 1])
(laa = lars(x, y, type = "lar")) #lars函数值用于矩阵型数据
## Call:
## lars(x = x, y = y, type = "lar")
## R-squared: 0.993 
## Sequence of LAR moves:
##      GNP Year Armed.Forces Unemployed Employed Population
## Var    1    5            3          2        6          4
## Step   1    2            3          4        5          6
# 由此可见，LASSO的变量选择依次是 GNP Year Armed.Forces Unemployed Employed Population
plot(laa)#绘出图
summary(laa)  #给出Cp值
## LARS/LAR
## Call: lars(x = x, y = y, type = "lar")
##   Df     Rss        Cp
## 0  1 1746.86 1210.0561
## 1  2 1439.51  996.6871
## 2  3   32.31   12.6400
## 3  4   23.18    8.2425
## 4  5   22.91   10.0505
## 5  6   22.63   11.8595
## 6  7   12.84    7.0000
## 根据课上对Cp含义的解释（衡量多重共线性，其值越小越好），我们取到第3步，使得Cp值最小，也就是选择GNP, Year, Armed.Forces这三个变量。

## -> 最后的是lar方法 不是lasso。type值变为lasso才对。
(las = lars(x, y, type = "lasso"))
## Call:
## lars(x = x, y = y, type = "lasso")
## R-squared: 0.993 
## Sequence of LASSO moves:
##      GNP Year Armed.Forces Unemployed Employed Population Year Employed Employed Year Employed Employed
## Var    1    5            3          2        6          4   -5       -6        6    5       -6        6
## Step   1    2            3          4        5          6    7        8        9   10       11       12
plot(las)
summary(las)
## LARS/LASSO
## Call: lars(x = x, y = y, type = "lasso")
##    Df     Rss        Cp
## 0   1 1746.86 1210.0561
## 1   2 1439.51  996.6871
## 2   3   32.31   12.6400
## 3   4   23.18    8.2425
## 4   5   22.91   10.0505
## 5   6   22.63   11.8595


哪个指标判断共线性？






========================================
五类模型的变量选择可采用R语言的glmnet包来解决。这五类模型分别是： //todo
----------------------------------------
1. 二分类logistic回归模型
2. 多分类logistic回归模型
3.Possion模型
4.Cox比例风险模型
5.SVM










ref:
https://blog.csdn.net/orchidzouqr/article/details/53582801
https://blog.csdn.net/jiabiao1602/article/details/39338181




========================================
----------------------------------------

========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------

