R中的统计学

负二项分布，常用于拟合RNAseq测序结果。


本文记录R和数理统计学。

还有一个paper中遇到的统计学: NGS/NGS_statistics.txt 
一个纯粹统计学: R/Math-statistics.txt

薛毅和陈立萍的《统计建模与R软件》（清华大学出版社，2007年）中



========================================
统计学和R书单
----------------------------------------

1.教科书
(1). 教科书《R语言与统计分析》 汤银才 主编 447页pdf


(2). 教科书《统计建模与R软件》上下册 共643页 薛毅陈立萍 编著
目录在PDF7页，


(3).《R Programming for Data Science》
https://bookdown.org/rdpeng/rprogdatascience/
podcast: 《Not So Standard Deviations》 http://nssdeviations.com/





2. 网络教程
(1) 学习R编程
https://iowiki.com/r/r_poisson_regression.html






========================================
R常见用途: 探索性数据分析、统计推断、回归分析、机器学习、可视化报告
----------------------------------------
https://www.imooc.com/video/8778 该视频目前只讲了大概能干啥，没讲怎么做，比较水。


1. 发布平台
(1)github: 发布各种源代码，写作编程等；

(2)RPubs: https://www.rpubs.com/ 貌似是RStudio的产品。
Easy web publishing from R
Write R Markdown documents in RStudio.
Share them here on RPubs. (It’s free, and couldn’t be simpler!)


Prerequisites
You'll need R itself, RStudio (v0.96.230 or later), and the knitr package (v0.5 or later).

Instructions
1)In RStudio, create a new R Markdown document by choosing File | New | R Markdown.
选择R markdown和notebook没区别，个人倾向于使用后者。

2)Click the Knit HTML button in the doc toolbar to preview your document.
注意，保存时不要输入后缀名，会自动生成.Rmd后缀名。

3)In the preview window, click the Publish button.
打开一个新网页，需要输入用户名和密码。
不能更新！！只能删掉旧的，重新上传。
删除方法是打开web页，左下角可以删除。



(3)申请用户名:
https://www.rpubs.com/dawnEve
测试页:
http://rpubs.com/dawnEve/test001
http://rpubs.com/dawnEve/test002



(4)搜索方法：
google搜索“??+RPubs”即可。(如: heatmap rpubs)
找到: https://www.rpubs.com/tskam/heatmap




2. 探索性数据分析 - 其实就是作图
https://github.com/angelayuan/Exploratory-Data-Analysis

散点图
柱状图
折线图


3. 统计推断
基于数据得到正式结论的过程，从不确定和抽样中，得到有一定可信度(错误率控制在5%以内)的结论。
例子: 药效


4. 回归分析
线性模型拟合数据
- 预测变量
- 结果变量

父母的身高，预测孩子的身高。


5. 机器学习
训练模型 + 预测
例子: 分类模型。

机器学习是一门高深的学问，没有深入研究，不可能实现。
不过R包可以大大降低使用机器学习的门槛。

# BiocManager::install("caret")
library(caret)

# 训练集建立模型，测试集测试效果
inTrain=createDataPartition(y=training$classe, p=0.7, list=F)
training_set=training[inTrain,]
cv_set=training[-inTrain,]

# 使用训练集，建立随机森林
rf_fit=randomForest(class~., data=training_set)

# 把新数据集带入模型
test_pred=predict(rf_fit, newdata=testing)
test_pred





6. 开发数据产品，发布报告
- R制作html，调用Google charts, 交互式html图表;
- Manipulate包，实现人机交互;
- rCharts包，使用R制作交互式js可视化产品;
- Shiny包，制作嵌入式网页的交互式R程序的平台 https://www.shinyapps.io/
- Slidify包，制作和发布基于R的报告(类似ppt)
- Rmarkdown 可以生成html报告

实例: https://angelayuan.shinyapps.io/predict_bodyfat/







========================================
|-- 常见的R统计函数
----------------------------------------
1.统计函数   作用
max(x) 返回向量x中最大的元素
min(x) 返回向量x中最小的元素
which.max(x) 返回向量x中最大元素的下标
which.min(x) 返回向量x中最小元素的下标
mean(x) 计算样本(向量)x的均值
median(x) 计算样本(向量)x的中位数
mad(x) 计算中位绝对离差
var(x) 计算样本(向量)x的方差
sd(x) 计算向量x的标准差
...More(汤银才 P41)


(1)Median Absolute Deviation
#绝对中位差实际求法是用原数据减去中位数后得到的新数据的绝对值的中位数。
#但绝对中位差常用来估计标准差，估计标准差=1.4826*绝对中位差。

#R语言中返回的是估计的标准差。
#例如：原数据{2，3，4，5，6}中位数4，新数据{2，1，0，1，2}，即{0，1，1，2，2}。
#所以中位数是1。也就是绝对中位差是1.

#而R返回的是1*1.4826=1.4826

#mad(c(2,3,4,5,6))[1] 1.4826
#R语言里的绝对中位差还要乘上一个比例因子：constant=1.4826
#也可以不乘
mad(c(2,3,4,5,6), constant=1) #1
mad(c(2,3,4,5,6,100), constant=1) #[1] 1.5
#也就是相对于标准差，mad对少量异常值不太敏感
#https://www.zhihu.com/question/56537218/answer/163638714




========================================
|-- R做标准化与归一化，scale()函数、sweep()函数
----------------------------------------

一、
#数据集
x<-cbind(c(1,2,3,4),c(5,5,10,20),c(3,6,9,12))

#自己写标准化
x_min_temp<-apply(x,2,min) 
x_min<-matrix(rep(x_min_temp,4),byrow=TRUE,ncol=3)#需要输入行数和列数
abs(x-x_min)#当前值减去均值
x_extreme_temp<-apply(x,2,max)-apply(x,2,min)
x_extreme<-matrix(rep(x_extreme_temp,4),byrow=TRUE,ncol=3)#需要输入行数和列数
abs(x-x_min)/x_extreme


2. sweep函数
center <- sweep(x, 2, apply(x, 2, min),'-') #在列的方向上减去最小值，不加‘-’也行
R <- apply(x, 2, max) - apply(x,2,min)   #算出极差，即列上的最大值-最小值
x_star<- sweep(center, 2, R, "/")        #把减去均值后的矩阵在列的方向上除以极差向量

#sweep函数更简洁、易懂，且不需要输入行数和列数，二者性能也差不多

#sweep再举一个例子哈
m<-matrix(c(1:9),byrow=TRUE,nrow=3)
#第一行都加1，第二行都加4，第三行都加7
sweep(m, 1, c(1,4,7), "+")  



3. scale函数，这个比较简单，不多说
scale(x, center = TRUE, scale = TRUE)







二、归一化和标准化的区别？
1. 归一化（Normalization）
1).把数据变为（0，1）之间的小数。主要是为了方便数据处理，因为将数据映射到0～1范围之内，可以使处理过程更加便捷、快速。 
2).把有量纲表达式变换为无量纲表达式，成为纯量。经过归一化处理的数据，处于同一数量级，可以消除指标之间的量纲和量纲单位的影响，提高不同数据指标之间的可比性。 

主要算法： 
1).线性转换，即min-max归一化（常用方法） y=(x-min)/(max-min) 
2). 对数函数转换 y=log10(x) 
3).反余切函数转换 y=atan(x)*2/PI 



2.标准化（Standardization） 
数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。 

主要方法： 
1).z-score标准化，即零-均值标准化（常用方法） y=(x-μ)/σ 是一种统计的处理，基于正态分布的假设，将数据变换为均值为0、标准差为1的标准正态分布。但即使数据不服从正态分布，也可以用此法。特别适用于数据的最大值和最小值未知，或存在孤立点。 
2).小数定标标准化 y=x/10^j （j确保max(|y|)&lt;1） 通过移动x的小数位置进行标准化 
3).对数Logistic模式 y=1/(1+e^(-x))


refer:
http://blog.csdn.net/yitianguxingjian/article/details/51820758



========================================
|-- 使用R做随机抽样: 模拟——随机数、抽样、线性模型
----------------------------------------
为了可重复，设置随机数种子为一个具体值
set.seed(1)


1.
(1)Simulation
今天学习的内容是模拟 —— Simulation，在统计和一些其他的应用中很重要，所以在这里介绍一些R语言中可以做模拟的函数。

用于模拟已知概率分布的数字和变量
有些函数可以直接生成符合某种概率分布的随机数字或变量，例如：
rnorm()：指定一个均值和标准差，即可生成符合正态分布的随机数字变量
rpois()：从已知平均发生率（rate）的泊松分布中生成泊松随机变量

一共有四类基本的函数和概率分布函数相关，它们的前缀分别是d, r, p以及q：
d：用来估计密度（density）
r：用来产生随机数字（random）
p：估计累计分布（cumulative distribution）
q：估计分位数（quantile）
每种分布都有以上这四种前缀构成的函数，比如rpois(), dpois(), ppois(), qpois()等。

每种函数都有不同的参数，拿正态分布的四个函数举例：
dnorm(x, mean = 0, sd = 1, log = FALSE)
## 可以求密度的对数值

pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
## 可以对概率求对数；计算该分布的左尾，如果填FALSE就是计算右尾

qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
## 可以对概率求对数；计算该分布的左尾，如果填FALSE就是计算右尾

rnorm(n, mean = 0, sd = 1)
## n是你想生成的随机变量的个数

所有函数都需要我们给定均值和标准差，以此确定实际的概率分布，如果不指定，那么默认属于标准正态分布（均值为0，标准差为1）
生成最简单的服从标准正态分布的随机数：
> ## Simulate standard Normal random numbers
> x <- rnorm(10)   
> x
 [1]  0.01874617 -0.18425254 -1.37133055 -0.59916772  0.29454513
 [6]  0.38979430 -1.20807618 -0.36367602 -1.62667268 -0.25647839

修改参数：
> x <- rnorm(10, 20, 2) 
> x
 [1] 22.20356 21.51156 19.52353 21.97489 21.48278 20.17869 18.09011
 [8] 19.60970 21.85104 20.96596
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  18.09   19.75   21.22   20.74   21.77   22.20
#



(2)设置随机数字生成种子
set.seed()函数可以用来设置随机数字生成种子（seed）。
这种方法可产生相同的随机数，也叫“伪随机数”。它怎么用？

举例
我们可以设置种子为任意整数，然后生成随机数字如下：
> set.seed(1)
> rnorm(5)
[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078
> set.seed(2)
> rnorm(5)
[1] -0.89691455  0.18484918  1.58784533 -1.13037567 -0.08025176
> set.seed(5)
> rnorm(5)
[1] -0.84085548  1.38435934 -1.25549186  0.07014277  1.71144087

我们看到种子设定值不同时随机数字也不同，但是再设定相同种子就会出现完全一样的随机数字：
> set.seed(1)
> rnorm(5)
[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078

这个操作能让你重复得到之前生成的随机数字，可以让我们的模拟过程可重复，所以在生成随机数字之前最好设定一个种子。

再拿泊松分布举例：
生成不同发生率的10个随机变量：
> rpois(10, 1)    ## Counts with a mean of 1
 [1] 0 0 1 1 2 1 1 4 1 2
> rpois(10, 2)    ## Counts with a mean of 2
 [1] 4 1 2 0 1 1 0 1 4 1
> rpois(10, 20)   ## Counts with a mean of 20
 [1] 19 19 24 23 22 24 23 20 11 22
#

估计泊松分布的累积分布函数：

## 在平均发生率为2的泊松分布中，出现小于等于2的随机变量的概率是多少
> ppois(2,2) 
[1] 0.6766764
## 在平均发生率为2的泊松分布中，出现小于等于4的随机变量的概率是多少
> ppois(4,2)
[1] 0.947347
## 在平均发生率为2的泊松分布中，出现小于等于6的随机变量的概率是多少
> ppois(6,2)
[1] 0.9954662




(3)Simulating a Linear Model
我们再来看怎样从简单的线性模型中提取随机值

如果x是单一自变量
举例：
## 首先记得先设定一个种子
> set.seed(20)

## 设置自变量x取值，属于标准正态分布
> x <- rnorm(100)

## 随机噪声e是属于标准差为2的正态分布
> e <- rnorm(100, 0, 2)

## 设置线性方程的回归系数和截距
> y <- 0.5 + 2 * x + e

## 输出概要结果
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-6.4084 -1.5402  0.6789  0.6893  2.9303  6.5052 

## 画出图表
> plot(x,y)
这就是通过回归模型模拟出的x和y的图表，可以看出明显的线性关系。





(4)如果x是二元变量
x如果是二元变量，可以代表两种性别、两种实验处理（可以是实验组和对照组）这一类情况

使用二项分布函数rbinom()

## 重新设定种子
> set.seed(10)

## 随机取100个二元变量数据，并设置参数
> x <- rbinom(100, 1, 0.5)

## 取符合正态分布的随机变量
> e <- rnorm(100, 0, 2)

## 生成线性模型
> y <- 0.5 + 2 *x + e

## 查看结果和作图
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-3.4936 -0.1409  1.5767  1.4322  2.8397  6.9410 
> plot(x, y)

可以看到，x是二元变量，y依旧是连续的，符合正态分布






(5)从复杂模型中模拟取值
假设结果y服从于一个平均值为μ的泊松分布，log(μ)服从一个截距为β0，斜率β1的线性函数，x是其中的自变量

> set.seed(1)
> x <- rnorm(100)    

## 模拟线型变量log(μ)
> log.mu <- 0.5 + 0.3 * x

## 使用rpois函数来计算y，取幂
> y <- rpois(100, exp(log.mu))

## 查看结果
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.00    1.00    1.00    1.55    2.00    6.00 
> plot(x, y)

可以看到x越大y越大，呈线性关系






(6)Random Sampling
这里使用的函数是sample()，可以从你给定的一组对象中，随机抽取样本

假如我们提供一个数值向量，sample()可以从中随机抽取样本。因此我们可以人为确定一个分布，指定给一个向量并从中取样

举例：
从整数1到10中取样，取出的数字不放回，如果重复去取样两次，得到的数字是不重复的，对字母取样也是同理：

> set.seed(1)
> sample(1:10, 4)
[1] 3 4 5 7
> sample(1:10, 4)
[1] 3 9 8 5

> ## Doesn't have to be numbers
> sample(letters, 5)    
[1] "q" "b" "e" "x" "p"



如果只提供向量，不指定任何条件，返回结果就是这些整数重新排列，数字内部无重复：
> ## Do a random permutation
> sample(1:10)          
 [1]  4  7 10  6  9  2  8  3  1  5
> sample(1:10)
 [1]  2  3  4  1  9  5 10  8  6  7
#


有放回的取样，设置参数replace = TRUE，因为有放回，所以可以看到数字内部有重复：
> ## Sample w/replacement
> sample(1:10, replace = TRUE)  
 [1] 2 9 7 8 2 8 5 9 7 8
#







2.Simulation 小结
- 使用R中的函数从指定的概率分布中取样
使用rnorm(), rpois(), rbinom()等。

常用分布包括正态分布（normal）、泊松分布（poission）、二项分布（binomial）、指数分布（exponential）、伽马分布（gamma）等。

- sample函数可用来从指定向量中随机取样
- 无论何时记得用seed生成种子



ref:
https://mp.weixin.qq.com/s/6VrRjH4tx6VSqNIJ8qw7Kg

视频课程 R Programming by Johns Hopkins University: https://www.coursera.org/learn/r-programming/home/welcome

讲义 Programming for Data Science: https://bookdown.org/rdpeng/rprogdatascience/




========================================
|-- 非线性拟合 nls()函数
----------------------------------------

1.非线性拟合 nls()函数
x=1:12;x
y1=c(7098.00, 7498,7848,8254,8761,8801.12,8951.32,9325.03,9680.90,10200,11000,12360.74)
plot(y1~x, col="red")

fit1=nls(y1~a*b^x, start=list(a=1,b=2))
lines(seq(1,12,by=0.1), predict(fit1, data.frame(x=seq(1,12,by=0.1))), col="#0096ff", lty="dotted" )
#end






========================================
分布和检验
----------------------------------------





========================================
|-- 连续型分布: 正态分布、均匀分布、指数分布
----------------------------------------
1. 正态分布 Normal Distribution
dnorm(x, mean, sd) 概率密度
pnorm(x, mean, sd) 累积
qnorm(p, mean, sd) 分位数(由左侧曲线下面积p，查x值)
rnorm(n, mean, sd) 产生随机数

Density, distribution function, quantile function and random generation for the normal distribution with mean equal to mean and standard deviation equal to sd.

以下是上述功能中使用的参数的说明 -
	x是数字的向量。
	p是概率的向量。
	n是观察次数（样本量）。
	mean是样本数据的平均值。 它的默认值为零。
	sd是标准偏差。 它的默认值是1。
#


(1)dnorm() 概率密度曲线, 钟形曲线，y值最大的时候该x出现的可能性最高。
对于给定的平均值和标准偏差，此函数给出每个点处的概率分布的高度。

# Create a sequence of numbers between -5 and 5 incrementing by 0.1.
x <- seq(-5, 5, by = 0.1);x
# Choose the mean as 0 and standard deviation as 1.
y <- dnorm(x, mean = 0, sd = 1)
plot(x,y, type='o')


(2)pnorm()，累积曲线，S形单调递增曲线
此函数给出正态分布随机数的概率小于给定数字的值。 它也被称为“累积分布函数”。

x <- seq(-5,5,by = 0.1)
y <- pnorm(x, mean = 0, sd = 1)
plot(x,y, type='o', col='red')


(3)qnorm() 分位数
此函数获取概率值并给出其累积值与概率值匹配的数字。

qnorm(0.05, 0,1) #x=-1.644854 时左侧曲线下面积为0.05
qnorm(0.95, 0,1) #x=1.644854 时左侧曲线下面积为0.95
qnorm(1-(1e-16), 0,1) #x=8.209536 时左侧曲线下面积为1-(1e-16)

#
x <- seq(-5,5,by = 0.1)
y <- dnorm(x, mean = 0, sd = 1)
plot(x,y)
abline(v=qnorm(0.05, 0,1), col='red', lty=2)
abline(v=qnorm(0.95, 0,1), col='blue', lty=2)


(4)rnorm() 产生随机数
此函数用于生成分布正常的随机数。 它将样本大小作为输入并生成许多随机数。 我们绘制直方图以显示生成的数字的分布。

y <- rnorm(200)
hist(y,n=50) #频率直方图也是钟形






========================================
|-- 离散型分布: 二项分布/负二项分布、几何分布/超几何分布
----------------------------------------

1. 二项分布 Binomial Distribution

二项分布模型用于找出事件成功的概率，该事件在一系列实验中仅具有两种可能的结果。 例如，投掷硬币总是给出头部或尾部。 在二项分布期间估计在重复投掷硬币10次时准确找到3个头的概率。

dbinom(x, size, prob)
pbinom(x, size, prob)
qbinom(p, size, prob)
rbinom(n, size, prob)

参数说明:
	x是数字的向量。
	p是概率的向量。
	n是观察次数。
	size是试验次数。
	prob是每次试验成功的概率
#


(1)每个点的概率密度分布
x <- seq(0,50,by = 1);x
y <- dbinom(x,50,0.5) #抛硬币正面向上的概率是0.5，抛50次，正面向上的概率。
plot(x,y)

(2)pbinom() 此函数给出事件的累积概率。 它是表示小于等于给定值出现的概率。
x <- seq(0,50,by = 1);x
y <- pbinom(x,50,0.5)
plot(x,y)

#
pbinom(25,50,0.5) #累积概率: 25次及25次以下朝上的概率
# 则正好25次朝上的概率:
pbinom(25,50,0.5) -pbinom(24,50,0.5)  #0.1122752
choose(50,25)*0.5**25*0.5**25 #0.1122752


(3)qbinom() 分位数
abline(v=qbinom(0.05,50,1/2), col='red', lty=2)
abline(v=qbinom(0.95,50,1/2), col='blue', lty=2)

#
qbinom(0.05,50,1/2) #[1] 19 概率密度曲线左侧面积达到0.05时的x值为19
qbinom(0.95,50,1/2) #[1] 31


(4) rbinom() 产生随机数
rbinom(8,50,0.5) #每次正面向上0.5，抛50次，正面向上的次数。产生8个次数。
# [1] 22 23 25 26 24 23 25 29

hist(rbinom(800,50,0.5)) #产生足够多的时，这些次数分布符合二项分布的密度曲线

# 画抽样的密度图
plot(density(rbinom(800,50,0.5)))
# 定义的概率密度图
x <- seq(0,50,by = 1);x
y <- dbinom(x,50,0.5)
points(x,y, col='red',lty=2) #和样本密度曲线重叠





========================================
|-- *** 泊松分布
----------------------------------------




========================================
回归
----------------------------------------




========================================
|-- lowess和loess方法: 局部多项式回归 Local Polynomial Regression Fitting(loess), Scatter Plot Smoothing(lowess)
----------------------------------------

1. 二维变量之间的关系研究是很多统计方法的基础，例如回归分析通常会从一元回归讲起，然后再扩展到多元情况。局部加权回归散点平滑法（locally weighted scatterplot smoothing，LOWESS或LOESS）是查看二维变量之间关系的一种有力工具。

LOWESS主要思想是取一定比例的局部数据，在这部分子集中拟合多项式回归曲线，这样我们便可以观察到数据在局部展现出来的规律和趋势；而通常的回归分析往往是根据全体数据建模，这样可以描述整体趋势，但现实生活中规律不总是（或者很少是）教科书上告诉我们的一条直线。我们将局部范围从左往右依次推进，最终一条连续的曲线就被计算出来了。显然，曲线的光滑程度与我们选取数据比例有关：比例越少，拟合越不光滑（因为过于看重局部性质），反之越光滑。

LOESS and LOWESS (locally weighted scatterplot smoothing) are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model. "LOESS" is a later generalization of LOWESS; 


Lowess和Loess都是非参数回归方法，Loess相比Lowess更加灵活和有用。Lowess通过窗口来考虑周边数据的影响，其预测值由窗口中的数据决定，窗口外的数据其贡献为0。但这种方法对野值非常敏感，Loess方法相比Lowess是一种更加robust的方法，其不仅仅考虑的局部的权重，还提出了robust权重，此权重主要是对野值进行加权，当某数据点被判断是野值后，其robust权重被设置为0，无贡献。

LOWESS本质上就是（加权）局部回归，所以理论上只要回归能做，LOWESS就能做，但我没见过你说的这种情况。LOWESS的初衷是为了检查散点图中的趋势（它具有较好的耐抗性，离群点的影响不大），而散点图通常是连续变量对连续变量的图。若因变量是离散变量，那么散点图本身的意义就不大了，仅仅在一些非常特殊的情况下可能有用，例如因变量为二分类。

Lowess and Loess的详细解释:
http://streaming.stat.iastate.edu/~stat416/LectureNotes/handout_LOWESS.pdf

Lowess and Loess in WiKi:
http://en.wikipedia.org/wiki/Local_regression



loess(): Fit a polynomial surface determined by one or more numerical predictors, using local fitting.

LOWESS(): This function performs the computations for the LOWESS smoother which uses locally-weighted polynomial regression (see the references).










2.
局部多项式回归拟合是对两维散点图进行平滑的常用方法，它结合了传统线性回归的简洁性和非线性回归的灵活性。当要估计某个响应变量值时，先从其预测变量附近取一个数据子集，然后对该子集进行线性回归或二次回归，回归时采用加权最小二乘法，即越靠近估计点的值其权重越大，最后利用得到的局部回归模型来估计响应变量的值。用这种方法进行逐点运算得到整条拟合曲线。 

在R语言中进行局部多项式回归拟合是利用loess函数，我们以cars数据集做为例子来看下使用方法。该数据中speed表示行驶速度，dist表示刹车距离。用loess来建立模型时重要的两个参数是span和degree，
- span表示数据子集的获取范围，取值越大则数据子集越多，曲线越为平滑。
span: the parameter α which controls the degree of smoothing.

- degree表示局部回归中的阶数，1表示线性回归，2表示二次回归，也可以取0，此时曲线退化为简单移动平均线。
degree: the degree of the polynomials to be used, normally 1 or 2. (Degree 0 is also allowed, but see the ‘Note’.)



(1)
# code 1
# 这里我们设span取0.4和0.8，从下图可见取值0.8的蓝色线条较为平滑。
plot(cars,pch=19)
model1=loess(dist~speed,data=cars,span=0.4)
lines(cars$speed,model1$fit,col='red',lty=2,lwd=2)
model2=loess(dist~speed,data=cars,span=0.8)
lines(cars$speed,model2$fit,col='blue',lty=2,lwd=2)


# code 2
# 当模型建立后，也可以类似线性回归那样进行预测和残差分析 
x=5:25
predict(model2,data.frame(speed=x))
plot(model2$resid~model2$fit)


# code 3
# 查看degree的影响
plot(cars,pch=19)
model0=loess(dist~speed,data=cars,degree=0)
lines(cars$speed,model0$fit,col='green',lty=1,lwd=2)

model1=loess(dist~speed,data=cars,degree=1)
lines(cars$speed,model1$fit,col='red',lty=3,lwd=2)

model2=loess(dist~speed,data=cars,degree=2)
lines(cars$speed,model2$fit,col='blue',lty=2,lwd=2)




## 还看到一个 poly 函数拟合曲线的，使用ggplot2绘制。不知道有啥区别。 //todo
data1 <- cars
for (i in 1:3) {
  mdl <- lm(dist ~ poly(speed, degree=i), data = data1)
  data1[,2+i] <- predict(mdl,data1)
}

# 作图
library(ggplot2)
ggplot(data1)+
  geom_point(aes(x=speed,y=dist))+
  geom_line(data=data1, aes(x=speed,y=V3),color="red")+
  geom_line(data=data1, aes(x=speed,y=V4),color="blue")+
  geom_line(data=data1, aes(x=speed,y=V5),color="green")+
  scale_fill_discrete(breaks=c("trt1","ctrl","trt2"))+
  guides(fill=guide_legend(title=NULL))+
  theme_bw()
#


(2)
# R语言中另一个类似的函数是lowess，它在绘图上比较方便，但在功能上不如loess强大和灵活。 
plot(cars,pch=19) 
lines(lowess(cars),lty=2,lwd=2) 



LOESS的优势是并不需要确定具体的函数形式，而是让数据自己来说话，其缺点在于需要大量的数据和运算能力。LOESS作为一种平滑技术，其目的是为了探寻响应变量和预测变量之间的关系，所以LOESS更被看作一种数据探索方法，而不是作为最终的结论。



ref:
https://blog.csdn.net/bbbeoy/article/details/72124019





========================================
|-- 使用Lasso（套索算法） 缩减基因组变量（去除多重共线性、选择变量）
----------------------------------------

http://agetouch.blog.163.com/blog/static/228535090201712103844146/

文献中看到：
The ‘‘glmnet’’ R package was used to perform the LASSO Cox regression model analysis. Complete details are provided in the Supplementary Materials, http://links.lww.com/SLA/B161.
lars包[4]也实现了改进的lasso。

1.
Construction of ISGC using LASSO Cox Regression Model

LASSO is a popular method for regression of high-dimensional predictors.3-5 The method uses an L1 penalty to shrink some regression coefficients to exactly zero. The penalty parameter l, called the tuning parameter, controls the amount of shrinkage. 

Lasso是高维度回归预测的常用方法。该方法使用L1罚分来把一些回归因子缩减到精确的0。这个罚分参数lamada叫做转换参数，控制着缩减的程度。



The larger the l value, the fewer the number of predictors selected. 

这个lamada值越大，预测选择的参数越少。



LASSO has been extended and broadly applied to the Cox proportional hazard regression model for survival analysis with high-dimensional data. LASSO can also be used for optimal selection of markers in high-dimensional data with a strong prognostic value and low correlation among each other to prevent overfitting. 

Lasso被扩展，并广泛应用到高维度数据生存分析的Cox比例风险回归模型中。Lasso也被用于在高纬度数据中选择有最强预后值、低关联度的最优marker，防止过渡拟合。



We adopted the penalized Cox regression model with LASSO penalty to simultaneously achieve shrinkage and variable selection. 

我们采用Lasso罚分式的Cox回归模型来模拟同时实现缩减和变量筛选。



Five-time cross validations were used to determine the optimal values of l. We selected l via 1-SE (standard error) criteria, i.e., the optimal l is the largest value for which the partial likelihood deviance is within one SE of the smallest value of partial likelihood deviance. 

5倍交叉验证被用于选择最优的lamada。我们通过1-SE(standard error)来选择lamada，最优的lamada是最小偏似然离差的1倍SE内最大lamada值。



Thus, we plotted the partial likelihood deviance versus log (l), where l is the tuning parameter. A value l = 0.176 with log (l) = -1.738 was chosen by cross-validation via the 1-SE criteria. A vertical line was drawn at log (l) = -1.738, which corresponds to the optimal value l = 0.176 (Figure S5). The optimal tuning parameter resulted in five non-zero coefficients. Five features, CD3IM, CD3CT, CD8IM, CD45ROCT, and CD66bIM, with coefficients 0.14855447, 0.02054805, 0.04325494, 0.09574467, and 0.17309582, respectively, were selected in the LASSO Cox regression model (Figure 1D).

所以，我们画了偏似然离差vs log[lamda]，这里的lamda就是调整参数。通过1SE标准的交叉验证，我们选择了lamda=0.176，log[lamda]=-1.738。对应于lamda=0.176，也就是log[lamda]=-1.738位置画一条竖线。最优调整参数产生了5个非零系数。通过Lasso cox回归模型，选择了5个特征CD3IM, CD3CT, CD8IM, CD45ROCT, 和 CD66bIM, 系数分别是 0.14855447, 0.02054805, 0.04325494, 0.09574467, 和 0.17309582。


We investigated the prognostic or predictive accuracy of the ISGC using time-dependent ROC analysis. The AUC at different cutoff times was used to measure prognostic or predictive accuracy. The “survival ROC” package were used to perform the time-dependent ROC curve analysis.

我们研究了预后、使用时间依赖的ROC分析预测了ISgc的精度。 在不同时间截断点的AUC被用于衡量预后或者预测精度。“生存ROC”包被用于进行时间依赖的ROC曲线分析。




2.
X-tile plots offer a single and intuitive method to evaluate the association between variables and survival. The X-tile program can automatically choose the optimum data cutoff on the basis of the highest χ? value (minimum p value) formed by Kaplan–Meier survival analysis and log-rank test. We selected the optimum cutoff score for the density of each feature using X-tile software (version 3.6.1) based on the association with the patients’ DFS.

X-tile点图提供了单个的、直观的方法来评估变量二号生存率的相关性。X-tile程序可以根据最高卡方值自动选择最优的数据cutoff值（最低p-value），这是KM-生存分析和log-rank检验中的一部分。我们使用X-tile软件（3.6.1版本），选择了每个特征的最优cutoff值，依据是与病人DFS的关联。



3.
主要思路：
1.Lasso通过最小Cp值选择变量；
2.使用coef()获取变量的系数； 这个系数是标准化过的吗？
3.怎么求回归方程的截距？
用predict把零向量代进去，就有截距值了。predict(laa,t(c(0,0,0,0)))$fit

用R做Lasso如果使用lars，那么得到的系数是非标准化的，即原先变量的系数。（不信你把标准化的预处理后做下回归，会发现不一样的）。要求截距，确实用predict比较方便，当然根据数理推导代入公式也可以的。

http://f.dataguru.cn/forum.php?mod=viewthread&tid=265747&extra=&highlight=lasso&page=2
关于Fitting the Penalized Cox Model：https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf
X-tile画图： http://medicine.yale.edu/lab/rimm/research/software.aspx






一、Lasso概述
lasso estimate的提出是Robert Tibshirani在1996年JRSSB上的一篇文章Regression shrinkage and selection via lasso[2]。全称是least absolute shrinkage and selection operator。该方法是一种压缩估计。它通过构造一个罚函数得到一个较为精炼的模型，使得它压缩一些系数，同时设定一些系数为零。因此保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。Lasso算法是一种能够实现指标集合精简的估计方法。LASSO的主要作用是降维[1,4]，排除多重共线性，进行变量选择，是有偏估计[5]。


基本思想是在回归系数的绝对值之和小于一个常数的约束条件下，使残差平方和最小，从而能够产生某些严格等于0的回归系数，得到可以解释的模型。其想法可以用如下的最优化问题来表述：


其他降维方法，如逐步回归有可能遗漏最优方程，而Lasso在错误率方面有着无可替代的优势[3]，可用于初级基因组学中经常出现的高纬度数据。传统的回归方法（最小二乘、逐步回归）用来处理业务背景相对熟悉、影响因素比较明确且不容易出现异常点的实际问题较为合适，而对于业务背景未知特别是海量自变量的问题（如基因、文本挖掘、语音识别等），甚至自变量超过观测数的问题，基于收缩机制的Lasso方法是更好的选择。



 lasso estimate具有shrinkage和selection两种功能，shrinkage这个不用多讲，本科期间学过回归分析的同学应该都知道岭估计会有shrinkage的功效，lasso也同样。关于selection功能，Tibshirani提出，当t" role="presentation" style="box-sizing: border-box; display: inline; line-height: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px;position:inherit;" >

//todo 




========================================
|-- R语言做岭回归
----------------------------------------
ridge regression可以用来处理下面两类问题：一是数据点少于变量个数；二是变量间存在共线性。

当变量间存在共线性的时候，最小二乘回归得到的系数不稳定，方差很大。这是因为系数矩阵X与它的转置矩阵相乘得到的矩阵不能求得其逆矩阵，而ridge regression通过引入参数lambda，使得该问题得到解决。在R语言中，MASS包中的函数lm.ridge()可以很方便的完成。它的输入矩阵X始终为n x p 维，不管是否包含常数项。

Usage
lm.ridge(formula, data, subset, na.action, lambda = 0, model = FALSE,
         x = FALSE, y = FALSE, contrasts = NULL, ...)
#
lambda: A scalar or vector of ridge constants.



ridge3.3<-lm.ridge(y~.-1,longley,lambda=seq(0,0.02,0.0001))
#标准化数据要-1没截距项




##############
# code 1
#install.packages("MASS")
library('MASS')
longley 
names(longley)[1] <- "y"
lm.ridge(y ~ ., longley)
##                   GNP          Unemployed    Armed.Forces     Population      Year          Employed 
## 2946.85636017    0.26352725    0.03648291    0.01116105       -1.73702984   -1.41879853    0.23128785 
plot(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.001)))

select(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.0001)))
# modified HKB estimator is 0.006836982 
# modified L-W estimator is 0.05267247 
# smallest value of GCV  at 0.0057 





##############
# code 2
# 做岭回归，对于标准化后的数据模型不包含截距项，其中lambda为岭参数k的所有取值
ridge3.3<-lm.ridge(y~.,longley,lambda=seq(0,0.02,0.0001)) #seq(0,0.2,0.001)
str(ridge3.3)
plot(ridge3.3)

#
ridge.sol=ridge3.3
matplot(x=ridge3.3$lambda, y=t(ridge3.3$coef), 
        xlab = expression(lamdba), ylab= "Cofficients", type = "l", lty = 1:20) # lty = 1:20可加可不加，设置线的形状.
#作出lambdaGCV取最小值时的那条竖直线
abline(v = ridge3.3$lambda[which.min(ridge3.3$GCV)]) #GCV是什么？
#上图在lambad在0.006左右，自变量的系数值趋于稳定。
#下面的语句绘出lambda同GCV之间关系的图形：
plot(ridge.sol$lambda, ridge.sol$GCV, type = "l", xlab = expression(lambda), ylab = expression(beta))
abline(v = ridge.sol$lambda[which.min(ridge.sol$GCV)]) 
#语句ridge.sol$coef[which.min(ridge.sol$GCV)]  为找到GCV最小时对应的系数
ridge.sol$lambda[which.min(ridge.sol$GCV)] #[1] 0.0057

#在上面的代码中，还可以调整lambda = seq(0, 1, length = 2000)的范围及大小，
#如果lambad只是一个值，则得到的图像为空，只有在lambad变化的时候，才能得到岭迹曲线。







##############
# code 3 https://blog.csdn.net/li603060971/article/details/49508279
library('MASS')
head(longley)
#先标准化数据
data.norm=as.data.frame(apply(longley,2,scale) )
names(data.norm)[1] <- "y"
head(data.norm)
# y   GNP Unemployed Armed.Forces Population       Year   Employed
#
la=seq(0,1,0.01)
ridge.rs=lm.ridge(y~.-1, data=data.norm, lambda=la) #标准化数据要-1没截距项
head(ridge.rs)
plot(ridge.rs)
coef(ridge.rs)
#
#删除Population 由-变+
ridge.rs2<-lm.ridge(y~.-Population-1, data=data.norm,
                    lambda=seq(0,1,0.001))#岭回归
plot(ridge.rs2)
coef(ridge.rs2)
#
#删除year 由-变+
ridge.rs3<-lm.ridge(y~.-Population-Year-1, data=data.norm,
                    lambda=seq(0,1,0.001))#岭回归
plot(ridge.rs3)
coef(ridge.rs3) #全为正了
#
select(ridge.rs3)
#modified HKB estimator is 0.05941355 
#modified L-W estimator is 0.03466392 
#smallest value of GCV  at 0.479
lm.ridge(y~.-Population-Year-1, data=data.norm,lambda=0.479)
#     GNP      Unemployed   Armed.Forces  Employed 
# 0.4546171    0.1769713    0.1210725    0.3684340


#
# 下面利用ridge包中的linearRidge()函数进行自动选择岭回归参数
#install.packages('ridge')
library(ridge)
mod <- linearRidge(y ~ ., data = data.norm)
summary(mod)
## Call:
## linearRidge(formula = y ~ ., data = data.norm)
## 
## Coefficients:
##                Estimate Scaled estimate Std. Error (scaled) t value (scaled) Pr(>|t|)    
## (Intercept)  -3.106e-18              NA                  NA               NA       NA    
## GNP           3.995e-01       1.547e+00           3.419e-01            4.526  6.0e-06 ***
## Unemployed    1.026e-01       3.972e-01           2.323e-01            1.710   0.0873 .  
## Armed.Forces  8.903e-02       3.448e-01           1.765e-01            1.953   0.0508 .  
## Population   -1.825e-02      -7.068e-02           4.898e-01            0.144   0.8853    
## Year          2.897e-01       1.122e+00           2.493e-01            4.500  6.8e-06 ***
## Employed      2.195e-01       8.502e-01           4.630e-01            1.836   0.0663 .  
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
## 
## Ridge parameter: 0.01046912, chosen automatically, computed using 2 PCs
## 
## Degrees of freedom: model 3.67 , variance 3.218 , residual 4.123 

#从模型运行结果看，测岭回归参数值为0.01046912，2个变量的系数显著（GNP，Year）
#最后，利用Lasso回归解决共线性问题






##############
## code 4
library(lars)
## Loaded lars 1.2
x = as.matrix(longley[, 2:7])
y = as.matrix(longley[, 1])
(laa = lars(x, y, type = "lar")) #lars函数值用于矩阵型数据
## Call:
## lars(x = x, y = y, type = "lar")
## R-squared: 0.993 
## Sequence of LAR moves:
##      GNP Year Armed.Forces Unemployed Employed Population
## Var    1    5            3          2        6          4
## Step   1    2            3          4        5          6
# 由此可见，LASSO的变量选择依次是 GNP Year Armed.Forces Unemployed Employed Population
plot(laa)#绘出图
summary(laa)  #给出Cp值
## LARS/LAR
## Call: lars(x = x, y = y, type = "lar")
##   Df     Rss        Cp
## 0  1 1746.86 1210.0561
## 1  2 1439.51  996.6871
## 2  3   32.31   12.6400
## 3  4   23.18    8.2425
## 4  5   22.91   10.0505
## 5  6   22.63   11.8595
## 6  7   12.84    7.0000
## 根据课上对Cp含义的解释（衡量多重共线性，其值越小越好），我们取到第3步，使得Cp值最小，也就是选择GNP, Year, Armed.Forces这三个变量。

## -> 最后的是lar方法 不是lasso。type值变为lasso才对。
(las = lars(x, y, type = "lasso"))
## Call:
## lars(x = x, y = y, type = "lasso")
## R-squared: 0.993 
## Sequence of LASSO moves:
##      GNP Year Armed.Forces Unemployed Employed Population Year Employed Employed Year Employed Employed
## Var    1    5            3          2        6          4   -5       -6        6    5       -6        6
## Step   1    2            3          4        5          6    7        8        9   10       11       12
plot(las)
summary(las)
## LARS/LASSO
## Call: lars(x = x, y = y, type = "lasso")
##    Df     Rss        Cp
## 0   1 1746.86 1210.0561
## 1   2 1439.51  996.6871
## 2   3   32.31   12.6400
## 3   4   23.18    8.2425
## 4   5   22.91   10.0505
## 5   6   22.63   11.8595


哪个指标判断共线性？






========================================
五类模型的变量选择可采用R语言的glmnet包来解决。这五类模型分别是： //todo
----------------------------------------
1. 二分类logistic回归模型
2. 多分类logistic回归模型
3.Possion模型
4.Cox比例风险模型
5.SVM










ref:
https://blog.csdn.net/orchidzouqr/article/details/53582801
https://blog.csdn.net/jiabiao1602/article/details/39338181





========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------




========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------




========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------

