R03 - 统计学

先学习负二项分布


本文记录R和数理统计学。
还有一个paper中遇到的统计学: NGS/NGS_statistics.txt 


薛毅和陈立萍的《统计建模与R软件》（清华大学出版社，2007年）中



========================================
统计学资源汇总
----------------------------------------
G:\baiduDisk\考研数学2014基础班\概率统计

1.StatQuest系列笔记汇总
https://mp.weixin.qq.com/s?__biz=MzAxMzkzNDUyOQ==&mid=2247484185&idx=1&sn=59944cbe7a078148287707e25be70467&scene=21


2.  概率论与数理统计学习笔记
https://blog.csdn.net/hpdlzu80100

G:\baiduDisk\考研数学2014基础班\概率统计





3. 教科书《R语言与统计分析》 汤银才 主编 447页pdf





4.教科书《统计建模与R软件》上下册 共643页 薛毅陈立萍 编著
目录在PDF7页，







========================================
1.事件与概率
----------------------------------------
1. 概念的定义

(1)随机事件
 
条件相同可以重复进行
结果多样
实验前不知道结果

比如，丢色子；


(2)样本空间
所有事件的基本结果，用 欧米伽Ω 表示。

比如：丢6面色子有6个结果。
但是偶数面朝上，包含{2,4, 6}三个基本结果。






========================================
2.一维随机变量及其分布
----------------------------------------


========================================
|-- 泊松分布 Poisson distribution
----------------------------------------
关于泊松分布，不算特别好理解。可以参考资料：
http://www.ruanyifeng.com/blog/2013/01/poisson_distribution.html
http://maider.blog.sohu.com/304621504.html

1. 泊松分布主要满足3个条件：
（一）A为小概率事件
（二）A发生概率是稳定的
（三）A与下一次A事件的发生，是相互独立的

#泊松分布适合于描述单位时间（或空间）内随机事件发生的次数（事件发生的次数只能是离散的整数）。
#如某一服务设施在一定时间内到达的人数，电话交换机接到呼叫的次数，汽车站台的候客人数，
#机器出现的故障数，自然灾害发生的次数，一块产品上的缺陷数，显微镜下单位分区内的细菌分布数等等。

#P(X=k)=lambda^k/k!*e^lamda, k=0,1,2
#对于泊松分布而言，其均值和方差是相等的。而我们的测序数据，通常方差比均值还大，怎么办？

#λ是波松分布所依赖的唯一参数。 λ值愈小分布愈偏倚， 随着λ的增大 ， 分布趋于对称。 
#当λ=20时分布接近于正态分布；当λ=50时， 可以认为波松分布呈正态分布。



(2)Density, distribution function, quantile function and random generation for the Poisson distribution with parameter lambda.
dpois(x, lambda, log = FALSE)
ppois(q, lambda, lower.tail = TRUE, log.p = FALSE)
qpois(p, lambda, lower.tail = TRUE, log.p = FALSE)
rpois(n, lambda)
lower.tail = FALSE允许在默认情况下获得更精确的结果，lower.tail = TRUE将返回1


#产生泊松分布的点
points=rpois(300, lambda=4) #points
plot(density(points))



(3)#泊松分布 (poisson)
library(RColorBrewer)
display.brewer.all()
#
par(mar=c(5, 4, 4, 5))
n=6
color2=brewer.pal(n,"Dark2") #11个
#
for(i in seq(1,n) ){
  print(i)
  if(i==1){
    plot(dpois(0:15, lambda = 1), col = color2[i], xlim = c(-1,15), type="o",
         pch=i,
         xlab = "n", ylab = "Probability", main = "Poisson Distribution")
  }else{
    lines(0:15, y=dpois(0:15, lambda = i),col =color2[i], type="o",pch=i)
  }
}
#
legend("topright", legend=seq(1,n), 
       #inset=-0.15,xpd=TRUE,
       box.col="white",
       lty=1, pch=seq(1,6), col=color2)
box()
###end





2.一家医院，统计下来平均每分钟接待2个客人，问假设某次一分钟接待4个客人的概率是：
dpois(4, lambda = 2) #[1] 0.09022352
其中，参数2为泊松分布公式中的λ * t

泊松分布概率分布律图：
plot(dpois(0:30, lambda = 2), col = "red", xlim = c(-1,30), xlab = "发生次数", ylab = "概率", main = "泊松分布图")





3. 假设检验
poisson.test(x, T = 1, r = 1, alternative = c(“two.sided”, “less”, “greater”), conf.level = 0.95)

例: 
poisson.test(137, 24.19893)

##
## 		Exact Poisson test
## 
## data:  137 time base: 24.19893
## number of events = 137, time base = 24.199, p-value < 2.2e-16
## alternative hypothesis: true event rate is not equal to 1
## 95 percent confidence interval:
##  4.753125 6.692709
## sample estimates:
## event rate 
##   5.661407 





4. 基因表达量的分布不符合泊松分布。

横坐标为基因在所有样本中的均值，纵坐标为基因在所有样本中的方差，直线的斜率为1，代表泊松分布的均值和方差的分布。
可以看到，真实数据的分布是偏离了泊松分布的，方差明显比均值要大。

如果假定总体分布为泊松分布， 根据我们的定量数据是无法估计出一个合理的参数，能够符合上图中所示分布的，这样的现象就称之为over dispersion。

由于真实数据与泊松分布之间的overdispersion，选择泊松分布分布作为总体的分布是不合理。

以上只证明了泊松分布是个不太恰当的分布估计，那怎么证明负二项分布就是合适的分布估计呢？



refer:
https://blog.csdn.net/qq_33335484/article/details/80389807


========================================
|-- 正态分布( normal distribution /Gaussian又叫高斯分布) 与 over dispersion(过度离散)
----------------------------------------
1. 正态分布

dnorm(x, mean = 0, sd = 1, log = FALSE)
pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
rnorm(n, mean = 0, sd = 1)


#pic0
x <- seq(-4, 4, length.out = 100)
p <- dnorm(x, mean = 0, sd = 1)
plot(x, p, type = 'l', lwd = 2, col = "red",
     xlab = "Residual", ylab = "Density")
#


#pic1
library(RColorBrewer)
display.brewer.all()
color2=brewer.pal(n,"Dark2")
i=2
plot(dnorm(0:100, mean= i*10, sd=4), col = color2[i], xlim = c(0,100), type="o",
     pch=i,
     xlab = "n", ylab = "Probability", main = "Normal Distribution")
text(40,0.095,"mean=20,sd=4",col=color2[i])
#


#pic1.2
x <- seq(0, 40, length.out = 100)
p=dnorm(x, mean= i*10, sd=4)
plot(x,p, col = color2[i], xlim = c(0,40), type="l", lwd=2,
     xlab = "n", ylab = "Probability", main = "Normal Distribution")
text(28,0.095,"mean=20,sd=4",col=color2[i])
#


#pic2
data=rnorm(n=10000,mean=20,sd=4)
plot(density(data))
#






2. What does under or over-dispersion look like?
https://www.r-bloggers.com/what-does-under-or-over-dispersion-look-like/

(1) 正态分布
x <- seq(-4, 4, length.out = 100)
p <- dnorm(x, mean = 0, sd = 1)
plot(x, p, type = 'l', lwd = 2, col = "red",
    xlab = "Residual", ylab = "Density")
#

(2)更离散的分布， add an over-dispersed curve using the Student t distribution
p_t <- dt(x, df = 1.4, ncp = 0)

plot(x, p, type = 'l', lwd = 2, col = "red",
    xlab = "Residual", ylab = "Density")
lines(x, p_t, col = "darkblue", lwd = 2)

(3)更不离散的分布draw an under-dispersed distribution, using the Laplace distribution
library(rmutil)
p_l <- dlaplace(x, m = 0, s = 0.4)


(4) 以上三个图合起来
plot(x, p_l, xlim = c(-4, 4), lwd = 2, xlab = "Residuals", main = "", type = "l")
lines(x, p, col = "red", lwd = 2, lty = 2)
lines(x, p_t, col = "darkblue", lwd = 2)
legend("topright", legend = c("normal", "under-dispersed", "over-dispersed"),
      lty = c(2,1,1), col = c("red", "black", "darkblue"), lwd = 2)
#

(5) 画QQ图是检查是否符合正态分布的重要标准(注意：怎么理解QQ图？)
plot them on a normal QQ plot, which you may be familiar with. It is
one of the standard checks of model residuals:

set.seed(1997)
par(mfrow = c(1,3))
qqnorm(rnorm(1000, mean = 0, sd = 1), main = "normal")
abline(0,1)
qqnorm(rt(1000, df = 1.4, ncp = 0), main = "over-dispersed")
abline(0,1)
qqnorm(rlaplace(1000, m = 0, s = 0.4), main = "under-dispersed")
abline(0,1)

# 我的理解：QQ图取的是样品和理论值的百分位数字。
第一个值1%位置就是最小的值，过度离散的样品由于扁平分布，很小的地方(-60)还有值，而正态分布在-3位置已经是最小值了。
在100%分位数是最大值，过度离散样本最大值很大(250)，而正态分布最大值在3左右。
中间的部分则比较接近，在0的位置是相等的。


#改变参数，能加深对结果的理解。

#####
# 增大Laplace分布的scale值，它会变成过离散的。因为两侧尾巴相对正态分布越来越胖(虽然中位数还有峰)。
scale for the Laplace distribution to a larger number it will become over-dispersed, 
because it gets fatter tails than the normal (despite being more peaked at its mode).


#####
# 低离散比你想象的更常见。只要有审核过程，就有可能发生。
Under-dispersion is more common than you might think. It can occur when you have a censoring process. 
比如你的机器只能测试某个精度的值，更小的值四舍五入为0了。
For instance, perhaps your machine can only measure length’s to a certain precision and 
any distance that is to small gets rounded down to zero.





3. Overdispersion
https://en.wikipedia.org/wiki/Overdispersion

When the observed variance is higher than the variance of a theoretical model, overdispersion has occurred. 
当观察到的变异高于理论模型时，过离散就出现了。

Conversely, underdispersion means that there was less variation in the data than predicted.
相反，低离散意味着观察到的变异比期望的更少。

在应用数据分析中，过度离散是一个非常常见的特征，因为在实践中，总体常常是异质性的(非均匀的)，这与广泛使用的简单参数模型中隐含的假设相反。








========================================
|-- qqplot和qqnorm有什么区别
----------------------------------------
1.QQ图的主要作用是判断样本是否近似于某种类型的分布，这里的“QQ”是两个Quantiles的大写字母，即两个分位数，一个是样本分位数（Sample Quantiles），一般画在纵轴，一个是理论分位数（Theoretical Quantiles），一般画在横轴。

n <-rnorm(1000, 0, 1) #生成1000个均值为0，方差为1的正态分布样本
qqnorm(n) #做变量n的QQ图
abline(0,1) #画一个截距是0，斜率是1的直线

这个（x,y）形式的二维的散点图的坐标是如何确定的?
要回答这个问题需要先从qqnorm()入手，把QQ图的结果保存到变量中，再对这个变量进行分析。

# q是一个列表变量
q<- qqnorm(n) # 将变量n的QQ图的结果保存到变量q中
str(q)
q$x[1:5]
q$y[1:5]

变量q$y是样本分位数，也是刚才随机产生的1000个样本的值，即和变量n是完全一样的，为了验证，如果运行下面的代码，会得到1000个“TRUE”

table(n == q$y)

接下来讨论这个q$x，q$x是理论分位数，之所以是“理论”，指的是如果按照正态分布的假设（qqnorm对应的是正态分布假设，qqpois对应的是泊松分布的假设），这个分位数的理论值应该是多少。

例如如果有5个样本，那么理论分位数应该给出的是20%，40%，60%，80%，100%的分位数，但是100%的分位数一般会无限大，因此在这里需要进行一下数据处理，找一个“近似”的理论样本，来替代“真正”的理论样本。在这里问题就在这个“近似”的理论样本的数据处理方法。举个例子，假设P()为正态分布函数，是正态分布函数的反函数，假设一共有N个样本，则第n个样本的“真正”的理论样本分位数应该是 


直线由四分之一分位点和四分之三分位点这两点确定的，四分之一分位点的坐标中横坐标为实际数据的四分之一分位点(quantile(data,0.25)),纵坐标为理论分布的四分之一分位点(qF(0.25)),四分之三分位点类似，这两点就刚好确定了QQ图中的直线。

##理论x1=qnorm(0.25,0,1)
##实际y1=quantile(n,0.25)
qqnorm(n)
# points(qnorm(0.25,0,1), quantile(n,0.25), col='red',type="p",lwd=5)
x2=c(qnorm(0.25,0,1),qnorm(0.75,0,1))
y2=c(quantile(n,0.25),quantile(n,0.75))
reg=lm(y2~x2)
reg
abline(reg,col='blue')


//todo?? 公式不清楚




## N=100#设定样本数量，可以将100改为别的数进行验证
d <- matrix(nrow= N ,ncol= 3)#生成一个N *3的矩阵
d[,1]<-1:N #给每一次N的值编上序号
for(i in 2:N){ #从i=2开始计算，一直计算到i=N
  x <- rnorm(i,0,1) #生成均值为0,、方差为1的i个数的样本，保存到x
  y <- qqnorm(x) #生成样本x的QQ图，保存到变量y
  yy <- sort(y$x) #将QQ图中的理论样本值保存到变量yy中
  yyy<- pnorm(yy,0,1) # 计算分位数yy的概率，保存到yyy中
  a <- 1:i
  ayyy<-lm(a~yyy) #计算公式“ayyy=a*系数+截距”中的参数
  b <-summary(ayyy)$coefficients
  d[i,2]<-b[1,1] #将截距保存到第二列
  d[i,3]<-b[2,1] #将系数保存到第三类
}
d






########

qqplot应该是两样本的正态性对比，而qqnorm应该是样本与样本期望的正态性对比。
在R里面如果单是qqplot(x)，其中x为一组样本，运行时会出错的，但qqnorm(x)就不会……

其实关于这个问题我也不是很确定，只是之前尝试得到的一些经验。具体你可以查R里面的help()，那里对于两者的参数设置会比较详细


refer:
http://blog.sina.com.cn/s/blog_5eb5c9370102vyim.html





========================================
|-- 负二项分布 (negative binomial)
----------------------------------------
1. 二项分布

dbinom(x, size, prob, log = FALSE)
pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
rbinom(n, size, prob)

二项分布描述的是n重伯努利实验，在n重贝努利试验中，事件A恰好发生x(0≤x≤n)次的概率为：
Pn(x)=C(n,x)*p^x*(1-p)^(n-x)
参数n和p

#pic1
library(RColorBrewer)
display.brewer.all()
color2=brewer.pal(n,"Dark2")
i=2
plot(dbinom(0:100, size= i*20, prob=0.3), col = color2[i], xlim = c(0,25), type="o",
     pch=i,
     xlab = "n", ylab = "Probability", main = "Binomial Distribution")
text(20,0.12,"size=40,prob=0.3",col=color2[i])



#pic2
data=rbinom(n=10000,size=40,prob=0.3)
plot(density(data))









2.负二项分布

dnbinom(x, size, prob, mu, log = FALSE)
pnbinom(q, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
qnbinom(p, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
rnbinom(n, size, prob, mu)

负二项分布描述的也是伯努利实验，不过它的目标事件变成了：
对于Bernoulli过程，我们设定，当某个结果出现固定次数的时候，整个过程的数量，比如我们生产某个零件，假设每个零件的合格与否都是相互独立的，且分布相同，那么当我们生产出了r=5个不合格零件时，一共生产了多少合格的零件，这个数量就是一个负二项分布，公式如下：
f(k;r;p)=P(x=k)=C(r+k-1, k)*p^k*(1-p)^r

该公式描述的是，在合格率为p的一堆产品中，进行连续有放回的抽样，当抽到r个次品时，停止抽样，此时抽到的正品正好为k个的概率
它的概率分布如下：

#pic1
library(RColorBrewer)
display.brewer.all()
color2=brewer.pal(n,"Dark2")
i=2
plot(dnbinom(0:100, size= i*1, prob=0.1), col = color2[i], xlim = c(0,65), type="o",
     pch=i,
     xlab = "n", ylab = "Probability", main = "negative binomial distribution")
text(30,0.03,"size=2,prob=0.3",col=color2[i])

#pic2
data=rnbinom(n=10000,size=2,prob=0.1)
plot(density(data))



负二项回归。同泊松计数回归不同是，在负二项回归中假设方差大于均值，这种情形为通常称为过度离散(over dispersion)，相反，若方差小于均值，称之为低扩散(under dispersion)。负二项回归可以有效地对过离散数据建模，

负二项分布的均值和方差分别为:
mu=p*r/(1-p)
Sigma的平方  σ^2=p*r/(1-p)^2

将p用mu表示，得到：
p=mu/(mu+r)
1-p=r/(mu+r)

将上一步推出的p和1-p带入到方差的表达式中，得到
σ^2=p*r/(1-p)^2=mu^2/r + mu

记1/r=alpha，则
σ^2 = mu + alpha*mu^2 > mu

可见，方差是均值的二次函数，方差随着均值的增加而进行二次函数形式的递增，正好符合上文。
其中alpha和r称为dispersion parameter


负二项分布与泊松分布的关系，可以用α或r推出：
当 r -> ∞ 时，α -> 0，此时 σ2= μ，为泊松分布；
当 r -> 0 时，α -> ∞，此时 overdispersion



(2)
同泊松计数回归不同是，在负二项回归中假设方差大于均值，这种情形为通常称为过度离散(overdispersion)，相反，若方差小于均值，称之为低扩散(underdispersion)。

易见，这个负二项回归模型的方差为均值的二次函数，因此也被称为NB2模型。当时，NB2模型退化为泊松回归模型。负二项回归的系数可通过牛顿迭代法获得，在R中利用MASS包的glm.nb函数求取。最后一个问题，对于计数型数据集，如何判断它是否过度离散。因为在NB2中

σ^2=mu =alpha*mu^2

而在泊松回归中均值和方差相等，因此只需判断是否等于0，可通过R中的AER::dispersiontest函数实现。
具体的推导过程可参见Cameron和Trivedi在1990对这一问题的论述。


案例:
MASS包中的 quine 数据集描述了新南威尔士农村学校旷课情况，该数据集记录146个学生的宗教信仰(Eth), 性别(Sex),年龄(Age) ,学习状态(Lrn),旷课天数(Days)五个指标。我们想通过这一数据集合研究旷课天数与其他因素之间的关系。

library(MASS)
library(AER)
library(COUNT)

首先检查数据是否过度离散，先进行泊松回归，然后使用dispersiontest函数对泊松回归结果进行检测，看alpha是否大于0，注意需设置dispersiontest函数中的trafo=1，否则认为alpha大于1过离散。

po <- glm(Days ~ Sex+Age + Eth +Lrn, data = quine,family=poisson)
dispersiontest(po,trafo=1)
## 	Overdispersion test
## 
## data:  po
## z = 5.469, p-value = 2.263e-08
## alternative hypothesis: true alpha is greater than 0
## sample estimates:
##    alpha 
## 11.53013

过离散测试中alpha=11.53013,因此泊松回归不能有效反映这组数据真实特征，下面采用负二项回归研究这组数据

nb <- glm.nb(Days ~ Sex+Age + Eth +Lrn, data = quine)
#
list(nb=unlist(modelfit(nb)), po=unlist(modelfit(po)))
## $nb
##         AIC        AICn         BIC       BICqh 
## 1109.151018    7.596925 1130.036265    7.687628 
## 
## $po
##        AIC       AICn        BIC      BICqh 
## 2299.18363   15.74783 2320.06888   15.83854 

结果表明，对于quie数据集，在各项指标下，负二项回归都远比泊松回归好。

> nb
## 
## Call:  glm.nb(formula = Days ~ Sex + Age + Eth + Lrn, data = quine, 
##     init.theta = 1.274892646, link = log)
## 
## Coefficients:
## (Intercept)         SexM        AgeF1        AgeF2        AgeF3         EthN        LrnSL  
##     2.89458      0.08232     -0.44843      0.08808      0.35690     -0.56937      0.29211  
## 
## Degrees of Freedom: 145 Total (i.e. Null);  139 Residual
## Null Deviance:	    195.3 
## Residual Deviance: 168 	AIC: 1109






3. 方差估计
在生物学重复很少时，我们是很难准确计算每个基因表达的标准差的（相当于这个数据集的离散程度）。我们很可能会低估数据的离散程度。

被逼无奈的科学家提出了一个假设：表达丰度相似的基因，在总体上标准差应该也是相似的。我们把不同生物学重复中表达丰度相同的基因的总标准差取个平均值，低于这个值的都用这个值，高于这个值的就用算出来的值。

（图来自 H. J. Pimentel, et al. Differential analysis of RNA-Seq incorporating quantification uncertainty. bioRxiv, 2016）



(2)
然后为了建模，本来需要估计负二项分布的两个值，均值和方差，但是发现方差其实可以用均值和dispersion表示，于是就只要求均值和dispersion就行了。有了这两个值，就开以进行广义线性模型建模，就可以搞p值，找差异基因对吧。

另外，还有一个比较重要的值叫做Log Fold Change,这个受count数影响很大，原因是count数据是方差不齐性的，方差跟他的count均值有很大关系。

如果你的样本没有重复，那么你只要自己给每个基因都来一个dispersion就能算你最可爱的p值了，然后在低count，高LFC的基因留个心眼就行了。













refer: 
1.https://www.jianshu.com/p/ad24bb90b972
2.https://mp.weixin.qq.com/s/UTmSzCgDIFYbG2WByzaqQQ / http://www.jintiankansha.me/t/QicEQRzHra
3.转录组差异表达筛选的真相
https://mp.weixin.qq.com/s/VcjnvI5FqwOFEC9wSUfdSw

4.数萃大数据 2018-09-16 数据分析师应该知道的16种回归方法：负二项回归


========================================
----------------------------------------











========================================
3.二维随机变量及其分布
----------------------------------------


========================================
4.二维随机变量及其分布
----------------------------------------



========================================
5.大数定律和中心极限定理
----------------------------------------



========================================
6.数理统计基础
----------------------------------------


========================================
7.参数估计
----------------------------------------



========================================
统计学: x2检验（chi-square test）或称卡方检验
----------------------------------------

http://www.cnblogs.com/emanlee/archive/2008/10/25/1319569.html



========================================
线性代数
----------------------------------------
笔记： https://www.cnblogs.com/tongkey/p/7170890.html



========================================
微积分 //todo
----------------------------------------




========================================
----------------------------------------

========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------

