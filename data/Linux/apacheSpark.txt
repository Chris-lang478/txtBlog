Apache Spark 怎么用？




========================================
Review: Bioinformatics applications on Apache Spark
----------------------------------------
1. 
August 2018 https://academic.oup.com/gigascience/article/7/8/giy098/5067872

(1)是什么？
高扩展性、强大的计算系统。
并行平台中，Spark是快速、通用、使用内存、可迭代的计算框架，高容错、高扩展性。
Therefore, there is an urgent need for highly scalable and powerful computational systems. Among the state-of–the-art parallel computing platforms, Apache Spark is a fast, general-purpose, in-memory, iterative computing framework for large-scale data processing that ensures high fault tolerance and high scalability by introducing the resilient distributed dataset abstraction.

性能上，spark相对于Hadoop，内存访问提速100倍，硬盘访问提速10倍。
In terms of performance, Spark can be up to 100 times faster in terms of memory access and 10 times faster in terms of disk access than Hadoop.
Moreover, it provides advanced application programming interfaces in Java, Scala, Python, and R.

先进组件上，包括 Spark SQL 处理结构化数据， MLlib处理机器学习，GraphX处理图像，Spark Streaming处理流计算。
It also supports some advanced components, including Spark SQL for structured data processing, MLlib for machine learning, GraphX for computing graphs, and Spark Streaming for stream computing. 


(2)应用？
高通量测序，及其他生物领域：表观遗传学、进化树、药物发现。
We surveyed Spark-based applications used in next-generation sequencing and other biological domains, such as epigenetics, phylogeny, and drug discovery. The results of this survey are used to provide a comprehensive guideline allowing bioinformatics researchers to apply Spark in their own fields.





2. Spark in single-cell RNA sequencing

(1) 单细胞有助于理解生物学过程。
Single-cell RNA sequencing (scRNA-seq) is crucial for understanding biological processes. 

相对于 bulk RNA-seq实验，单细胞通常产生很多细胞的数据。
Compared with standard bulk RNA-seq experiments, scRNA-seq experiments typically generate a greater number of cell profiles. 

虽然有几个RNA-seq处理管线可用(比如 Halvade, SparkSeq, and SparkBWA)，他们不能处理多细胞。
Although several RNA-seq processing pipelines are available (such as Halvade, SparkSeq, and SparkBWA), they cannot process large numbers of profiles. 

Falco能处理多细胞转录组，使用HaDoop和Spark并行。
Therefore, Falco [32] was created to process large-scale transcriptomic data in parallel by using Hadoop and Spark. 

使用2个公开数据集，发现相对于单个节点，Falco可以至少提速2.6倍。
Experiments were conducted with two public scRNA-seq datasets. The results [32] showed that, compared with a highly optimized single-node analysis, Falco was at least 2.6 times faster. 

随着节点的提高，运行时间下降。
Moreover, as the number of computing nodes increased, running time decreased. 

还能使用低成本的AWS spot实例。
Furthermore, it allowed users to employ the low-cost spot instances of AWS, which reduced the cost of analysis by 65%.







3. Spark in variant association and population genetics studies

Effectively analyzing thousands of individuals and millions of variants is a computationally intensive problem. Traditional parallel strategies such as MPI/OpenMP show poor scalability. While Hadoop provides an efficient and scalable computing framework, it is heavily dependent on disk operations. Therefore, in 2015, O'Brien et al. proposed VariantSpark [33] to parallel population-scale tasks based on Spark and an associated machine-learning library, MLlib. Experiments that were conducted on 3,000 individuals with 80 million variants showed that VariantSpark was 80% faster than ADAM, the Hadoop/Mahout implementation, and ADMIXTURE [71]. Moreover, compared with R and Python implementations, it was more than 90% faster. In 2017, Di et al. proposed SEQSpark [34] to perform rare variant association analysis using Spark. It was evaluated with whole-genome and simulated exome sequence data. The former was completed in 1.5 hours and the latter in 1.75 hours. Moreover, it was always faster than Variant Association Tools and PLINK/SEQ; in some cases, running time was reduced to 1%.






4. 缺点和不足

(1) However, the nature of RDD means that Spark is not suitable for applications requiring asynchronous, fine-grained updates in execution, such as web service storage or incremental web crawlers and indexes. 
不支持异步，比如 网络服务存储、增量网络爬虫和索引。


(2) In addition, we must consider the potential complexity of creating and maintaining a Spark cluster. 
构建Spark集群很复杂。

(3) Moreover, when Spark runs on a commercial cloud-computing platform such as AWS, there is a certain delay in the transmission of large-scale datasets over the Internet. This issue does not exist when Spark runs on a local computer cluster. 
如果是云上运行，传数据很耗时。


(4) Furthermore, we need to learn a new API and perhaps even a new language (especially given the functional programming nature of the API).
需要学习心得API，甚至新的语言。




5. Conclusion

In summary, Spark is a fast and general-purpose computing framework designed for large-scale data processing. 

It ensures high fault tolerance and high scalability by introducing RDD abstraction and DAG scheduling. 

We believe that bioinformatics applications based on Spark will show promise in terms of performance for biological researchers in the future.


(2)
Spark引入了一个称为弹性分布式数据集（RDD，Resilient Distributed Dataset）的概念





ref:
1. Falco: a quick and flexible single-cell RNA-seq processing framework on the cloud 
https://academic.oup.com/bioinformatics/article/33/5/767/2559427
这文章也太短了？！



========================================
|-- Spark 简介与安装
----------------------------------------
1. Spark是一个Apache项目，它被标榜为“快如闪电的集群计算”。

https://spark.apache.org/
Lightning-fast unified analytics engine
Apache Spark™ is a unified analytics engine for large-scale data processing.


(1)它拥有一个繁荣的开源社区，并且是目前最活跃的Apache项目。
Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。
Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架，Spark，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是——Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。


(2)Spark 是在 Scala 语言中实现的，它将 Scala 用作其应用程序框架。与 Hadoop 不同，Spark 和 Scala 能够紧密集成，其中的 Scala 可以像操作本地集合对象一样轻松地操作分布式数据集。


(3) SparkR：SparkR是一个为R提供了轻量级的Spark前端的R包。 

SparkR提供了一个分布式的data frame数据结构，解决了 R中的data frame只能在单机中使用的瓶颈，它和R中的data frame 一样支持许多操作，比如select,filter,aggregate等等。（类似dplyr包中的功能）这很好的解决了R的大数据级瓶颈问题。 

SparkR也支持分布式的机器学习算法，比如使用MLib机器学习库。 

SparkR为Spark引入了R语言社区的活力，吸引了大量的数据科学家开始在Spark平台上直接开始数据分析之旅。






2. 安装
(1) 运行方式 Runs Everywhere
Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. It can access diverse data sources.

You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes. Access data in HDFS, Alluxio, Apache Cassandra, Apache HBase, Apache Hive, and hundreds of other data sources.


(2) 开始 Getting Started
Learning Apache Spark is easy whether you come from a Java, Scala, Python, R, or SQL background:

- Download the latest release: you can run Spark locally on your laptop.
	https://spark.apache.org/downloads.html
- Read the quick start guide.
	https://spark.apache.org/docs/latest/quick-start.html
- Learn how to deploy Spark on a cluster.
	https://spark.apache.org/docs/latest/#launching-on-a-cluster


需要先安装 HaDoop，等我搞明白后者是啥再继续吧。





ref:
https://www.cnblogs.com/wing011203/p/4737374.html




========================================
----------------------------------------





========================================
----------------------------------------




========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------




========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------




========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------

