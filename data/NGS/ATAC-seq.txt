ATAC-seq




========================================
ATAC-seq 简介 及教程
----------------------------------------

0. ATAC-seq经典之作
论文标题：Transposition of native chromatin for fast and sensitive epigenomic profiling of open chromatin, DNA-binding proteins and nucleosome position

表观测序领域大牛William J Greenleaf和Howard Y Chang的经典之作，引用上千次的ATAC经典原文
翻译: https://mp.weixin.qq.com/s?__biz=MzA4NTIyMzk4Mg%3D%3D&mid=2247483678&idx=1&sn=de7b6bcbf76c2a9051d0db7a4d9cd074



(2) harvard ATAC 分析流程
https://informatics.fas.harvard.edu/atac-seq-guidelines.html
https://informatics.fas.harvard.edu/atac-seq-guidelines-old-version.html
https://informatics.fas.harvard.edu/category/software.html



(3) ATAC 个性化分析思路
https://www.cnblogs.com/leezx/p/12953732.html


ChIP-seq流程(snakemake) https://zhuanlan.zhihu.com/p/48320500

质控指标 https://cloud.tencent.com/developer/article/1346044


(4) galaxy 培训资料
https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html







2. 基础
(1).复习核小体知识：https://en.wikipedia.org/wiki/ATAC-seq
核小体有间距，则间距部分会被Tn5酶插入测序接头，也就能看到reads 峰。
而核小体密集区域，则几乎没有reads，也就看不到任何峰。

如果一个基因处于开放区域，就是测到很多ATAC reads，则该基因一般就是高表达的。
如果一个promoter在开放区域，则其下游基因一般高表达。
如果一个enhancer 处在比较弱(少ATAC reads)的区域，则该 enhancer 不太容易发生近端效应。


(2) ATAC-seq 就是越松散的genome区域，测得的reads越多。该区域基因表达量比较多。
测序reads分布示意图 https://github.com/crazyhottommy/pyflow-ATACseq/blob/master/ATAC.jpg
https://blog.csdn.net/u012110870/article/details/102804164
https://www.jianshu.com/p/eb02b95cf049

ATAC文库构建通常借助Illumina的Nextera®（Illumina）试剂盒，然而Nextera文库涉及的序列不同于其他形式的文库。

### Tn5转座酶切割开放DNA
Tn5转座子切割相同的DNA片段并在片段两端添加如下序列，其中加粗斜体部分紧挨着文库的插入片段序列
序列如下:
Read 1 ——>  5’ TCGTCGGCAGCGTC[AGATGTGTATAAGAGACAG] 
Read 2 ——>  5’ GTCTCGTGGGCTCGG[AGATGTGTATAAGAGACAG] 
可以发现Read1、Read2部分序列一致： AGATGTGTATAAGAGACAG

### 随后PCR扩增连接P5、P7接头
连接测序接头以锚定在flowcell芯片上的接头以供测序反应：

序列如下：
[P5--index5-R1]  5’ AATGATACGGCGACCACCGAGATCTACAC[i5]TCGTCGGCAGCGTC 
[P7--index7-R2]  5’ CAAGCAGAAGACGGCATACGAGAT[i7]GTCTCGTGGGCTCGG

### 完整文库结构
Index 2 (i5)5’-[AATGATACGGCGACCACCGAGATCTACAC]IIIIIIII[TCGTCGGCAGCGTC->AGATGTGTATAAGAGACAG]-NNNNNN-[CTGTCTCTTATACACATCT<-CCGAGCCCACGAGAC]IIIIIIII[ATCTCGTATGCCGTCTTCTGCTTG]-3’ Index 1 (i7)

IIIIIIII: Index 2 (i5), 8 bases 
IIIIIIII: Index 1 (i7), 8 bases 
-NNNNNN-: 插入序列
ref:http://bioinformatics.cvr.ac.uk/blog/illumina-adapter-and-primer-sequences/

这个图很奇怪，
左边: P5,N5, index2(i5), 最后来个Read 1.
右边: P7,N7, index1(i7), 最后来个Read 2.
index内 14+19 + 19+15=67 形式上浪费这么多序列
	假设测150，还剩余 150-67=83
	假设测100，还剩余 150-67=33

如何过滤去接头？
如果截去接头序列需要在 CTGTCTCTTATACACATCT 位置截取，工具可以是cutadapt、trim_galore等类似可以自定义截去接头的软件。






(3) 分析过程： 五个大步骤
raw FASTQ cut adapter
mapping to the reference with aligner like bwa, bowtie2
sort alignment result (BAM files)
remove BAM file duplications
peak-calling with MACS2


测序数据文件是两个重复的双端测序：
ATAC_seq_rep1_R1.fq.gz ATAC_seq_rep1_R2.fq.gz 
ATAC_seq_rep2_R1.fq.gz ATAC_seq_rep2_R2.fq.gz 
ATAC_seq_rep3_R1.fq.gz ATAC_seq_rep3_R2.fq.gz 



ENCODE 项目公开了ATAC数据、标准和分析过程（基于wdl的），值得参考。
提供了从原始的fastq数据开到，到peak caling结束的基础分析功能，尽管缺少了下游的差异分析和motfi分析，这套流程依然值得推荐。
https://github.com/ENCODE-DCC/atac-seq-pipeline

trim, mapping, peak calling三部曲：通过cutadapt软件去除adapter和低质量序列，然后是bowitie2比对参考基因组，最后调用MACS2进行peak calling。






========================================
ATAC-seq分析走流程：基础分析 (测试数据)
----------------------------------------

1. 准备数据：用哪一套数据？
https://blog.csdn.net/weixin_43569478/article/details/108079790

(1) 检索 GEO
all: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE97669
ATAC-seq for HeLa cell line(n=2) PMID: 29731168
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE106145


ATAC-seq for MDA-MB-231, ctrl vs KO(n=6, 2ctrl, 2KO*2)
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE97583


HeLa ATAC-Seq, address the effect of TLK loss on chromatin accessibility(n=6, 2ctrl, 2KO*2)
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE131022


(2) 选中这个，并下载
ATAC-seq of MCF-7 cells post ligand treatment(n=18) 这个数据不错。


文章: https://www.ncbi.nlm.nih.gov/pubmed/31353221
Guan J, Zhou W, Hafner M, Blake RA et al. Therapeutic Ligands Antagonize Estrogen Receptor Function by Impairing Its Mobility. 
Cell 2019 Aug 8;178(4):949-963.e18. PMID: 31353221


Results: 
Ligand 4-OH tamoxifen, a selective ER modulator (SERM), significantly alters chromatin accessibility and partially mimics the effect of natural ligand E2 on chromatin accessibility in MCF-7 breast cells. 
Selective ER degraders (SERD) fulvestrant and GDC-0927 on the other hand have very little impact on chromatin accessibility.

https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117940
GSM3315603	Vehicle Rep 1 SRR7629152
GSM3315604	Vehicle Rep 3
GSM3315605	Vehicle Rep 2

GSM3315612	4-OH tamoxifen Rep 3  SRR7629161
GSM3315613	4-OH tamoxifen Rep 2
GSM3315614	4-OH tamoxifen Rep 1


https://www.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA483774
$ cd /home/wangjl/data/ATAC/raw
$ cat SRR_Acc_List.txt
SRR7629152
SRR7629153
SRR7629154
SRR7629161
SRR7629162
SRR7629163

$ cat SRR_Acc_List.txt | head -n 3 | while read id; do echo $id; 
fasterq-dump --split-files -e 10 $id;
done;


$ cat SRR_Acc_List.txt | tail -n 3 | while read id; do echo $id; 
fasterq-dump --split-files -e 10 $id;
done;

# 22:36 --> 0:09, 1.5h 66G,
$ ls -lh
total 66G
-rw-rw-r-- 1 wangjl wangjl 4.3G Jun 17 22:48 SRR7629152_1.fastq
-rw-rw-r-- 1 wangjl wangjl 4.3G Jun 17 22:48 SRR7629152_2.fastq
-rw-rw-r-- 1 wangjl wangjl 5.9G Jun 17 23:27 SRR7629153_1.fastq
-rw-rw-r-- 1 wangjl wangjl 5.9G Jun 17 23:27 SRR7629153_2.fastq
-rw-rw-r-- 1 wangjl wangjl 5.3G Jun 18 00:01 SRR7629154_1.fastq
-rw-rw-r-- 1 wangjl wangjl 5.3G Jun 18 00:01 SRR7629154_2.fastq

-rw-rw-r-- 1 wangjl wangjl 4.8G Jun 17 22:51 SRR7629161_1.fastq
-rw-rw-r-- 1 wangjl wangjl 4.8G Jun 17 22:50 SRR7629161_2.fastq
-rw-rw-r-- 1 wangjl wangjl 6.3G Jun 17 23:32 SRR7629162_1.fastq
-rw-rw-r-- 1 wangjl wangjl 6.3G Jun 17 23:32 SRR7629162_2.fastq
-rw-rw-r-- 1 wangjl wangjl 6.5G Jun 18 00:09 SRR7629163_1.fastq
-rw-rw-r-- 1 wangjl wangjl 6.5G Jun 18 00:09 SRR7629163_2.fastq


Reads很短，就40bp?
$ head SRR7629152_1.fastq 
@SRR7629152.1 1 length=41
CACCANTGGCAGCCTAGCATTAGCAGGAATACCTTTCCTCA
+SRR7629152.1 1 length=41
A/AAA#AEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/


(3) 先压缩一下，减少磁盘占用
$ cat SRR_Acc_List.txt | tail -n 3 | while read id; do echo $id; 
gzip ${id}_1.fastq;
done;

变换 _1 为 _2, tail 为 head，共4种组合。
9:40 --> 10:28 可能之前就结束了。缩小为原来的 20%。

$ ls -lth
total 13G
-rw-rw-r-- 1 wangjl wangjl 1.3G Jun 18 00:09 SRR7629163_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.3G Jun 18 00:09 SRR7629163_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.1G Jun 18 00:01 SRR7629154_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.1G Jun 18 00:01 SRR7629154_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.2G Jun 17 23:32 SRR7629162_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.2G Jun 17 23:32 SRR7629162_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.2G Jun 17 23:27 SRR7629153_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.1G Jun 17 23:27 SRR7629153_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 909M Jun 17 22:51 SRR7629161_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 935M Jun 17 22:50 SRR7629161_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 826M Jun 17 22:48 SRR7629152_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 863M Jun 17 22:48 SRR7629152_2.fastq.gz

(4) 先抽取部分测试
$ zcat SRR7629152_1.fastq.gz | wc
111588868 223177736 4533824964
111/4 Million reads /sample. 抽取 1e5 reads/sample.

$ mkdir ../fq
$ cat SRR_Acc_List.txt | while read id; do echo $id; 
zcat ${id}_1.fastq.gz | head -n 400000 | gzip > ../fq/s_${id}_1.fastq.gz;
zcat ${id}_2.fastq.gz | head -n 400000 | gzip > ../fq/s_${id}_2.fastq.gz;
done;
# 几秒结束。

$ cd ../fq/
$ zcat s_SRR7629163_2.fastq.gz  | wc
 400000  800000 15147256















========================================
|-- QC: raw fastq 
----------------------------------------
先用抽样数据走流程
$ cd /data/wangjl/ATAC/fq/

(1) 质控
$ mkdir QC_raw
$ fastqc -t 10 *.fastq.gz -o QC_raw/


$ sudo pip3 install multiqc
$ multiqc --version
multiqc, version 1.10.1

$ mkdir QC_raw/multiqc/
$ multiqc QC_raw/*fastqc.zip -o QC_raw/multiqc/

检查: 
$ http-server -p 12345
多数40bp长度，没有其他异常，连adapter都没有。



========================================
|-- 去接头
----------------------------------------
(2) 去接头
https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/src/detect_adapter.py
'Nextera ': b'CTGTCTCTTATA', #经过grep检测，这个接头最多，其他2个没有发现。


$ id="SRR7629163" && cutadapt -j 4 --time 1 -e 0.1 -O 3 --quality-cutoff 25 \
-m 15 -a CTGTCTCTTATA \
-A AGACGGGAAGAGCGTCGTGAGGGAAGAGTGTAGATCTCGGTGGTCGCGTATCATT \
-o clean/${id}_1.fastq.gz -p clean/${id}_2.fastq.gz s_${id}_1.fastq.gz s_${id}_2.fastq.gz > clean/cut.log 2>&1
## 98.1% rewrite.

$ ls *1.fastq.gz | sed -e 's/s_//' -e 's/_1.fastq.gz//'
SRR7629152
SRR7629153
SRR7629154
SRR7629161
SRR7629162
SRR7629163

$ ls *1.fastq.gz | sed -e 's/s_//' -e 's/_1.fastq.gz//' | while read id; do echo $id; 
cutadapt -j 4 --time 1 -e 0.1 -O 3 --quality-cutoff 25 \
-m 15 -a CTGTCTCTTATA \
-A AGACGGGAAGAGCGTCGTGAGGGAAGAGTGTAGATCTCGGTGGTCGCGTATCATT \
-o clean/${id}_1.fastq.gz -p clean/${id}_2.fastq.gz s_${id}_1.fastq.gz s_${id}_2.fastq.gz >> clean/cut.log 2>&1;
done;



========================================
|-- mapping with BWA
----------------------------------------
(3). mapping with BWA
因为reads太短了，决定使用 bwa。
$ bwa 
Program: bwa (alignment via Burrows-Wheeler transformation)
Version: 0.7.17-r1188

#########################
# 构建索引
#########################
UCSC GRCh38
$ cd /home/wangjl/data/ref/human/UCSC/
$ bwa index -a bwtsw -p bwa/hg38_fa hg38.fa.gz
其中 -p bwa/hg38_ 是指定前缀
939M Jan 16  2014 hg38.fa.gz
[main] Real time: 2974.649 sec(49min); CPU: 2929.810 sec

$ ls /home/wangjl/data/ref/human/UCSC/bwa -lth
total 5.3G
-rw-rw-r-- 1 wangjl wangjl 1.5G Jun 18 15:39 hg38_fa.sa
-rw-rw-r-- 1 wangjl wangjl  21K Jun 18 15:24 hg38_fa.amb
-rw-rw-r-- 1 wangjl wangjl  22K Jun 18 15:24 hg38_fa.ann
-rw-rw-r-- 1 wangjl wangjl 766M Jun 18 15:24 hg38_fa.pac
-rw-rw-r-- 1 wangjl wangjl 3.0G Jun 18 15:23 hg38_fa.bwt


#########################
# 比对 & sort
#########################
bwa mem genome.fa  A1_clean_1.fq.gz A1_clean_2.fq.gz | samtools sort -O bam -T A1 > A1.bam

$ cd /data/wangjl/ATAC/fq
$ mkdir map

$ id="SRR7629163"
$ bwa mem -t 10 -R "@RG\tID:${id}\tSM:${id}" /home/wangjl/data/ref/human/UCSC/bwa/hg38_fa  clean/${id}_1.fastq.gz clean/${id}_2.fastq.gz -o map/${id}.sam
##[main] Real time: 6.269 sec; CPU: 34.911 sec
## -t 10 线程数

$ grep -v '^[#@]' map/SRR7629163.sam |wc
 196244 3373449 38998448

$ samtools sort -O bam -T ${id}_tmp map/${id}.sam > map/${id}.sort.bam
## -T 是临时文件夹
$ ls -lth map/
total 45M
-rw-rw-r-- 1 wangjl wangjl 7.6M Jun 18 16:18 SRR7629163.sort.bam  #体积缩小为原来的19%。
-rw-rw-r-- 1 wangjl wangjl  38M Jun 18 16:15 SRR7629163.sam


二合一，批量（注意：使用双引号，变量才能替换）
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
bwa mem -t 10 -R "@RG\tID:${id}\tSM:${id}" /home/wangjl/data/ref/human/UCSC/bwa/hg38_fa  clean/${id}_1.fastq.gz clean/${id}_2.fastq.gz | samtools sort -O bam -T ${id}_tmp > map/${id}.sort.bam;
done;

$ ls -lth map/
total 44M
-rw-rw-r-- 1 wangjl wangjl 7.6M Jun 18 16:21 SRR7629163.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.0M Jun 18 16:21 SRR7629162.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.0M Jun 18 16:21 SRR7629161.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.9M Jun 18 16:21 SRR7629154.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.0M Jun 18 16:21 SRR7629153.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.6M Jun 18 16:21 SRR7629152.sort.bam

$ samtools view map/SRR7629154.sort.bam | wc
 197452 3398838 39337777




#########################
# index
#########################
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
samtools index map/${id}.sort.bam;
done;







========================================
|-- 去除线粒体上的reads，去重复 (推荐使用 sambamba，其次 picard，最不推荐 samtools)
----------------------------------------
由于细胞器DNA蛋白结合少，所以显然更容易被Tn5 转座酶切割，普通的ATAC-Seq的read就会有大量是细胞器的DNA，这就是为啥需要用INTACT技术。

此外如果不是PCR-free的建库方法，会有大量重复的read，也就需要标记或去除重复。



1. 去掉线粒体上的reads。
首先将不含质体的染色体名称写到一个chrlist文件中，染色体名称之间用空格隔开，然后执行如下命令即可得到去除质体的bam
$ samtools view -b A1.bam `cat chrlist` > A1.del_MT_PT.bam


执行
$ samtools view  map/SRR7629153.sort.bam| grep -v "^#"| awk '{print $3}' | sort | uniq -c| grep -E -v "GL|KI|JH|KB|chrM"
    678 *
  16554 chr1
   5261 chr10
   5175 chr11
   5527 chr12
   2483 chr13
   4475 chr14
   3155 chr15
   3655 chr16
   6137 chr17
   1604 chr18
   2189 chr19
   9700 chr2
   5257 chr20
   1694 chr21
   1163 chr22
   8552 chr3
   5334 chr4
   8615 chr5
   6031 chr6
   7196 chr7
   7400 chr8
   4485 chr9
   3427 chrX
    215 chrY

$ cat map/chrlist
chr1 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr2 chr20 chr21 chr22 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chrX chrY


$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id; 
samtools view -b map/${id}.sort.bam `cat map/chrlist` > map/${id}.del_MT.sort.bam;
done;






2. 去重复
用于后续分析的reads需要是唯一比对且去重复的，bwa比对结果可以通过MAPQ值来提取唯一比对reads，可以用picard、sambamba等软件去除dup，最终得到唯一比对且去重复的bam文件。

samtools rmdup 是如何行使去除PCR重复reads功能的？和 picard MarkDuplicates 现在改名为 GATK MarkDuplicates 有什么区别？

(1) The Genome Analysis Toolkit (GATK) v4.1.9.0
HTSJDK Version: 2.23.0
Picard Version: 2.23.3

$ gatk MarkDuplicates
Using GATK jar /data/wangjl/soft/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar
Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/wangjl/soft/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar MarkDuplicates
USAGE: MarkDuplicates [arguments]

命令行给出的例子
java -jar picard.jar MarkDuplicates \
I=input.bam \
O=marked_duplicates.bam \
M=marked_dup_metrics.txt



## 命令方式1: 新语法，官方推荐
$ id="SRR7629152"
$ gatk MarkDuplicates -I map/${id}.del_MT.sort.bam -O map/${id}.rmdup.bam -M map/${id}.rmdup.matrix -ASO coordinate -REMOVE_DUPLICATES true

$ samtools view map/SRR7629152.rmdup.bam | wc  #确实删掉了些行
 143879 2604778 31278683
$ samtools view map/SRR7629152.del_MT.sort.bam | wc
 145083 2481870 28650942


## 命令方式2: 控制内存和CPU占用，更安全的用法。
$ PICARD="/data/wangjl/soft/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar"
$ java -Xms5g -Xmx10g -XX:ParallelGCThreads=4 \
    -jar ${PICARD} MarkDuplicates \
    I=map/${id}.del_MT.sort.bam O=map/${id}.rmdup.bam M=map/${id}.rmdup.matrix \
    ASO=coordinate REMOVE_DUPLICATES=true 2>map/rmdup.log

$ samtools view map/SRR7629152.rmdup.bam | wc
 143879 2604778 31278683
和 方式1 结果一样。


批量运行，使用新语法
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id; 
gatk MarkDuplicates -I map/${id}.del_MT.sort.bam -O map/${id}.rmdup.bam -M map/${id}.rmdup.matrix -ASO coordinate -REMOVE_DUPLICATES true;
done;



$ ls -lth map
total 147M
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629163.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 8.5M Jun 18 21:38 SRR7629163.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629162.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 7.3M Jun 18 21:38 SRR7629162.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629161.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 7.2M Jun 18 21:38 SRR7629161.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629154.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 8.5M Jun 18 21:38 SRR7629154.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629153.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 7.0M Jun 18 21:38 SRR7629153.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:37 SRR7629152.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 8.4M Jun 18 21:37 SRR7629152.rmdup.bam






(2) 使用 samtools rmdup 去重复 (去的太多了，不建议用)
Usage:  samtools rmdup [-sS] <input.srt.bam> <output.bam>
-s    rmdup for SE reads
-S    treat PE reads as SE in rmdup (force -s)

$ samtools rmdup -S map/${id}.del_MT.sort.bam map/${id}.samtools_rmdup.bam
[bam_rmdupse_core] 4139 / 144722 = 0.0286 in library '	'

$ samtools view map/SRR7629152.samtools_rmdup.bam | wc
 140944 2408371 27736668
这个删除的有点多啊，
samtools 140944 (保留97.14%)，对比着 picard 的 143879 (保留99.17%)





(3) 用samblaster和sambamba来代替picard做sam文件的去重复，这两款软件比picard快30倍 (保留reads数和picard一样)
https://www.it610.com/article/1228439887515062272.htm

要注意的是picard Markduplicates 和sambamba markdup的输入文件是bam格式，samblaster是sam格式。这里只测试 sambamba。
sambamba 主要有filter，merge,slice和duplicate等七个功能来处理sam/bam文件。


## 安装
https://github.com/lomereiter/sambamba/releases
$ wget https://github.com/biod/sambamba/releases/download/v0.8.0/sambamba-0.8.0-linux-amd64-static.gz
$ gunzip sambamba-0.8.0-linux-amd64-static.gz
$ chmod +x sambamba-0.8.0-linux-amd64-static
$ ln -s /home/wangjl/soft/sambamba-0.8.0-linux-amd64-static ~/bin/sambamba


## 查看版本号
$ sambamba --version
sambamba 0.8.0
 by Artem Tarasov and Pjotr Prins (C) 2012-2020
    LDC 1.10.0 / DMD v2.080.1 / LLVM6.0.1 / bootstrap LDC - the LLVM D compiler (0.17.4)

$ sambamba markdup --help
Usage: sambamba-markdup [options] <input.bam> [<input2.bam> [...]] <output.bam>
       By default, marks the duplicates without removing them
-r, --remove-duplicates  remove duplicates instead of just marking them
-t, --nthreads=NTHREADS  number of threads to use
--tmpdir=TMPDIR    specify directory for temporary files



##--> 去重
$ sambamba markdup -r -t 5 --tmpdir map/tmp/${id} map/${id}.del_MT.sort.bam map/${id}.sambamba_rmdup.bam

$ ls -lth map/
total 155M
-rw-rw-r-- 1 wangjl wangjl 2.7M Jun 18 22:00 SRR7629152.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 6.1M Jun 18 22:00 SRR7629152.sambamba_rmdup.bam
drwxrwxr-x 2 wangjl wangjl 4.0K Jun 18 22:00 SRR7629152_tmp
同时生成一个bai文件。注意临时文件夹没删

$ samtools view map/SRR7629152.sambamba_rmdup.bam | wc
 143879 2460899 28401109

reads数和 picard 一模一样，速度更快，且文件更小。


批量运行
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id; 
sambamba markdup -r -t 5 --tmpdir map/tmp/${id} map/${id}.del_MT.sort.bam map/${id}.sambamba_rmdup.bam;
done;


$ ls -lth map/
total 196M
-rw-rw-r-- 1 wangjl wangjl 2.7M Jun 18 22:04 SRR7629163.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 6.2M Jun 18 22:04 SRR7629163.sambamba_rmdup.bam
drwxrwxr-x 8 wangjl wangjl 4.0K Jun 18 22:04 tmp
-rw-rw-r-- 1 wangjl wangjl 2.5M Jun 18 22:04 SRR7629162.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 5.4M Jun 18 22:04 SRR7629162.sambamba_rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 5.2M Jun 18 22:04 SRR7629161.sambamba_rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 2.5M Jun 18 22:04 SRR7629161.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 2.6M Jun 18 22:04 SRR7629154.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 6.3M Jun 18 22:04 SRR7629154.sambamba_rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 5.1M Jun 18 22:04 SRR7629153.sambamba_rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 2.5M Jun 18 22:04 SRR7629153.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 2.7M Jun 18 22:04 SRR7629152.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 6.1M Jun 18 22:04 SRR7629152.sambamba_rmdup.bam







3. 去除mapQ质量较低的reads
$ samtools view -h -f 2 -q 30 ${filename}.rmdup.bam


samtools view -h -f 2 -q 30 ${filename}.rmdup.bam \
| grep -v -e "mitochondria" -e "*" -e "chloroplast" \
| samtools sort -O bam -@ 10 -o - > ${filename}.last.bam
# 实时监测我们的数据发生了什么变化
samtools index ${filename}.last.bam
samtools flagstat ${filename}.last.bam > ./${filename}.last.stat








========================================
|-- map QC: deepTools 系列工具，比对结果质控 (TSS 热图 、插入片段长度分布图)
----------------------------------------
比对后，去掉线粒体、去重后的bam，再index后用于下游分析。


1.Reads在TSS前后的分布图

比对后得到的bam文件可以转化为bigWig（bw）格式，可通过可视化软件进行展示。deeptools软件可以实现bw格式转化和可视化展示。

(1) 首先需要在linux环境中安装deeptools软件，可以用以下命令实现bam向bw格式的转换：
$ id="SRR7629163"
## sambamba index -t 6 ${id}.sambamba_rmdup.bam ## 之前rmdup时已经自动生成过了
$ bamCoverage -b map/${id}.sambamba_rmdup.bam -o QC_map/${id}.bw;  ##必须提前index


## 批量化
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
bamCoverage -b map/${id}.sambamba_rmdup.bam -o QC_map/${id}.bw;
done;


二进制文件，也没法直接看文件内容。不报错就认为没问题吧。
得到的BW文件可以用 IGV 进行可视化





(2) 可以使用deeptools软件展示reads在特定区域的分布，如：
computeMatrix reference-point   \ # reference-pioint表示计算一个参照点附近的reads分布，与之相对的是scale-regions，计算一个区域附近的reads分布
       --referencePoint TSS   \#以输入的bed文件的起始位置作为参照点
       -S  A1.bw \ #可以是一个或多个bw文件
       -R  gene.bed \ #基因组位置文件
       -b 3000   \ #计算边界为参考点上游3000bp
       -a 3000   \ #计算边界为参考点下游3000bp，与-b合起来就是绘制参考点上下游3000bp以内的reads分布
       -o  A1.matrix.mat.gz \ #输出作图数据名称
#图形绘制
plotHeatmap \
 -m  A1.matrix.mat.gz\ #上一步生成的作图数据
 -out A1.pdf \ # 输出图片名称


computeMatrix 有两种模式可以选择，这里我们用的是reference-point，另外还有scale-regions，
- 前者适合看一个点附近的信号，后者则适合看指定的不同长度的区域。
- 当我们选用reference-point模式时，会默认用bed文件的第二列作为中心扩展。
- 工作原理 https://www.jianshu.com/p/ab2bb3f55d6f





## (i)下载bed格式的基因组位置文件(The BED file of the gene model can be downloaded from UCSC Table Browser.)
http://genome.ucsc.edu/cgi-bin/hgTables?command=start

genome: human
assembly: Dec. 2013 (GRCh38/hg38)
group: Genes and Gene Predictions
track: NCBI RefSeq
table: RefSeq All(ncbiRefSeq)
region: genome
output format: BED - browser extensible data
output file: hg38_refseq_whole_gene.bed

Click ‘get output’ button, and in the next page ‘Output refGene as BED’ click ‘get BED’ button.




##(ii) 统计
单个测试
$ computeMatrix reference-point   \
       --referencePoint TSS   \
       -S  QC_map/${id}.bw \
       -R  /home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.bed \
       -b 3000   \
       -a 3000   \
       -o  QC_map/matrix/${id}.matrix.mat.gz
很慢: 17:02-> 18:07


批量化，放后台运行。
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
computeMatrix reference-point   \
       --referencePoint TSS   \
       -S QC_map/${id}.bw \
       -R /home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.bed \
       -b 1000 \
       -a 1000 \
       -o QC_map/matrix/${id}.mat.gz &
done;
## 检查后台 $ ps -aux | grep computeMatrix
风扇声音很大。

https://deeptools.readthedocs.io/en/latest/content/tools/plotHeatmap.html
https://deeptools.readthedocs.io/en/latest/content/tools/plotProfile.html
$ plotHeatmap -m $matrix -out $heatmap #上下图都有
$ plotProfile -m $matrix -out $profile #只有上图





## (iii) 画图
单个测试
$ mkdir QC_map/pdf
$ plotHeatmap -m QC_map/matrix/${id}.mat.gz -out QC_map/pdf/${id}.pdf

批量化
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
plotHeatmap -m QC_map/matrix/${id}.mat.gz -out QC_map/pdf/${id}_.pdf;
done;


目测：峰图和热图都出来了，5/6看着正常。其中一个热图不正常，没颜色变化。

上面的峰图 profile 图：表示在每个gene的TSS位点的 reads 富集情况，可以理解为 peak，这说明 peak 大多数分布在TSS区域。
下面的热图 gene heatmap图：每行就是一个基因，因此这个热图一般很长。





(3) 还可以对 TES 可视化
$ id="SRR7629163"
$ computeMatrix scale-regions  -p 15  \
    -b 2000 -a 2000 \
    -R /home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.bed \
    -S QC_map/${id}.bw  \
    --skipZeros -o QC_map/matrix/${id}.body.gz  ## 耗时 10:02 --> 10:11(9min)
$ plotHeatmap -m QC_map/matrix/${id}.body.gz -out QC_map/matrix/${id}.body.Heatmap.pdf --plotFileFormat pdf  ## 上下图都有。
$ plotProfile -m QC_map/matrix/${id}.body.gz -out QC_map/matrix/${id}.body.Profile.pdf --plotFileFormat pdf --perGroup # 这个干啥的？貌似只有上图。

还可以输入多个参考基因组 -R all.bed chrY.bed ，比如一个全基因组bed，一个chrY.bed，然后 profile 就是2条不同颜色的线。
还可以输入多个输入 -S A.bw B.bw 

上图是 profile 图，可见基因 body 区， reads 富集程度远小于TSS/TES，因为大多数 peak 都不在基因内部（在基因间区）。






ref:
https://www.jianshu.com/p/9aa719faa4b5









2. 片段长度分布图(bam第9列)
https://www.jianshu.com/p/0a8d57dbfc3c

insert size与 fragment length 差别 http://tiramisutes.github.io/2016/09/19/Insert-Size.html

(1) 获取insert fragment 长度数据
$ echo $id  #SRR7629163

$ samtools view map/${id}.sambamba_rmdup.bam | \
awk -F'\t' 'function abs(x){return ((x < 0.0) ? -x : x)} {print $1"\t"abs($9)}' | \
sort | uniq | cut -f2 > map/${id}.fragment.length.txt
解释: 
awk中
	-F"\t" 按照tab分割
	然后定义一个函数 function abs(x){return ((x < 0.0) ? -x : x)} 表示返回绝对值。
	{print $1"\t"abs($9)} 打印第一列、第9列的绝对值
	
	
感觉不该使用uniq，因为不需要删重复？
$ samtools view map/${id}.sambamba_rmdup.bam | awk -F'\t' 'function abs(x){return ((x < 0.0) ? -x : x)} {print $1"\t"abs($9)}' |  sort | uniq -c | sort -k1n | head -n 80| tail
 1 SRR7629163.94576	0
      1 SRR7629163.97738	0
      1 SRR7629163.979	0
      1 SRR7629163.98342	0
      1 SRR7629163.98752	0
      1 SRR7629163.99702	0
      2 SRR7629163.100000	66
      2 SRR7629163.10001	45
      2 SRR7629163.10003	53
      2 SRR7629163.10004	61
目测除了0是出现1次，其他都是2次出现。
着重复是怎么来的呢？嗯，理解了，就是双端测序，每一端一行，一对reads是2行，分别记录本行信息和另一行的坐标，插入长度正负号不同。
$ samtools view map/${id}.sambamba_rmdup.bam | grep -P "SRR7629163.10004"
SRR7629163.10004	163	chr3	73049179	60	41M	=	73049200	61	AACTAAATGAATACATTCAAGATTAGAATACTTCTCGGGGC	AAAAAEEAEEE6EEEEEEEEEEEEEEEEAEEEEEAEEEEEE	NM:i:0	MD:Z:41	MC:Z:40M	AS:i:41	XS:i:19	RG:Z:${id}
SRR7629163.10004	83	chr3	73049200	60	40M	=	73049179	-61	ATTAGAATACTTCTCGGGGCCAGGTGTGGTGGCTCACGCC	AEEEEEEEEEAEEEEEEEEEE6EE/EEEE/EEE/EAAAAA	NM:i:0	MD:Z:40	MC:Z:41M	AS:i:40	XS:i:27	RG:Z:${id}








(2) 画图
hist 函数可以直接绘图，但是不太好看，可以用plot函数来画；同样density 函数也可以画图，但是也可以导出list,再进行plot 或者 ggplot 画图.

################
# R 包直接绘图
################
#BiocManager::install("ATACseqQC")
library(ATACseqQC)
fragSize <- fragSizeDist("map/SRR7629163.sambamba_rmdup.bam", "treat3")



################
# 原生R绘制
################

setwd("/data/wangjl/ATAC/fq")
getwd()

library(tidyverse)
data <- read.table("map/SRR7629163.fragment.length.txt")
dim(data) #73718     1
head(data)

######
# 过滤
######
# 去除0行
d2=data[data$V1>0,]
length(d2)
# 去掉太长的
d2=d2[d2<1200]
length(d2)

fragment=d2
length(fragment)


######
##Part1：基础语法画图
######
# 利用直方图统计频数分布，设置柱子个数
breaks_num <- 500
res <- hist(fragment, breaks = breaks_num, plot = FALSE)

# 添加坐标原点
plot(x = c(0, res$breaks),
     y = c(0, 0, res$counts) / 10^2,
     type = "l", col = "red",full="red",
     xlab = "Fragment length(bp)",
     ylab = expression(Normalized ~ read ~ density ~ 10^2),
     main = "Sample Fragment sizes")



######
##Part2：ggplot2 画图及其拼图
######
## 不同数据分布
df1 <- data.frame(x1 = c(0, res$breaks),y1=c(0, 0, res$counts) / 10^2)
p1 <- ggplot(df1,aes(x =x1,y = y1 ))+
  geom_line(col="red")+
  xlab("Fragment length(bp)")+
  ylab(expression(Normalized ~ read ~ count ~ 10^2))+
  ggtitle("Sample Fragment sizes")+
  theme_classic()
p1

## 画小图
df2 <- data.frame(x1 = c(0, res$breaks),y1=log10(c(0, 0, res$counts) / 10^2)+1)
p2 <- ggplot(df2,aes(x =x1,y = y1 ))+
  geom_line(col="red")+
  xlab("Fragment length(bp)")+
  ylab(expression(Normalized ~ read ~ count ~ (log)))+
  ggtitle("Sample Fragment sizes")+
  theme_classic()
p2

## 小图插入右上角
library(cowplot)
ggdraw() +
  draw_plot(p1, 0, 0, 1, 1) +
  draw_plot(p2, 0.5, 0.52, 0.5, 0.4) +
  draw_plot_label(c("A", "B"), c(0, 0.5), c(1, 0.92), size = 15)


######
##Part3：其他（密度图）
######
P3 <- ggplot(as.data.frame(fragment))+
  geom_density(aes(x=fragment),bw=3,color = "red")+
  xlim(0,NA)+
  scale_y_continuous(breaks = c(seq(0,0.004,0.001)),
                     labels=c(seq(0,4)),
                     name = expression(Normalized ~ read ~ density ~ 10^-3))+
  theme_classic()
P3





(3) 也可以使用工具: picard的CollectInsertSizeMetrics， bedtools的bamPEFragmentSize也都可以计算插入片段长度。

$ picard CollectInsertSizeMetrics \
I=./${filename}.last.bam \
O=./${filename}.last.insertsize \
H=./${filename}.last.hist.pdf












========================================
|-- ATAC的peak shift(Tn5酶的9bp缺口移动)：bed文件 正链右移4bp(+4)，负链左移5bp(-5)
----------------------------------------

ATAC使用Tn5转座酶来完成文库的构建工作，Tn5转座酶在连接adapter序列时，会存在9bp的gap。

图见 [Curr Protoc Mol Biol. 2015] fig.1B
ATAC-seq: A Method for Assaying Chromatin Accessibility Genome-Wide
https://pubmed.ncbi.nlm.nih.gov/25559105/



对于正链上的reads需要向右偏移4bp, 比对的起始位置加4，对于负链上的reads, 则向左偏移5bp, 比对的起始位置减5bp。

在Encode给出的ATAC pipeline中，对于原始的bam文件，首先利用bedtools转换成bed文件，
只保留reads比对上参考基因组的位置，然后再进行比对位置的偏移，具体的代码如下


def tn5_shift_ta(ta, out_dir):
	prefix = os.path.join( out_dir, os.path.basename(strip_ext_ta(ta)) )
	shifted_ta = '{}.tn5.tagAlign.gz'.format(prefix)
	cmd = 'zcat -f {} | '
	cmd += 'awk \'BEGIN {{OFS = "\\t"}}'
	cmd += '{{ if ($6 == "+") {{$2 = $2 + 4}} '
	cmd += 'else if ($6 == "-") {{$3 = $3 - 5}} print $0}}\' | '
	cmd += 'gzip -nc > {}'
	# 
	cmd = cmd.format( ta, shifted_ta)
	run_shell_cmd(cmd) 
	return shifted_ta

对于chip_seq的数据分析，拿到bam文件之后直接peak calling就可以了，
对于ATAC_seq而言，一定要偏移之后再进行peak calling。


ref:
http://www.360doc.com/content/19/1231/19/68068867_883377740.shtml
[搜 shift]https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html





========================================
|-- Peak calling
----------------------------------------
(4) MACS2 找 peak 
MACS2能够检测DNA片断的富集区域，是ATAC-seq数据call peak的主流软件。

$ macs2 --version
macs2 2.2.7.1

峰检出的原理如下：
首先将所有的reads都向3'方向延伸插入片段长度，然后将基因组进行滑窗，计算该窗口的dynamic λ，
λ的计算公式为：λlocal = λBG（λBG是指背景区域上的reads数目），
然后利用泊松分布模型的公式计算该窗口的显著性P值，最后对每一个窗口的显著性P值进行FDR校正。
默认校正后的P值（即qvalue）小于或者等于0.05的区域为peak区域。


(i)需要在linux环境中安装macs2软件，然后执行以下命令：
macs2 callpeak \
	-t A1.uni.dedup.bam \ #bam文件
	-n A1 \ # 输出文件前缀名
	--shift -100 \ #extsize的一半乘以-1
	--extsize 200 \ #一般是核小体大小
	--call-summits #检测峰顶信息
注：以上参数参考文献（Jie Wang，et.al.2018.“ATAC-Seq analysis reveals a widespread decrease of chromatin accessibility in age-related macular degeneration.” Nature Communications）


==> 尝试1: 报错
$ macs2 callpeak \
	-f BAM -g hs \
	-t map/${id}.sambamba_rmdup.bam \
	-n macs2_test/${id} \
	--shift -100 \
	--extsize 200 \
	--call-summits
报错
INFO  @ Fri, 18 Jun 2021 22:52:18: #2 looking for paired plus/minus strand peaks...
INFO  @ Fri, 18 Jun 2021 22:52:18: #2 number of paired peaks: 0
WARNING @ Fri, 18 Jun 2021 22:52:18: Too few paired peaks (0) so I can not build the model! Broader your MFOLD range parameter may erase this error. If it still can't build the model, we suggest to use --nomodel and --extsize 147 or other fixed number instead.
WARNING @ Fri, 18 Jun 2021 22:52:18: Process for pairing-model is terminated!



==> 尝试2: 不知道怎么改。试试软件给的建议: --nomodel and --extsize 147
$ mkdir macs2_test
$ macs2 callpeak \
	-f BAM -g hs \
	-t map/${id}.sambamba_rmdup.bam \
	-n macs2_test/${id}_73 \
	--nomodel --shift -73 --extsize 147 \
	--call-summits
## Shift 模型参数：
--nomodel	这个参数和extsize、shift是配套使用的，有这个参数才可以设置 extsize 和 shift 。
--extsize	当设置了 nomodel 时，MACS会用--extsize这个参数从5'->3'方向扩展reads修复fragments。比如说你的转录因子结合范围200bp，就设置这个参数是200。
--shift	当设置了--nomodel，MACS用这个参数从5'端移动剪切，然后用--extsize延伸，如果--shift是负值表示从3'端方向移动。
	建议ChIP-seq数据集这个值保持默认值为0，对于检测富集剪切位点如DNAsel数据集设置为EXTSIZE的一半。
	那ATAC怎么设置? ATAC-seq关心的是在哪切断，断点才是peak的中心，所以使用shift模型，--shift -75或-100

MACS2输出文件解
$ ls -lth macs2_test/
total 32K
-rw-rw-r-- 1 wangjl wangjl 3.7K Jun 19 09:54 SRR7629163_73_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.7K Jun 19 09:54 SRR7629163_73_summits.bed
-rw-rw-r-- 1 wangjl wangjl 5.1K Jun 19 09:54 SRR7629163_73_peaks.xls

$ head macs2_test/SRR7629163_73_peaks.narrowPeak
chr1    629325  629494  macs2_test/SRR7629163_73_peak_1         54      .       2.96016 11.3533 5.45488 71
chr1    629602  629967  macs2_test/SRR7629163_73_peak_2a        126     .       3.67855 19.1081 12.6363 100
chr1    629602  629967  macs2_test/SRR7629163_73_peak_2b        41      .       2.70196 9.90816 4.16214 318
chr1    631059  631344  macs2_test/SRR7629163_73_peak_3a        58      .       2.91161 11.8142 5.88874 78
chr1    631059  631344  macs2_test/SRR7629163_73_peak_3b        87      .       3.22924 14.9192 8.73225 208




==> 尝试3: 人细胞系ATAC-seq 数据call peak的参数设置如下：
$ macs2 callpeak -t map/${id}.sambamba_rmdup.bam --outdir macs2_test -n ${id}_shift100 \
	-f BAM -g hs --shift -100 --extsize 200 --nomodel \
	-B --SPMR --call-summits 2>macs2_test/sample.macs2.log
参数解释
-B, --bdg  Whether or not to save extended fragment pileup, and local lambda tracks (two files) at every bp into a bedGraph file. DEFAULT: False
	是否保存2个文件(默认: 否): extended fragment pileup, and local lambda tracks to bedGraph file.
-B --SPMR 会多2个输出文件。

$ ls macs2_test/ -lth
total 8.8M
-rw-rw-r-- 1 wangjl wangjl 3.1K Jun 19 10:10 sample.macs2.log
-rw-rw-r-- 1 wangjl wangjl 4.4M Jun 19 10:10 SRR7629163_shift100_control_lambda.bdg  ##额外多的文件1
-rw-rw-r-- 1 wangjl wangjl 2.8K Jun 19 10:10 SRR7629163_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 4.2K Jun 19 10:10 SRR7629163_shift100_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 2.0K Jun 19 10:10 SRR7629163_shift100_summits.bed
-rw-rw-r-- 1 wangjl wangjl 4.4M Jun 19 10:10 SRR7629163_shift100_treat_pileup.bdg ##额外多的文件2


## 批量化 
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
macs2 callpeak -t map/${id}.sambamba_rmdup.bam --outdir macs2 -n ${id}_shift100 \
	-f BAM -g hs --shift -100 --extsize 200 --nomodel \
	-B --SPMR --call-summits 2>macs2/${id}.macs2.log;
done;


$ ls macs2/*narrowPeak -lth
-rw-rw-r-- 1 wangjl wangjl 2.9K Jun 19 14:52 macs2/SRR7629163_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.2K Jun 19 14:52 macs2/SRR7629162_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.7K Jun 19 14:52 macs2/SRR7629161_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.7K Jun 19 14:52 macs2/SRR7629154_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.5K Jun 19 14:52 macs2/SRR7629153_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 1.7K Jun 19 14:52 macs2/SRR7629152_shift100_peaks.narrowPeak















(ii) 按照另一个教程的参数
$ macs2 callpeak -t map/${id}.sambamba_rmdup.bam -f BAM -g hs -n macs2_/${id} -m 2 100
参数 [-m MFOLD MFOLD]

## 批量化
$ makir macs2_/
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
macs2 callpeak -t map/${id}.sambamba_rmdup.bam -f BAM -g hs -n macs2_/${id} -m 2 100;
done;


$ ls -lth macs2_/*narrowPeak
-rw-rw-r-- 1 wangjl wangjl 3.6K Jun 19 14:54 macs2_/SRR7629163_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 3.2K Jun 19 14:54 macs2_/SRR7629162_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 3.1K Jun 19 14:54 macs2_/SRR7629161_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.2K Jun 19 14:54 macs2_/SRR7629154_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 3.2K Jun 19 14:54 macs2_/SRR7629153_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 1.5K Jun 19 14:54 macs2_/SRR7629152_peaks.narrowPeak



(iii) 另一个教程推荐使用 -f BAMPE
macs2 callpeak  -t <BED>  -f BEDPE  -n NAME  -g ce  --keep-dup all

Analyze only properly paired alignments with -f BAMPE. Here, the fragments are defined by the paired alignments' ends, and there is no modeling or artificial extension. Singleton alignments are ignored. This is the preferred option for using only properly paired alignments.

仅使用配对数据，单端被忽略。

$ macs2 callpeak -t map/${id}.sambamba_rmdup.bam  -f BAMPE --outdir macs2_/  -n ${id}_BAMPE  -g hs

$ ls -lth macs2_ | grep SRR7629163
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 22 20:06 SRR7629163_BAMPE_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.4K Jun 22 20:06 SRR7629163_BAMPE_summits.bed
-rw-rw-r-- 1 wangjl wangjl 4.7K Jun 22 20:06 SRR7629163_BAMPE_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 2.5K Jun 19 14:54 SRR7629163_summits.bed
-rw-rw-r-- 1 wangjl wangjl 3.6K Jun 19 14:54 SRR7629163_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 5.0K Jun 19 14:54 SRR7629163_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  74K Jun 19 14:54 SRR7629163_model.r

比 -f BAM 少了2行。



















#############
# treat vs control 可以跳过下面这一段 macs分析
#############

比较两组间的呢？

$ macs2 callpeak -c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam map/SRR7629163.sambamba_rmdup.bam \
	-f BAM -g hs -n macs2/treat_vs_control -m 2 100;

$ ls -lth macs2/ | grep vs
-rw-rw-r-- 1 wangjl wangjl    0 Jun 18 23:06 treat_vs_control_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 1.5K Jun 18 23:06 treat_vs_control_peaks.xls
-rw-rw-r-- 1 wangjl wangjl    0 Jun 18 23:06 treat_vs_control_summits.bed
-rw-rw-r-- 1 wangjl wangjl  95K Jun 18 23:06 treat_vs_control_model.r

这里没有 narrowPeak 文件，长度是0, why?


#==> 去掉一个处理组，保持处理和对照样本数一致(n=2:2):
$ macs2 callpeak \
	-c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam \
	-f BAM -g hs -n macs2/treat_vs_control -m 2 100;

$ ls -lth macs2/ | grep vs
-rw-rw-r-- 1 wangjl wangjl  24K Jun 18 23:12 treat_vs_control_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  27K Jun 18 23:12 treat_vs_control_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  18K Jun 18 23:12 treat_vs_control_summits.bed
-rw-rw-r-- 1 wangjl wangjl  82K Jun 18 23:12 treat_vs_control_model.r

$ Rscript macs2/treat_vs_control_model.r
能看到 Peak Model 图。
Cross−Correlation 图2条竖线。




#==> 使用三组，处理和对照样本数一致(n=3:3):
$ macs2 callpeak \
	-c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam map/SRR7629154.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam map/SRR7629163.sambamba_rmdup.bam \
	-f BAM -g hs -n macs2/treat_vs_control-3 -m 2 100;

$ ls -lth macs2/ | grep -P "\-3"
-rw-rw-r-- 1 wangjl wangjl  47K Jun 19 09:39 treat_vs_control-3_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  52K Jun 19 09:39 treat_vs_control-3_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  34K Jun 19 09:39 treat_vs_control-3_summits.bed
-rw-rw-r-- 1 wangjl wangjl  95K Jun 19 09:39 treat_vs_control-3_model.r


$ Rscript macs2/treat_vs_control-3_model.r
能看到 Peak Model 图。
Cross−Correlation 图就剩下一条竖线。



==> 使用2组，使用ATAC-seq推荐参数
## 在 macs2/下清理文件 ls -t | tail -n 20 | xargs -i mv {} dustbin/

$ macs2 callpeak \
	-c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam \
	--shift -100 --extsize 200 --nomodel \
	-f BAM -g hs -n macs2/treat_vs_control-shift100 2>macs2/shift100.macs2.log
## 没有 model 图，去掉 --nomodel 再执行一遍有了。

$ macs2 callpeak \
	-c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam \
	--shift -100 --extsize 200 \
	-f BAM -g hs -n macs2/treat_vs_control-shift100-m 2>macs2/shift100-m.macs2.log
$ Rscript macs2/treat_vs_control-shift100-m_model.r

















========================================
ATAC-seq 高级分析（peak注释、差异peak、motif、转录因子的footprint和核小体占位）
----------------------------------------
1. paper
ATAC-seq数据分析工具的比较和推荐（Genome Biology综述）
https://mp.weixin.qq.com/s?__biz=MzA4NTIyMzk4Mg%3D%3D&mid=2247483678&idx=1&sn=de7b6bcbf76c2a9051d0db7a4d9cd074
https://pubmed.ncbi.nlm.nih.gov/32014034/


2. 技巧
https://www.cnblogs.com/leezx/p/12953732.html







========================================
|-- Annotation 峰注释
----------------------------------------
在科研分析中我们往往需要将peak区域与基因联系起来，也就是通过对peak进行注释找到peak相关基因。

It is helpful to know what genomic features are near the peaks called by MACS2. One program that is commonly used to annotate peaks is ChIPseeker. Like MACS2, ChIPseeker was originally designed to be used in the analysis of ChIP-seq, but it works just as well with ATAC-seq.

ChIPseeker requires that the genome of interest be annotated with locations of genes and other features. The ChIPseeker user guide is extremely helpful in using this R/Bioconductor package.


常见的peak注释软件有 ChIPseeker、homer、PeakAnnotator等。



1. 以ChIPseeker为例，需要在R中安装ChIPseeker包和GenomicFeatures包，然后就可以进行分析了。

library(ChIPseeker)
library(GenomicFeatures)
txdb<- makeTxDbFromGFF('gene.gtf') #生成txdb对象，如果研究物种没有已知的TxDb,可以用GenomicFeatures中的函数生成
peakfile <-readPeakFile('A1_peaks.narrowPeak') #导入需要注释的peak文件
peakAnno <- annotatePeak(peakfile,tssRegion=c(-2000, 2000), TxDb=txdb)
## 用peak文件和txdb进行peak注释，这里可以通过tssRegion定义TSS区域的区间


对于peak注释的结果，也可以进行可视化展示，如：
p <- plotAnnoPie(peakAnno)

一个饼图。










2. homer 注释峰
(1) Usage: annotatePeaks.pl <peak file | tss> <genome version>  [additional options...]
$ annotatePeaks.pl H3K4Me3.bed hg19 1>ChIP-Seq_H3K4Me3_1_peakAnn.xls 2>H3K4Me3_annLog.txt
$ annotatePeaks.pl peak.bed hg19 > peak.annotation.xls

$ annotatePeaks.pl peaks.txt hg38 -gtf Ensembl_hg38.gtf -cTSS Ensembl_hg38.tss > annotated_peaks.txt
参数解释: https://www.biostars.org/p/360106/
User defined annotation files (default is UCSC refGene annotation): 默认使用UCSC refGene 注释。
		annotatePeaks.pl accepts GTF (gene transfer formatted) files to annotate positions relative to custom annotations, such as those from de novo transcript discovery or Gencode.
			不知道怎么翻译： 相对于自定义注释位置，该脚本也接受 GTF文件来注释位置，比如 de novo 转录本或 GenCode。
		-gtf <gtf format file> (Use -gff and -gff3 if appropriate, but GTF is better) 后面跟着gtf文件
		-gid (by default the GTF file is processed by transcript_id, use this option for gene_id) 这个指定使用gene id，默认是使用 转录本id。
		-ann <custom homer annotation file> (created by assignGenomeAnnotation, see website) 自定义 homer 注释文件。

Peak vs. tss/tts/rna mode (works with custom GTF file): 转录起始位点TSS
		If the first argument is "tss" (i.e. annotatePeaks.pl tss hg18 ...) then a TSS centric analysis will be carried out.  
			如果第一个参数是 tss，则会进行 TSS 为核心的的分析。
		Tag counts and motifs will be found relative to the TSS.  序列和 motif 都是相对于 TSS的。
		(no position file needed) ["tts" now works too - e.g. 3' end of gene]  现在 tss 参数也正常工作了，比如对于基因的3‘端。
		["rna" specifies gene bodies, will automaticall set "-size given"]  而参数 rna 则指定 gene bodies，会自动设置 “-size given”
		NOTE: The default TSS peak size is 4000 bp, i.e. +/- 2kb (change with -size option) 默认的TSS 峰是 +-2kb，使用参数 -size 设置。
		-list <gene id list> (subset of genes to perform analysis [unigene, gene id, accession, probe, etc.], default = all promoters)
			取子集分析，默认是 全部启动子。
		-cTSS <promoter position file i.e. peak file> (should be centered on TSS)
			围绕TSS。




(2) 实例
$ annotatePeaks.pl macs2/SRR7629152_shift100_peaks.narrowPeak hg38 1>macs2/ann/SRR7629152_peakAnn.xls #2>macs2/ann/SRR7629152_ann.log

##==> 报错:
!!!!Genome hg38 not found in /home/wangjl/soft/homer/.//config.txt
        To check if is available, run "perl /home/wangjl/soft/homer/.//configureHomer.pl -list"
        If so, add it by typing "perl /home/wangjl/soft/homer/.//configureHomer.pl -install hg38"
$ perl /home/wangjl/soft/homer/.//configureHomer.pl -list  #查看所有内置的物种
查了一下，发现有 hg38
下载安装。
安装很慢，主要是网速慢。
$ perl /home/wangjl/soft/homer/.//configureHomer.pl -install hg38
## Current base directory for HOMER is /home/wangjl/soft/homer/.//
顺便把常用的都安装了: hg19, mm10
$ perl /home/wangjl/soft/homer/.//configureHomer.pl -install hg19

下载的信息保存在homer安装目录的data/genome/目录下，
$ ls /home/wangjl/soft/homer/./data/genomes/
hg19  hg38

以hg38为例，在data/genome/hg38目录下，文件列表如下
$ ls /home/wangjl/soft/homer/data/genomes/hg38/ -lth
total 4.4G
drwxrwxr-x 2 wangjl wangjl 4.0K Jun 24 21:07 preparsed/
-rw-r--r-- 1 wangjl wangjl 673M Oct 19  2019 hg38.full.annotation
-rw-r--r-- 1 wangjl wangjl  42M Oct 19  2019 hg38.basic.annotation
-rw-r--r-- 1 wangjl wangjl 505M Oct 19  2019 hg38.repeats
-rw-r--r-- 1 wangjl wangjl 3.2M Oct 19  2019 hg38.aug
-rw-r--r-- 1 wangjl wangjl 161K Oct 19  2019 hg38.miRNA
-rw-r--r-- 1 wangjl wangjl  29M Oct 19  2019 hg38.splice3p
-rw-r--r-- 1 wangjl wangjl  29M Oct 19  2019 hg38.splice5p
-rw-r--r-- 1 wangjl wangjl 3.2M Oct 19  2019 hg38.stop
-rw-r--r-- 1 wangjl wangjl 3.2M Oct 19  2019 hg38.tss
-rw-r--r-- 1 wangjl wangjl 3.2M Oct 19  2019 hg38.tts
-rw-r--r-- 1 wangjl wangjl  24M Oct 19  2019 hg38.rna
-rw-r--r-- 1 wangjl wangjl  12K Oct 19  2019 chrom.sizes
drwxr-xr-x 5 wangjl wangjl 4.0K Oct 19  2019 annotations/
-rw-r--r-- 1 wangjl wangjl 3.1G Jan 16  2014 genome.fa

包含了参考基因组的fasta序列以及不同区域的区间文件。
查看一下 hg38.basic.annotation 文件，共7列。
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation | awk -F "\t" '{print $7}' 查看某一列
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation #各列不对齐
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation | column -ts $'\t'
Intergenic                           chr1  1      10873  +  N       1900000000
promoter-TSS (NR_046018)             chr1  10874  11974  +  P       1
non-coding (NR_046018, exon 1 of 3)  chr1  11975  12227  +  pseudo  165001
intron (NR_046018, intron 1 of 2)    chr1  12228  12612  +  I       1089459
non-coding (NR_046018, exon 2 of 3)  chr1  12613  12721  +  pseudo  165002
intron (NR_046018, intron 2 of 2)    chr1  12722  13220  +  I       1089459




同时在data/accession目录下，还有参考基因组对应的基因注释文件。
$ ls /home/wangjl/soft/homer/./data/accession
homologene.data  human2gene.tsv  human.description  taxids.tsv

其中 human2gene.tsv记录了基因的ubigene id, gene symbol等信息，内容如下所示
$ head /home/wangjl/soft/homer/./data/accession/human2gene.tsv
AAQ88605        388007  Hs.527795       NR_015340       ENSG00000187483         SERPINA13P
127219  -
ENSP00000379387 353274  Hs.250481       NM_181489       ENSG00000185219         ZNF445
28336   3499                                    IGHEP2
NM_173803       255027  Hs.720673       NM_173803       ENSG00000156968         MPV17L

human.description记录表了基因的功能描述，类别等信息 10列，只能一列一列看了
$ head /home/wangjl/soft/homer/./data/accession/human.description | awk -F'\t' '{print $10}'
$ head /home/wangjl/soft/homer/./data/accession/human.description #整体看时，不能按列对齐
GeneID  Unigene RefSeq  Ensembl name    alias   orf     chromosome      description     type
57573   Hs.230188       NM_020813       ENSG00000196263 ZNF471  ERP1|Z1971      -       19q13.43        zinc finger protein 471 protein-coding
113391337       Hs.148569                       ZEBTR   BX111   -       -       ZEB1 transcriptional regulator RNA      ncRNA
107984694                               LOC107984694    -       -       14q23.3 probable ribosome biogenesis protein RLP24 pseudogene   pseudo
8563    Hs.75361        NM_003678       ENSG00000100296 THOC5   C22orf19|Fmip|PK1.3|fSAP79      -       22q12.2 THO complex 5   protein-coding





(3)再次执行 峰注释，成功。
$ head  macs2/ann/SRR7629152_peakAnn.xls
PeakID (cmd=annotatePeaks.pl macs2/SRR7629152_shift100_peaks.narrowPeak hg38)   Chr     Start   End     Strand  Peak Score      Focus Ratio/Region Size Annotation      Detailed Annotation     Distance to TSS Nearest PromoterID      Entrez ID       Nearest Unigene Nearest Refseq Nearest Ensembl Gene Name       Gene Alias      Gene Description        Gene Type
SRR7629152_shift100_peak_8      chr13   109424078       109424389       +       669     NA      Intergenic      Intergenic      23537   NR_126361       104326054       Hs.569298       NR_126361       ENSG00000229792 LINC00399       TCONS_00021596  long intergenic non-protein coding RNA 399     ncRNA
SRR7629152_shift100_peak_6      chr11   10509241        10509764        +       286     NA      promoter-TSS (NM_001190702)     promoter-TSS (NM_001190702)     -316    NM_001190702    100463486               NM_001190702    ENSG00000255823 MTRNR2L8        HN8     MT-RNR2 like 8 protein-coding
SRR7629152_shift100_peak_18     chr5    80651408        80651656        +       281     NA      promoter-TSS (NM_001190470)     promoter-TSS (NM_001190470)     -524    NM_001190470    100462981               NM_001190470            MTRNR2L2        HN2     MT-RNR2 like 2protein-coding
# 我加的列编号
# 1                             2       3               4               5       6       7       8                                9                               10     11              12                      13                       14              16       17            


1 PeakID (cmd=annotatePeaks.pl macs2/SRR7629152_shift100_peaks.narrowPeak hg38)   
2 Chr     
3 Start   
4 End     
5 Strand  

6 Peak Score       ## 215
7 Focus Ratio/Region Size
8 Annotation              ## Intergenic
9 Detailed Annotation      ## (TAACCC)n|Simple_repeat|Simple_repeat
10 Distance to TSS    #-1780
11 Nearest PromoterID      ## NR_046018
12 Entrez ID              ## 100287102
13 Nearest Unigene  ## Hs.618434
14 Nearest Refseq   ## HR_046018
15 Nearest Ensembl   ## ENSG00000223972
16 Gene Name         ## DDX11L1
17 Gene Alias       ## CWH43-C|PGAP2IP
18 Gene Description        ## DEAD/H-box helicase 11 like 1
19 Gene Type  ## protein-coding


看注释的效果，就是看第8和9列的分布，然后用R回个饼图。
$ head  macs2/ann/SRR7629152_peakAnn.xls| awk -F "\t" '{print $8}'
$ cat macs2/ann/SRR7629152_peakAnn.xls| awk -F "\t" '{print $8}' | awk -F'(' '{print $1}'| sed 's/ $//' |   sort | uniq -c
      1 Annotation
      9 Intergenic
      5 intron
      5 promoter-TSS
      1 TTS
#






(4) 批量化
$ makir macs2/ann/
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
annotatePeaks.pl macs2/${id}_shift100_peaks.narrowPeak hg38 1>macs2/ann/${id}_peakAnn.txt 2>macs2/ann/${id}_ann.log;
done;









========================================
|-- 差异分析: R包 DiffBind /  bedtools subtract //todo
----------------------------------------

1. 差异peak代表着比较组合染色质开放性有差异的位点，ChIP-seq和ATAC-seq都可以用 DiffBind 进行差异分析。DiffBind通过bam文件和peak的bed文件计算出peak区域标准化的readcount，可以选择edgeR、DESeq2等模型进行差异分析。

DiffBind是鉴定两个样本间差异结合位点的一个R包。
主要用于peak数据集，包括对peaks的重叠和合并的处理，计算peaks重复间隔的测序reads数，并基于结合亲和力鉴定具有统计显著性的差异结合位点。适用的统计模型有DESeq、DESeq2、edgeR。

library("DiffBind")


准备输入文件
需要准备一个SampleSheet文件，与ChIPQC的方法一样。SampleSheet文件是根据实验设计和数据存储地址等信息创建的一个csv格式文件，包含的表头信息有"SampleID"、 "Tissue"、 "Factor"、 "Condition" 、"Treatment"、"Replicate" 、"bamReads" 、"ControlID"、 "bamControl" "Peaks"、 "PeakCaller"（bam,peak文件分别在比对和call peak的步骤产生）。

""、 ""、 ""、 "" 、""、"" 、"" 、""、 "bamControl" "Peaks"、 "PeakCaller"

SampleID	Treatment	bamReads	ControlID
ctrl1	ctrl	c1.bam 	
ctrl2	ctrl	c2.bam 	
treat1	treat	t1.bam 	
treat2	treat	t2.bam 	



dbObj <- dba(sampleSheet="SampleSheet.csv")








2. Comparing peak files
Determining genomic regions that are common or different to a set of peak files is best accomplished with BEDTools, a suite of software tools that enables "genome arithmetic."

(1) 重复内共有的峰 bedtools intersect
For example, bedtools intersect determines regions that are common to two peak files, such as replicates of the same experimental group.


(2) 实验-对照特有的峰 bedtools subtract
Finding differences between two peak files, such as control vs. experimental groups, is accomplished via bedtools subtract.






ref:
https://www.jianshu.com/p/f849bd55ac27







========================================
|-- Motif 分析 ( homer )
----------------------------------------
1.Motif富集分析

活性开放的染色质区域通过结合特定的转录因子影响转录，转录因子结合识别的DNA序列成为motif，人体中大约有1600转录因子，其中一半多已经有明确的motif。对motif的分析包括motif富集分析和转录因子的footprint。

目前有很多Motif数据库，其中最普遍的JASPAR数据库，包含了很多物种的数据，可以很容易地通过APIs或Bioconductor的R包获得相关数据。除此之外，CIS-BP和TRANSFAC包含真核转录因子的motif；HOCOMOCO专注于人和小鼠；RegulonDB是E.coli数据。

Motif信息主要以txt文件格式保存，比如位置权重矩阵（as a position weight matrix , PWM）。HOMER、Bioconductor中的TFBSTools和motifmatchr包可以利用PWM搜索特定序列中的motif。PWMScan可以通过网页快速搜索motif。MEME suit里面的FIMO、MAST和MCAST在motif搜索上有不同的应用。MEME suit和PWMScan有网页版本，更容易使用。

得到每个peak region里motif的位置和频率，再和随机背景或其它条件比较，就可以做motif的富集分析。HOMER、MEME-AME、MEME-CentriMo和DAStk分别采用不同的统计检验比较peak和背景区域motif频率的差异。ChromVAR是为单细胞ATAC-seq开发的motif分析工具，把每个细胞当作一个重复。

需要注意的是并时不时所有的TFs都有鉴定好的motif，而同一个家族TFs有类似的motif。在软件选择上，MEME-CentriMo是用的比较多的一个网页应用，可以产生可视化的报告，单细胞ATAC-seq可以选择chromVAR软件。




2. homer 

HOMER is a suite of software designed for motif discovery. It takes a peak file as input and checks for the enrichment of both known sequence motifs and de novo motifs.

http://homer.ucsd.edu/homer/ngs/peakMotifs.html



ATAC分析得到的peak是染色质上的开放区域，这些染色质开放区域常常预示着转录因子的结合，因此对peak区域进行motif分析很有意义。常见的motif分析软件有homer和MEME。以homer软件为例，首先在linux环境中安装homer，然后用以下命令进行motif分析：

findMotifsGenome.pl \
  A1_peaks.bed \ #用于进行motif分析的bed文件
  genome.fa  \ #参考基因组fa文件
  A1  \ #输出文件前缀
  -size  given \ #使用给定的bed区域位置进行分析，如果填-size -100,50则是用给定bed中间位置的上游100bp到下游50bp的区域进行分析


$ findMotifsGenome.pl
	Program will find de novo and known motifs in regions in the genome
	Usage: findMotifsGenome.pl <pos file> <genome> <output directory> [additional options]
	Example: findMotifsGenome.pl peaks.txt mm8r peakAnalysis -size 200 -len 8

homer分析motif的原理及结果参见 http://homer.ucsd.edu/homer/motif/index.html
homer 找motif的原理: https://www.jianshu.com/p/9a31f5f01e7b


示例2: 
findMotifsGenome.pl H3K4Me3.bed hg19 H3K4Me3_motif -len 8,10,12
# peak文件：ChIP-Seq_H3K4Me3_1_homer.bed
# 基因组版本：hg19
# 输出路径：ChIP-Seq_H3K4Me3_1_motifDir
# motif长度：-len 8,10,12 
# motif的软件默认长度为8，10，12
​
-p <#> (Number of processors to use, default: 1) 设置线程数
-S <#> (Number of motifs to optimize, default: 25) 所寻找的motif数目，默认为25。25已经不算少了，如果自定义数目，推荐设置更少而不是更多。








1. 按照bed文件范围分析(start to end)

$ findMotifsGenome.pl \
  macs2/treat_vs_control-shift100-m_peaks.narrowPeak \
  /home/wangjl/data/ref/human/UCSC/hg38.fa  \
  homer/treat_vs_control-bed  \
  -size given


## 报错：
        Peak file looks good!                                 

        Background files for 150 bp fragments found.
!!!! Might have something wrong with preparsed files
!!!! Rerun and add "-preparse" to the command line to force recreation of the files
        Custom genome sequence file: /home/wangjl/data/ref/human/UCSC/hg38.fa.gz

        Scanning input files...
!!! Something is wrong... are you sure you chose the right length for motif finding?
!!! i.e. also check your sequence file!!!


是不是必须解压fasta文件?
## /home/wangjl/data/ref/human/UCSC
$ gunzip -c hg38.fa.gz >hg38.fa
删除刚建立的文件夹 preparsed/
重新运行。
这一次很慢，估计能出结果。
$ ls -lth preparsed/
total 896M
-rw-rw-r-- 1 wangjl wangjl  80M Jun 19 12:07 hg38.fa.93.gcbins
-rw-rw-r-- 1 wangjl wangjl  80M Jun 19 12:07 hg38.fa.93.cgbins
-rw-rw-r-- 1 wangjl wangjl 199M Jun 19 12:07 hg38.fa.93.cgfreq
-rw-rw-r-- 1 wangjl wangjl 375M Jun 19 12:07 hg38.fa.93.seq
-rw-rw-r-- 1 wangjl wangjl 165M Jun 19 12:06 hg38.fa.93.pos



批量化 
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
findMotifsGenome.pl \
  macs2/${id}_shift100_peaks.narrowPeak \
  /home/wangjl/data/ref/human/UCSC/hg38.fa  \
  homer/${id}  \
  -size given;
done;




测试: 加上参数 -preparse, 后来发现是narrowPeak是空文件时报这个错。
$ findMotifsGenome.pl macs2_result/SRR7629153_peaks.narrowPeak /home/wangjl/data/ref/human/UCSC/hg38.fa homer/SRR7629153_1/ -size given -p 10
## 21:03--> 
$ findMotifsGenome.pl macs2_result/SRR7629153_peaks.narrowPeak /home/wangjl/data/ref/human/UCSC/hg38.fa homer/SRR7629153_1/ -size given -p 10 -preparse
$ findMotifsGenome.pl macs2_result/SRR7629153_peaks.narrowPeak hg38 homer/SRR7629153_2/ -size given -p 10
## 21:07-->





2. 按照 summit 上下游分析(下游100，上游50范围内)

$ findMotifsGenome.pl \
  macs2/treat_vs_control-shift100-m_summits.bed \
  /home/wangjl/data/ref/human/UCSC/hg38.fa  \
  homer_/treat_vs_control-summits  \
  -size  -100,50








3. 可视化

根据motif与已知转录因子的富集情况可以绘制气泡图，从而可以看到样本与已知转录因子的富集显著性。


已知转录因子的富集显著性
x轴为样本标签: A1, A2, B1, B2
Y轴为转录因子名字: JunB, Jun-AP1, Fra2, Fra1, Fosl2, CTCF, Atf3, AP-1
中间是dotplot，直径大小表示 motif enrichment(-logP)






ref:
https://www.jianshu.com/p/4e6b42152694













========================================
|-- 富集分析 //todo
----------------------------------------
通过注释得到的peak相关基因可以使用goseq、topGO等R包进行GO富集分析，用kobas进行kegg富集分析，也可以使用DAVID在线工具来完成富集分析。

可以通过挑选感兴趣的GO term或pathway进一步筛选候选基因。



















========================================
使用snakemake构建 ATAC-seq 分析流程: 从 fastq 到 call peak
----------------------------------------
1. 流程脚本:
https://github.com/crazyhottommy/pyflow-ATACseq

按照include模式写，写成一个就保存一个。

此后的修订记录在单独的库中:
https://github.com/DawnEve/snakemakeWorkflow
ref: https://github.com/snakemake-workflows



(2) 路径
脚本位置
/data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2

数据位置
/home/wangjl/data/ATAC/hair/raw/origin/  #192=2*96
$ ls -lth /home/wangjl/data/ATAC/hair/raw/origin/|head
total 3.0G
-r--r--r-- 1 wangjl wangjl 442M Jun 24 19:07 cgm-57_R1.fastq.gz
-r--r--r-- 1 wangjl wangjl 440M Jun 24 19:07 cgm-57_R2.fastq.gz
-r--r--r-- 1 wangjl wangjl  52K Jun 24 19:08 cgm-94_R1.fastq.gz
-r--r--r-- 1 wangjl wangjl  52K Jun 24 19:08 cgm-94_R2.fastq.gz




(2) 准备测试文件

工作目录
$ cd /data/wangjl/ATAC/fq/test5 

$ mkdir raw/
$ gunzip -c /home/wangjl/data/ATAC/hair/raw/origin/cgm-57_R1.fastq.gz | head -n 40000 | gzip > raw/cgm-57_R1.fastq.gz
$ gunzip -c /home/wangjl/data/ATAC/hair/raw/origin/cgm-57_R2.fastq.gz | head -n 40000 | gzip > raw/cgm-57_R2.fastq.gz

$ ln -s /home/wangjl/data/ATAC/hair/raw/origin/cgm-94_R1.fastq.gz raw/
$ ln -s /home/wangjl/data/ATAC/hair/raw/origin/cgm-94_R2.fastq.gz raw/



(3) 写 yaml 格式的配置文件 
$ ls raw/ > config.yaml
$ sed -i 's/_R[12]\.fastq\.gz//' config.yaml
$ sed -i 's/^/ - /' config.yaml 
$ sed -i '1c samples:' config.yaml
$ cat config.yaml 
samples:
 - cgm-57
 - cgm-94
 - cgm-94





(4) 另一种 yaml 格式配置文件的写法
$ cat config.yaml #格式有严格要求: 逗号后有空格, 冒号后有空格，不需要加引号了。
samples: [SRR7629152, SRR7629153, SRR7629154, SRR7629161, SRR7629162, SRR7629163]
workdir: /home/wangjl/data/ATAC/fq/test3
Snk_RootDir: /home/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo1

snake脚本中引用
$ cat test.sf
configfile: "config.yaml"
workdir: config["workdir"]
SI=config["samples"]










2. 边写边测试，模块化。

脚本结构
├── config.yaml-example
├── scripts
│   ├── script1.py
│   └── script2.R
├── rules
│   ├── 01qc.smk
│   ├── 02mapping.smk
│   └── 03plot.smk
└── main.sf

$ cd /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2


####===>>>>>>>>>>主程序脚本

$ cat main.sf 
configfile: "config.yaml"
SI=config["samples"]
BWA_INDEX="/home/wangjl/data/ref/human/UCSC/bwa/hg38_fa"

rule all:
	input:
		"QC_raw/multiqc/multiqc_report.html",
		#expand("clean2/{sample}_R1_val_1.fq.gz", sample=SI),
		#expand("clean/{sample}_R1.fastq.gz", sample=SI),
		"QC_clean/multiqc/multiqc_report.html",
		#expand("map/{sample}.sort.bam", sample=SI),
		#expand("map_clean/{sample}.final.bam", sample=SI),
		"map_clean/done.txt",
		expand("macs2_result/{sample}_peaks.narrowPeak", sample=SI),


include: "rules/01_QC_raw.sf"
include: "rules/02_trim.sf"
include: "rules/03_BWA_map.sf"
include: "rules/04_bam_filter.sf"
include: "rules/05_call_peak.sf"


## 原始质控 
$ cat rules/01_QC_raw.sf
rule QC_raw_fq:
	input: 
		"raw/{sample}_R1.fastq.gz",
		"raw/{sample}_R2.fastq.gz",
	output: 
		"QC_raw/{sample}_R1_fastqc.html",
		"QC_raw/{sample}_R2_fastqc.html",
	params:out_dir="QC_raw"
	threads: 2
	log: "QC_raw/{sample}_fastqc.html.log"
	shell: 
		"fastqc -t {threads} {input} -o {params.out_dir} >{log} 2>&1"

rule multi_QC_raw:
	input: expand("QC_raw/{sample}_R1_fastqc.html", sample=SI)
	output:"QC_raw/multiqc/multiqc_report.html"
	params: 
		in_dir="QC_raw",
		out_dir="QC_raw/multiqc/"
	log: "QC_raw/multiqc/multiqc_report.html.log"
	shell:"multiqc {params.in_dir}/*fastqc.zip -o {params.out_dir} >{log} 2>&1"


## 去接头
$ cat rules/02_trim.sf
rule trim_galore:
	input:
		"raw/{sample}_R1.fastq.gz",
		"raw/{sample}_R2.fastq.gz",
	output:
		"clean2/{sample}_R1_val_1.fq.gz",
		"clean2/{sample}_R2_val_2.fq.gz",
	params: out="clean2/"
	threads: 2
	log: "clean2/{sample}.trim.log"
	shell:"trim_galore -j {threads} -q 25 --phred33 --length 15 -e 0.1 --stringency 3 \
		--paired -o  {params.out}  {input} >{log} 2>&1"
## --stringency <INT>      Overlap with adapter sequence required to trim a sequence. Defaults to a
##	very stringent setting of 1, i.e. even a single bp of overlapping sequence
##	will be trimmed off from the 3' end of any read.

rule cut_adapter:
	input:
		"raw/{sample}_R1.fastq.gz",
		"raw/{sample}_R2.fastq.gz",
	output:
		"clean/{sample}_R1.fastq.gz",
		"clean/{sample}_R2.fastq.gz"
	log:"clean/{sample}_cutapapt.log"
	threads: 2
	shell:
		"cutadapt -j {threads} --time 1 -e 0.1 -O 3 --quality-cutoff 25 --pair-adapters \
		-m 15 -a CTGTCTCTTATA \
		-A AGACGGGAAGAGCGTCGTGAGGGAAGAGTGTAGATCTCGGTGGTCGCGTATCATT \
		-o {output[0]} -p {output[1]} {input[0]} {input[1]} >{log} 2>&1"

## -m LEN[:LEN2], --minimum-length LEN[:LEN2] 最小长度
## -O MINLENGTH, --overlap MINLENGTH 序列和接头重合长度
##   Require MINLENGTH overlap between read and adapter for an adapter to be found.
##   Default: 3
## -n COUNT, --times COUNT
##   Remove up to COUNT adapters from each read. Default: 1

# 结果行数是一样的，选更快的。


rule QC_clean_fq:
	input: 
		"clean/{sample}_R1.fastq.gz",
		"clean/{sample}_R2.fastq.gz"
	output: 
		"QC_clean/{sample}_R1_fastqc.html",
		"QC_clean/{sample}_R2_fastqc.html"
	params:out_dir="QC_clean"
	log: "QC_clean/{sample}_fastqc.html.log"
	threads: 2
	shell: 
		  "fastqc -t {threads} {input} -o {params.out_dir} >{log} 2>&1"

rule multi_QC_clean:
	input: expand("QC_clean/{sample}_R2_fastqc.html", sample=SI)
	output:"QC_clean/multiqc/multiqc_report.html"
	params: 
		in_dir="QC_clean",
		out_dir="QC_clean/multiqc/"
	log: "QC_clean/multiqc/multiqc_report.html.log"
	shell:"multiqc {params.in_dir}/*fastqc.zip -o {params.out_dir} >{log} 2>&1"





## 比对: 仅保存bam，删除sam中间文件
$ cat rules/03_BWA_map.sf
rule bwa_mapping:
	input:
		"clean/{sample}_R1.fastq.gz",
		"clean/{sample}_R2.fastq.gz"
	output:
		temp("map/{sample}_bwa_hg38.sam")
	params: bwa=r"@RG\tID:{sample}\tSM:{sample}"
	threads: 4
	log:"map/{sample}_bwa_hg38.sam.log"
	shell:
		"bwa mem -t {threads} -R '{params.bwa}' -o {output} {BWA_INDEX} {input} >{log} 2>&1"

rule sort_to_bam:
	input:"map/{sample}_bwa_hg38.sam"
	output:"map/{sample}.sort.bam"
	params: tmp="map/{sample}_tmp"
	threads: 2
	log: "map/{sample}.sort.bam.log"
	shell:"samtools sort -@ {threads} -O bam -T {params.tmp} -o {output} {input} >{log} 2>&1"

#$ samtools view map/cgm-57.sort.bam|  awk '{print $5}' | sort |uniq -c | sort -k1nr|head
#  14375 60
#   3434 0
#    345 40




## rm MT, rm dup, rm Low MapQ, then BAM QC (TSS分布图，插入片段长度)
$ cat rules/04_bam_filter.sf
rule index:
	input:"{sample}.bam"
	output:"{sample}.bam.bai"
	shell:"samtools index {input}"

## rm MT
rule get_chrlist:
	input:"map/{sample}.sort.bam"
	output:"map_clean/{sample}.chrlist"
	log:"map_clean/{sample}.chrlist.log"
	shell:"samtools view  {input}| awk '{{print $3}}' \
		| sort | uniq -c| grep -E -v 'GL|KI|JH|KB|chrM|\*' |  awk '{{print $2}}' | xargs > {output} 2>{log}"

rule del_MTs:
	input:"map_clean/{sample}.chrlist", "map/{sample}.sort.bam", "map/{sample}.sort.bam.bai"
	output:"map_clean/{sample}.sort.delMT.bam"
	threads: 2
	log:"map_clean/{sample}.sort.delMT.bam.log"
	shell:"samtools view -@ {threads} -bh {input[1]} `cat {input[0]}` > {output} 2>{log}"

# filter by MAPQ:
# 1 需要不需要按MAPQ过滤？需要，下例bowtie2按照>10过滤的。
# 2 三个过滤怎么排序好？不确定
# 3 怎么设置其他参数?
# https://informatics.fas.harvard.edu/atac-seq-guidelines-old-version.html#alignments
# https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/src/encode_task_filter.py
rule rm_low_MAPQ:
	input:"map_clean/{sample}.sort.delMT.bam"
	output:"map_clean/{sample}.sort.delMT.HmapQ.bam"
	threads: 8
	shell:"samtools view -@ {threads} -bh -q 30 {input} > {output}"

# remove duplicate: 这个放最后的优势是，会自动生成bai文件
rule rm_dup:
	input:"map_clean/{sample}.sort.delMT.HmapQ.bam"
	output:"map_clean/{sample}.final.bam"
	threads: 8
	params: tmp="map_clean/tmp/{sample}"
	log:"map_clean/{sample}.sort.delMT.HmapQ.sambambaRmDup.bam.log"
	shell:"sambamba markdup -r -t {threads} --tmpdir {params.tmp} {input} {output} >{log} 2>&1"

rule clean_done:
	input: expand("map_clean/{sample}.final.bam", sample=SI)
	output: "map_clean/done.txt"
	params: tmp="map_clean/tmp/"
	shell: "rm -rf {params.tmp} && touch {output}"



(5) call peak 
$ cat rules/05_call_peak.sf
rule macs2_callpeak:
	input:
		"map_clean/{sample}.final.bam"
	output:
		"macs2_result/{sample}_peaks.narrowPeak"
	params:
		"{sample}",
		"macs2_result"
	log:
		"macs2_result/{sample}_peaks.narrowPeak.log"
	shell:
		"macs2 callpeak -t {input} -f BAM -g hs \
		--shift -100 --extsize 200 --nomodel \
		-B --SPMR --call-summits \
		--outdir {params[1]} -n {params[0]} >{log} 2>&1"





测试: 在工作目录
$ snakemake -s /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/main.sf -j 10 -p










3. 实例: Hair project (Run by ATAC-seq workflow version v0.3)

共96个文件
$ ls /home/wangjl/data/ATAC/hair/raw/origin/ | head
cgm-1_R2.fastq.gz
cgm-96_R2.fastq.gz

$ seq 1 4
1
2
3
4

(1) 放fq文件到raw中
$ cd /data/wangjl/ATAC/fq/test5/
$ path1="/home/wangjl/data/ATAC/hair/raw/origin/"
$ seq 1 96 | while read i; do 
id="cgm-"${i};
echo $id; 
ln -s ${path1}/${id}_R1.fastq.gz raw/;
ln -s ${path1}/${id}_R2.fastq.gz raw/;
echo $id >>raw/cid.txt
done;


$ zcat raw/cgm-2_R1.fastq.gz | head
@A00582:646:H7CJTDSX2:4:1101:27534:1141 1:N:0:CTAAACGG+AGAGGATA
CTTATACACATCTCCGAGCCCACGAGACCTAAACGGATCTCGGATGCCGTCTTGGGCGTGGAAAGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
+
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,:F,,:,,,F,,,,,,F,,,F,,,,,,:::FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF


(2) 写配置文件
$ cp raw/cid.txt config.yaml
$ sed -i "s/^/ - /" config.yaml

$ vim config.yaml ###add three lines on top
workdir: /data/wangjl/ATAC/fq/test5/
Snk_RootDir: /home/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2
samples: 


(3) 生成总的流程图
$ mkdir images
$ rootPath="/data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2"
$ snakemake -s ${rootPath}/main.sf --dag | dot -Tsvg > images/ATAC-seq_Sample.svg
$ snakemake -s ${rootPath}/main.sf --rulegraph 2> /dev/null | dot -Tsvg > images/ATAC-seq_Rule.svg


(4) 运行
## dry run
$ snakemake -s /home/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/main.sf -np

## 实际运行
$ snakemake -s /home/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/main.sf -j 8 -p
Job counts:
        count   jobs
        96      QC_clean_fq
        96      QC_raw_fq
        1       all
        96      bwa_mapping
        1       clean_done
        96      cut_adapter
        96      del_MTs
        96      get_chrlist
        96      index
        96      macs2_callpeak
        1       multi_QC_clean
        1       multi_QC_raw
        96      rm_dup
        96      rm_low_MAPQ
        96      sort_to_bam
        1060
[Thu Jul 15 23:05:42 2021] --> [Fri Jul 16 00:18:47 2021]
耗时 1h15min。







========================================
|-- 合并bam并质控: TSS附近reads分布图(巨慢)、插入片段密度图
----------------------------------------
1. 因为这个步骤太慢。

(1) 6个完全样本: 12个fq文件
[Wed Jun 23 20:35:59 2021] 开始
[Wed Jun 23 22:53:40 2021] 晚上走之前，37 of 105 steps (35%) done。map还没结束。

[Thu Jun 24 05:39:03 2021] 早上过来，  78 of 105 steps (74%) done，QC_map/matrix
[Thu Jun 24 09:50:30 2021] 中午吃饭前  94 of 105 steps (90%) done，MAP_QC_plotHeatmap
[Thu Jun 24 13:32:22 2021]  最后一个文件不知道为什么这么慢？     103 of 105 steps (98%) done

[Thu Jun 24 16:43:37 2021]  105 of 105 steps (100%) done
一共运行了 20h 多。
晚8:35-半夜0点 3.5h
0点-16:43      16.7h
Complete log: /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo1/.snakemake/log/2021-06-23T203559.284905.snakemake.log



(2) 同时单细胞效果很差，合并后做才有效果。
合并bam文件: 
$ ls map_clean/*final.bam|wc
     96      96    2583
$ ls map_clean/*final.bam| head
map_clean/cgm-10.final.bam
map_clean/cgm-11.final.bam
map_clean/cgm-12.final.bam
总行数 63.3万行。
$ ls map_clean/*final.bam |xargs -i samtools view {} | wc
 632936 10835066 242551552
#

$ samtools merge total.bam c1.bam c2.bam c3.bam c4.bam

$ ls map_clean/*final.bam |xargs -i ls -l {}
$ ls map_clean/*final.bam |xargs -i echo {}

$ mkdir merged
$ ls map_clean/*final.bam |xargs -i samtools merge merged/total.bam {} #失败

$ samtools merge merged/total.bam `ls map_clean/*final.bam |xargs`
$ samtools view  merged/total.bam |wc
 632936 10835066 242551552
# 和合并之前的测试一致。








## check 中间文件
$ cat map_clean/cgm-10.chrlist
chr1 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr2 chr20 chr21 chr22 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chrX chrY
$ samtools view  map_clean/cgm-10.final.bam | awk '{print $5}'  | sort |uniq -c | sort -k1nr
   4649 60
    135 40
     89 37
     46 46
     26 45
     22 48
$ samtools view -q 60 map_clean/cgm-10.final.bam  | wc
 4649   79233 1859898

# check 合并后的文件
$ samtools view merged/total.bam | awk '{print $5}'  | sort |uniq -c | sort -k1nr
 570980 60
  15499 40
   4243 46
   3593 48
   3403 45








2. TSS 分布图、插入片段长度密度图
需要提前 index .

$ cat bam_QC.sf
REF_BED="/home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.bed"
SI=["total"]

rule all:
	input:
		expand("QC_map/TSS/{sample}.TSS.heatmap.pdf", sample=SI), #BAM QC: TSS (巨慢)
		expand("QC_map/fragment_length/fragment_length_{sample}.pdf", sample=SI),

include: "rules/B01_mapQC.sf"



$ cat rules/B01_mapQC.sf
rule index:
	input: "{sample}.bam"
	output:"{sample}.bam.bai"
	shell:"samtools index {input}"

## target 1: TSS
rule MAP_QC_bamCoverage:
	input: "merged/{sample}.bam"
	output: "QC_map/{sample}.bw"
	log: "QC_map/{sample}.bw.log"
	shell: "bamCoverage -b {input} -o {output} >{log} 2>&1"

rule MAP_QC_computeMatrix:
	input: "QC_map/{sample}.bw"
	output:"QC_map/TSS/{sample}.mat.gz"
	log: "QC_map/TSS/{sample}.mat.gz.log"
	shell:"computeMatrix reference-point --referencePoint TSS \
       -S {input} -R {REF_BED} \
       -b 1000 -a 1000 -o {output} >{log} 2>&1"

rule MAP_QC_plotHeatmap:
	input: "QC_map/TSS/{sample}.mat.gz",
	output:"QC_map/TSS/{sample}.TSS.heatmap.pdf"
	log: "QC_map/TSS/{sample}.TSS.heatmap.pdf.log"
	shell:"plotHeatmap -m {input} -out {output}"


## target 2: fragment_length
rule get_fragment_length:
	input:"merged/{sample}.bam"
	output:"QC_map/fragment_length/{sample}.fragment.length.txt"
	log:"QC_map/fragment_length/{sample}.fragment.length.txt.log"
	shell:"samtools view {input} | \
		awk -F'\t' 'function abs(x){{return ((x < 0.0) ? -x : x)}} {{print $1\"\t\"abs($9)}}' | \
		sort | uniq | cut -f2 > {output} 2>{log}"

rule draw_plot:
	input: 
		"merged/{sample}.bam",
		"QC_map/fragment_length/{sample}.fragment.length.txt",
		"merged/{sample}.bam.bai"
	output: "QC_map/fragment_length/fragment_length_{sample}.pdf"
	params: key="{sample}"
	log: "QC_map/fragment_length/fragment_length_{sample}.pdf.log"
	script: "../scripts/B01_draw_fragment_length.R"
	

$ cat scripts/B01_draw_fragment_length.R
## 同时生成pdf和png
# myArgs<-commandArgs(TRUE)
infile=snakemake@input[[1]]
outfile=snakemake@output[[1]]
params.key=snakemake@params[["key"]]

# 记录日志，屏幕上还是显示很多
log <- file(snakemake@log[[1]], open="wt")
sink(log)

library("ATACseqQC");
pdf(outfile, width=5, height=5);
fragSizeDist(infile, params.key)
dev.off();

outfile2=paste0(substring(outfile, 1, nchar(outfile)-3), "png")
png(outfile2, width=1500, height=1500, res=300);
fragSizeDist(infile, params.key)
dev.off();


## 在工作目录运行
$ snakemake -s /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/bam_QC.sf -j 5 -p
[Fri Jul 16 11:03:40 2021] --> [Fri Jul 16 11:21:03 2021] 20min.





fIn="QC_map/fragment_length/total.fragment.length.txt"
data <- read.table(fIn, header = F)
# 设置插入片段长度的阈值，过滤掉太长的片段
length_cutoff <- 1200
fragment <- data$V1[data$V1 <= length_cutoff]
# 利用直方图统计频数分布，设置柱子个数
breaks_num <- 500
res <- hist(fragment, breaks = breaks_num, plot = FALSE)

#pdf('1.pdf', width=6, height=4);
# 添加坐标原点
plot(x = c(0, res$breaks),
     y = c(0, 0, res$counts) / 10^2,
     type = "l", col = "red", ylim=c(0,100),
     xlab = "Fragment length(bp)",
     ylab = expression(Normalized ~ read ~ density ~ 10^2),
     main = "Sample Fragment sizes")
# dev.off()






========================================
|-- 合并后 call peak 和 motif 分析
----------------------------------------

1. call peak and find motif 
$ cat mainBulk.sf 
SI=["total"]  

rule all:
	input:
		expand("homer/{sample}/knownResults.html", sample=SI)

rule macs2_callpeak:
	input:"merged/{sample}.bam"
	output:"macs2_result/{sample}_peaks.narrowPeak"
	params:
		"{sample}",
		"macs2_result"
	log: "macs2_result/{sample}_peaks.narrowPeak.log"
	shell: "macs2 callpeak -t {input} -f BAM -g hs \
		--shift -100 --extsize 200 --nomodel \
		-B --SPMR --call-summits \
		--outdir {params[1]} -n {params[0]} >{log} 2>&1"

include:"rules/06_find_motif.sf"



$ cat rules/06_find_motif.sf
GENOME   ="/home/wangjl/data/ref/human/UCSC/hg38.fa"
## 命令中指定输出目录
rule homer_find_motif: 
	input: "macs2_result/{sample}_peaks.narrowPeak"
	output: "homer/{sample}/knownResults.html"
	params: out_dir="homer/{sample}/"
	threads: 4
	log:"homer/{sample}/knownResults.html.log"
	#shell:"findMotifsGenome.pl {input} {GENOME} {params.out_dir} -size given >{log} 2>&1"
	shell:"findMotifsGenome.pl {input} {GENOME} {params.out_dir} -size given -p {threads} -preparse >{log} 2>&1"


# 所有的选项都在最后添加，否则会出错。
# -p <#> (Number of processors to use, default: 1)
# -preparse (force new background files to be created)



测试
$ snakemake -s /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/mainBulk.sf -j 5 -p
11:34 --> 12:03, 30min







========================================
|-- 峰注释及可视化: 饼图
----------------------------------------
1. 峰注释

$ annotatePeaks.pl macs2_result/total_peaks.narrowPeak hg38 >anno/total_peakAnn.xls 
http://homer.ucsd.edu/homer/ngs/annotation.html




2. 结果可视化：饼图

$ head anno/total_peakAnn.xls -n3
PeakID (cmd=annotatePeaks.pl macs2_result/total_peaks.narrowPeak hg38)  Chr     Start   End     Strand  Peak Score      Focus Ratio/Region Size Annotation      Detailed Annotation     Distance to TSS Nearest PromoterID      Entrez ID       Nearest Unigene Nearest Refseq   Nearest Ensembl Gene Name       Gene Alias      Gene Description        Gene Type
total_peak_1    chr1    9940    10249   +       215     NA      Intergenic      (TAACCC)n|Simple_repeat|Simple_repeat   -1780   NR_046018       100287102       Hs.618434       NR_046018       ENSG00000223972 DDX11L1 -       DEAD/H-box helicase 11 like 1   pseudo
total_peak_8    chr15   41031867        41032170        +       200     NA      intron (NM_017553, intron 24 of 35)     (CACAG)n|Simple_repeat|Simple_repeat    78547   NM_024111       79094   Hs.155569       NM_024111       ENSG00000128965 CHAC1   -       ChaC glutathione specific gamma-glutamylcyclotransferase 1       protein-coding


使用第八列画图
$ cat anno/total_peakAnn.xls | awk -F "\t" '{print $8}' | head
Annotation
Intergenic
intron (NM_017553, intron 24 of 35)
Intergenic
TTS (NR_146144)
Intergenic

$ cat anno/total_peakAnn.xls | awk -F "\t" '{print $8}' | awk -F"(" '{print $1}' | sed  -e 's/[ ]*$//g' -e "1d" > anno/anno.col8-2.txt 
$ head anno/anno.col8-2.txt 
Intergenic
intron 
Intergenic


> R
d1=readLines("anno/anno.col8-2.txt")
d2=sub(" ", '', d1)
d3=data.frame(table(d2))
colnames(d3)=c("location", "Freq")
d3=d3[order(-d3$Freq),]

pdf("anno/pie.pdf", width=4, height=4)
par(mar=c(3,2,2,6))
pie(d3$Freq, labels=paste0(d3$location, '(', d3$Freq, ')' ) )
dev.off()






========================================
ChIP-seq 分析
----------------------------------------


ref:
ChIP-seq双端测序数据分析 https://www.jianshu.com/p/5fb041f09953










========================================
----------------------------------------







========================================
----------------------------------------








========================================
----------------------------------------







========================================
----------------------------------------








========================================
----------------------------------------







========================================
----------------------------------------










