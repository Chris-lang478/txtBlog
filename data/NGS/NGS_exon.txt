单基因遗传病的生信分析流程：SNP、Indel等(NGS_exon WES)


WES数据一般用于SNP和Indel分析，不能用于CNA和SA的分析。



========================================
单基因遗传病分析(SNP/Indel)的主要流程和软件
----------------------------------------

1.主要流程 
质控->比对得到map->BAM处理->call突变->合并gvcf

参考GATK Germline Best Practice


一文囊括全基因组测序各步骤工具
https://mp.weixin.qq.com/s/ONl1UuY3EC3eJG94SKDHAg#rd


2.SNP分析所需软件(来自诺禾致源的说明书)

分析内容	软件	备注	版本
1).比对	BWA、Samblaster	测序结果与参考基因组进行比对，得到bam结果	0.1.22
        Sambamba	标记重复read	v0.4.7
2).SNP/INDEL检测	SAMtools	检测、过滤SNP及INDEL变异	1.0
3).CNV检测	control-FREEC	检测样本CNV	v6.7
4).变异功能注释	ANNOVAR	对检测到的变异进行结构和功能注释	2013Aug23






参考序列比对分析结果
有效测序数据通过 BWA(version 0.7.8-r455)比对到人类参考基因组(UCSC hg19)，得到 BAM 格式的最初的比对结果。
在最初的比对结果的基础上做如下处理:
(1) 用samblaster（Version 0.1.21）从最初的比对结果中挑选split reads和discord reads；
(2) 用sambamba（0.6.6）对比对结果进行排序和merge;
(3) 用sambamba去除重复的reads。经过以上处理，得到 BAM 格式的最终比对结果。

后续变异检测基于这个比对结果。
如果一个或一对read(s)在基因上可以有多个比对位置，BWA的处理策略是从中 选择一个最好的，如果有两个或以上最好的比对位置，则从中随机选择一个。这种多重比对(multiple hit)的处理对 SNP、indel 以及 CNV 等的检测有重要影响。通常检测 SNP 或 INDEL 的时候要使用高质量的比对(alignment)，即比对质量值大于0或更高。

本文件夹下包含每个样本(样本名)的比对结果数据结果文件:
1.后缀 bam 文件 比对结果文件
2.后缀.bam.bai文件 对bam文件构建索引，用于对bam文件的快速处理

结果文件说明:
1.后缀 bam 文件是 sam 文件的压缩格式，解读请参考附件 SAMv1.pdf
2.后缀 bam.bai 文件是由命令 sambamba index *.bam 得到





3.外显子测序结题报告
https://wenku.baidu.com/view/ff8cd0f3f424ccbff121dd36a32d7375a417c6bb.html
1)对raw data 过滤，得到clean reads
	- 过滤掉 reads 的接头序列
	- SE测序低质量碱基数超过该reads的50%时，舍弃该reads
	- SE reads中的N碱基超过10%时，舍弃该reads
	- fastqc统计：reads数量、数据产量、质量分布。
2)使用BWA比对到参考基因组GRCh38上
3)使用Picard去除重复reads
4)使用GATK做局部重比对和碱基质量校正[好像已经去掉了]
5)基于比对结果，统计每个样品的测序深度、覆盖度、比对率等评价指标

6)使用 GATK 的HaplotypeCaller检测基因组变异，包括SNP和Indel
7)使用基于深度信号方法的 CNVnator v0.2.7检测拷贝数变异CNV
8)使用Breakdancer或者CREST检测结构变异
9)使用SnpEff软件对变异结果进行注释及影响预测。







========================================
Seqtools 及 参考基因组 及参考文献
----------------------------------------
1.参考基因组的md5sum值：https://taichimd.us/mediawiki/index.php/Seqtools
BRB-SeqTools： https://linus.nci.nih.gov/seqtools/




2.参考基因组
(1)ClinVar使用的是hg38基因组：
https://www.ncbi.nlm.nih.gov/clinvar/RCV000139793/#clinical-assertions



(2)refGene.txt 的说明：
$ head refGene.txt -n 2
75      NM_001172656    chr4    -       2271323 2420370 2272451 2420050 12      2271323,2273037,2273401,2274899,2275788,2306015,2321896,2339133,2341179,2343204,2355659,2420011,        2272583,2273141,2273506,2275016,2275943,2307263,2321998,2339223,2341382,2343342,2355800,2420370,    0       ZFYVE28 cmpl    cmpl    0,1,1,1,2,2,2,2,0,0,0,0,

每列的解释：http://genome.ucsc.edu/cgi-bin/hgTables

wget -c -O mm9.refGene.txt.gz http://hgdownload.soe.ucsc.edu/goldenPath/mm9/database/refGene.txt.gz
wget -c -O mm10.refGene.txt.gz http://hgdownload.soe.ucsc.edu/goldenPath/mm10/database/refGene.txt.gz
wget -c -O hg19.refGene.txt.gz http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz
wget -c -O hg38.refGene.txt.gz http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/refGene.txt.gz

$ gunzip hg38.refGene.txt.gz
$ less -S hg38.refGene.txt

提取其中的带有geneID的一列，并且没有重复
$ awk '{print $7}' refGene.list.txt | sort -u > uniqu.refGene.list.txt
查看列数
$ wc -l uniqu.refGene.list.txt 




(3)GTF格式的refGene如何在Ensembl及UCSC下载
http://blog.sina.com.cn/s/blog_751bd9440102v76a.html


(4) igenome
http://jp.support.illumina.com/sequencing/sequencing_software/igenome.html





3.单基因遗传病文章

单基因遗传病——家系样本研究思路
基因测序产业网  2016-05-06

[1] https://en.wikipedia.org/wiki/Genetic_disorder

[2] Feng R, Sang Q, Kuang Y,et al. Mutationgs in TUBB8 and Human Oocyte Meiotic Arrest. N Engl J Med. 2016, 374(3): 223-32. 
外显子组测序解析卵细胞发育障碍


[3] Lukacs V, Mathur J, Mao R, et al. Imparied PIEZO1 function in patients with a novel autosomal recessive congentital lymphatic dysplasia. Nat Commun. 2015, 6: 8329.
外显子组测序解析常染色体隐性先天性淋巴管发育不良


最全！华大基因单基因遗传病检测科研成果汇编
原创： 华大医学  华大医学  2018.9月3日



========================================
ClinVar数据库统计单基因遗传病致病位点人群频率
----------------------------------------
原创： 基因游侠  基因检测与解读  2018.5月1日

评估一个基因位点是否为患者的致病基因，游侠认为需要从三个方面来考虑，
第一是临床评估，即该基因在数据库（如OMIM）中记录的表型是否与患者的表型想符合，
第二是生物信息学评估，包括正常人群频率、突变类型、软件预测、序列保守性等等，
第三是遗传模式评估，即如果是隐性遗传是否为父母（父母正常）分别携带一个位点，如果是显性遗传是否为新发突变（父母正常）。

万分之一，后来的实际工作中发现这个cutoff过于严谨，容易把真阳性位点排除。


首先我们从ClinVar官网下载最新的VCF，下载地址为
ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/
ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/

个人推荐对于隐性遗传基因cutoff设为0.005或0.01，对于显性遗传基因cutoff设为0.001或0.005，以上只是初步粗略统计，推荐cutoff仅供参考，谢谢！





========================================
fastq数据的过滤、修剪、错误去除和质控[fastqc + cutadapt]
----------------------------------------
1.质控用fastqc
http://www.bioinformatics.babraham.ac.uk/projects/fastqc/

$ fastqc --version
FastQC v0.11.5

-o参数，指定输出到文件夹


This has some good examples: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/
Then you can look into the following: Trimmomatic, Fastx-toolkit to trim data.


更多质控软件： http://blog.sina.com.cn/s/blog_9b78c9110101couq.html





2.质控的参数：
http://www.biotrainee.com/thread-324-1-1.html

(1)如果测序质量不合格，则需要：
fastq_quality_filter -v -Q 64 -q 20 -p 75 -i sample.fastq -o sample_filtered.fastq

(2)如果都是reads的前6个bp碱基有问题，则需要
fastx_trimmer -v -f 7 -l 36 -i sample_filtered.fastq -o sample_filtered_and_trimmed.fastq

(3)如果混入了大量的接头，则需要！
cutadapt -m 20 -e 0.1 -a GATCGGAAGAGCACACGTCTGAACTCCAGTCACACA sample2.fastq \ -o sample2--cutadapt.fastq
需要自己去查自己的接头是什么序列：https://github.com/csf-ngs/fastq ... ontaminant_list.txt

质控可视化：
(A) Average quality score for each base position, 
(B) GC content distribution, 
(C) Average Phred quality score distribution, 
(D) Base composition and 
(E) read length distribution for both input (red) and HQ filtered (green) data. 
(F) Percentage of reads with different quality score ranges at each base position. 

如果是特殊测序，质控需要加一些步骤
WES：??


问题：如果双端测序，有一个被过滤掉了，下一步bwa比对不通过怎么办？http://www.biotrainee.com/thread-324-1-1.html
解答：用cutadapt软件来对双端测序数据去除接头 http://www.bio-info-trainee.com/1920.html
Posted on 2016年10月6日
一般来讲，我们对测序数据进行QC，就三个大的方向：Quality trimming， Adapter removal， Contaminant filtering。
当我们是双端测序数据的时候，去除接头时，也会丢掉太短的reads，就容易导致左右两端测序文件reads数量不平衡，有一个比较好的软件能解决这个问题，我比较喜欢的是cutadapt软件的PE模式来去除接头！尤其是做基因组或者转录组de novo 组装的时候，尤其要去掉接头，去的干干净净！

既然fastqc能探测到你的接头，说明它里面内置了所有的接头序列，在github可以查到：https://github.com/csf-ngs/fastqc/blob/master/Contaminants/contaminant_list.txt
或者：Download common Illumina adapters from   https://github.com/vsbuffalo/scythe/blob/master/illumina_adapters.fa
TruSeq Universal Adapter AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Small RNA 3p Adapter 1 ATCTCGTATGCCGTCTTCTGCTTG
最严重的一般是TruSeq Universal Adapter ， 而且它检测到你其它的接头可能就是这个 TruSeq Universal Adapter 的一部分而已~！ 




3.剪切接头
$ cutadapt --version
1.18

See https://cutadapt.readthedocs.io/ for full documentation.
https://www.biomart.cn/news/16/2842104.htm

测试方法：
$ cutadapt -o output.fastq.gz input.fastq.gz
$ tail -n 4 input.fastq | cutadapt -a AACCGGTT

## PE reads
$ cutadapt -a ADAPTER_FWD -A ADAPTER_REV -o out.1.fastq -p out.2.fastq reads.1.fastq reads.2.fastq
-p is the short form of --paired-output. 



要用PE模式来去除接头，保证过滤后的reads还是数量继续平衡的。
--pair-filter=both
 Processing reads on 5 cores in paired-end legacy mode ...
 WARNING: Legacy mode is enabled. Read modification and filtering options *ignore* the second read. To switch to regular paired-end mode, provide the --pair-filter=any option or use any of the -A/-B/-G/-U/--interleaved options.

--max-n=COUNT  Discard reads with more than COUNT 'N' bases. If COUNT在0和1之间，则为比例。
	--max-n=10 最多10个N碱基，否则舍弃；
-e 0.1 错误率超过10%舍弃。-e 引物匹配允许错误率，我调置0.15，一般引物20bp长允许3个错配，为了尽量把引物切干净 
-j N 或者--cores=N 多核
-O 5 Require MINLENGTH overlap between read and adapter for an adapter to be found. Default: 3
-q 30 3'端质量低于30的舍弃
-m 100 去除接头后如果read长度小于50舍弃
-o 输出文件
-p 另一个输出文件

看图，需要除掉低质量的碱基。
$ cutadapt --max-n=10 -e 0.1 -j 5 -q 30 -m 100 --pair-filter=both -o outN.1.fastq -p outN.2.fastq ../0_fastqc/B1709060700_NDHE00383-A84-A68_AH7277CCXY_L7_1.fq ../0_fastqc/B1709060700_NDHE00383-A84-A68_AH7277CCXY_L7_2.fq

Finished in 454.01 s (11 us/read; 5.30 M reads/minute).

=== Summary ===

Total read pairs processed:         40,082,567
  Read 1 with adapter:                       0 (0.0%)
  Read 2 with adapter:                       0 (0.0%)
Pairs that were too short:             626,386 (1.6%)
Pairs with too many N:                     811 (0.0%)
Pairs written (passing filters):    39,455,370 (98.4%)

Total basepairs processed: 12,024,770,100 bp
  Read 1: 6,012,385,050 bp
  Read 2: 6,012,385,050 bp
Quality-trimmed:             509,713,908 bp (4.2%)
  Read 1:   163,408,047 bp
  Read 2:   346,305,861 bp
Total written (filtered):  11,454,864,271 bp (95.3%)
  Read 1: 5,813,730,111 bp
  Read 2: 5,641,134,160 bp



可以删除那些原始fq数据了，保留一份fq.gz即可。





========================================
比对：BWA的安装 与使用
----------------------------------------
1.http://bio-bwa.sourceforge.net/

BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads.

BWA适用于变异比对。



2.下载
https://sourceforge.net/projects/bio-bwa/files/

https://github.com/lh3/bwa/releases
$ wget https://github.com/lh3/bwa/releases/download/v0.7.17/bwa-0.7.17.tar.bz2
$ tar -xjvf bwa-0.7.17.tar.bz2
$ cd bwa-0.7.17
$ make #报错


说明文件是这么说的：
git clone https://github.com/lh3/bwa.git
cd bwa; make
./bwa index ref.fa
./bwa mem ref.fa read-se.fq.gz | gzip -3 > aln-se.sam.gz
./bwa mem ref.fa read1.fq read2.fq | gzip -3 > aln-pe.sam.gz

最终是这么安装的：
$ sudo yum install bwa
版本号：
Program: bwa (alignment via Burrows-Wheeler transformation)
Version: 0.5.9-r16
发现版本太低，重新安装。


使用 sudo make 结果正常。
bwa 0.7.17-r1188

########




3.比对

Does BWA call SNPs like MAQ?
No, BWA only does alignment. Nonetheless, it outputs alignments in the SAM format which is supported by several generic SNP callers such as samtools and GATK.
比对之后去重，
然后用samtools或者GATK call SNP。

http://bio-bwa.sourceforge.net/bwa.shtml


(1)建立索引
参考基因组
$ ls -lth /home/data/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/genome.fa
-rwxrwxr-x. 1 wangjl wangjl 3.0G Aug 19  2015 /home/data/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/genome.fa


Algorithm for constructing BWT index. Available options are:
有两种建立参考基因组的策略：
is	IS linear-time algorithm for constructing suffix array. It requires 5.37N memory where N is the size of the database. IS is moderately fast, but does not work with database larger than 2GB. IS is the default algorithm due to its simplicity. The current codes for IS algorithm are reimplemented by Yuta Mori.
is 相对快，但是参考基因组不能大于2GB。
bwtsw	Algorithm implemented in BWT-SW. This method works with the whole human genome.
bwtsw能用于人全基因组分析。

https://zhuanlan.zhihu.com/p/36267250
-a指定建立索引的算法，bwtsw，is或者rb2，以前没有rb2，而是div。is还是适合小于2G的基因组，bwtwt是大于10M的基因组才行，这个地方不要选错了，细菌基因组一般都小于10M，只能用is算法，人基因组是3G，只能用bwtsw，处于中间的选择哪个都可以。


$ pwd
/home/data/Homo_sapiens/UCSC/hg38/Sequence/BWAIndex/version0.7.x
$ bwa index genome.fa
##14:51 to 16:05
#...
##[BWTIncConstructFromPacked] 680 iterations done. 6184133946 characters processed.
生成 hg19.fasta.amb、hg19.fasta.ann、hg19.fasta.bwt、hg19.fasta.pac、hg19.fasta.sa 五个文件。
#[main] Version: 0.7.17-r1188 
#[main] CMD: bwa index genome.fa
#[main] Real time: 4420.476 sec(73min); CPU: 4367.864 sec


好像不对，人基因组大于3GB，应该使用另一种索引方式：
$ bwa index -a bwtsw genome.fa
#[main] Version: 0.7.17-r1188
#[main] CMD: bwa index -a bwtsw genome.fa
#[main] Real time: 4529.281 sec; CPU: 4443.430 sec


https://www.biostars.org/p/53546/
按照这个来看，不指定，则bwa自动选择，选择的依据还是基因组大小。
// simplified sample from bwtindex.c:
if (algo_type == "auto") {
    if (l_pac > 50000000)
        algo_type = "bwtsw";
    else
        algo_type = "is";
}
所以，我以上两个建立的索引应该是一样。
比一下md5，真的一模一样。并且和version0.6.0一模一样。






(2)比对：bwa mem 或者 bwa aln 哪个好呢？
$ bwa mem ref.fa read1.fq read2.fq > aln-pe.sam
-t 5 线程
-M            mark shorter split hits as secondary
-R STR        read group header line such as '@RG\tID:foo\tSM:bar' [null]
  定义头文件 -R '@RG\tID:foo\tSM:bar'，如果在此步骤不进行头文件定义，在后续GATK分析中还是需要重新增加头文件。

示例代码
$ bwa mem -t 8 -M -R '@RG\tID:${name}\tLB:${name}\tPL:ILLUMINA\tPM:X10\tSM:${name}' ${INDEX} ${RAW_DATA}/${name}_1.fastq ${RAW_DATA}/${name}_2.fastq > ${WORKING_DIR}/2018rerun/processed_bam/${name}.sam


实际代码
$ INDEX="/home/data/Homo_sapiens/UCSC/hg38/Sequence/BWAIndex/genome.fa"
$ bwa mem -t 6 -M ${INDEX} ../1_cleanData/outN.1.fastq ../1_cleanData/outN.2.fastq > processed.sam
20:52 to 21:50

[main] Version: 0.7.17-r1188
[main] CMD: bwa mem -t 6 -M /home/data/Homo_sapiens/UCSC/hg38/Sequence/BWAIndex/genome.fa ../1_cleanData/outN.1.fastq ../1_cleanData/outN.2.fastq
[main] Real time: 3491.090 sec(0.96hour); CPU: 20559.125 sec






========================================
Samblaster
----------------------------------------

https://github.com/GregoryFaust/samblaster
https://doi.org/10.1093/bioinformatics/btu314

1.安装 (支持linux/mac OS Version 10.7以上)
$ git clone git://github.com/GregoryFaust/samblaster.git
$ cd samblaster
$ make
$ sudo cp samblaster /usr/local/bin/
$ samblaster --version
samblaster: Version 0.1.24


2.简介
Author: Greg Faust (gf4ea@virginia.edu)
Tool to mark duplicates and optionally output split reads and/or discordant pairs.
Input sam file must contain paired end data, contain sequence header and be sorted by read ids.
Output will be all alignments in the same order as input, with duplicates marked with FLAG 0x400.


refer:

2.中文
https://www.sohu.com/a/246097230_99971433




========================================
Sambamba: process your BAM data faster!（去重 比 Picard 快）
----------------------------------------
1.Picard官网：for users
http://broadinstitute.github.io/picard/

A set of command line tools (in Java) for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.

2.Sambamba 官网
https://github.com/biod/sambamba

http://lomereiter.github.io/sambamba/docs/sambamba-view.html
sambamba主要有filter，merge,slice和duplicate等七个功能来处理sam/bam文件。

推荐：https://www.sohu.com/a/246097230_99971433
推荐两款sam文件处理小工具samblaster和sambamba，它们具有排序、比对信息查看等常用功能之外，最棒的是可以用来代替picard去除重复序列，在筛选标准不变的前提下速度能提升30倍以上

下载地址
https://github.com/lomereiter/sambamba/releases
安装：拷贝至全局环境变量路径即可

$ wget https://github.com/biod/sambamba/releases/download/v0.6.8/sambamba-0.6.8-linux-static.gz
$ gunzip sambamba-0.6.8-linux-static.gz
$ mv sambamba-0.6.8-linux-static sambamba
$ chmod +x sambamba
$ sudo mv sambamba /usr/local/bin/
$ sambamba -v
#sambamba 0.6.8


refer:
1.官网 http://lomereiter.github.io/sambamba/docs/sambamba-view.html
2.中文参考：
http://blog.sciencenet.cn/home.php?mod=space&uid=1094241&do=blog&id=1041577
https://www.sohu.com/a/246097230_99971433
https://blog.csdn.net/msw521sg/article/details/65938377



========================================
samtools
----------------------------------------
1.SAM (Sequence Alignment/Map) format is a generic format for storing large nucleotide sequence alignments. SAM aims to be a format that:
http://samtools.sourceforge.net/



2.安装： 
$ wget https://sourceforge.net/projects/samtools/files/latest/download
$ mv download samtools-1.9.tar.bz2
$ tar -xjvf samtools-1.9.tar.bz2
$ cd samtools-1.9
$ make 
## FAILED.  Either configure --without-curses or resolve this error to build

$ sudo yum install ncurses-devel
没用，还报错。



算了，还是yum安装吧：
$ sudo yum install samtools
$ samtools

Program: samtools (Tools for alignments in the SAM format)
Version: 0.1.18 (r982:295)

$ sudo yum remove samtools #卸载yum安装的




版本老，还是尽力安装最新的：
$ sudo ./configure
$ sudo make #不报错了
$ sudo make install

$ samtools
Program: samtools (Tools for alignments in the SAM format)
Version: 1.9 (using htslib 1.9)



========================================
bedtools: a powerful toolset for genome arithmetic
----------------------------------------
1. bedtools: flexible tools for genome arithmetic and DNA sequence analysis.
usage:    bedtools <subcommand> [options]

https://bedtools.readthedocs.io/en/latest/index.html


2.安装
https://bedtools.readthedocs.io/en/latest/content/installation.html

$ wget https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz
$ tar -zxvf bedtools-2.25.0.tar.gz
$ cd bedtools2
$ make

缺少  #include <zlib.h>
http://zlib.net/
$ wget http://zlib.net/zlib-1.2.11.tar.gz
$ tar zxvf zlib-1.2.11.tar.gz
$ ./configure
$ make 
$ sudo make install


还是报错。
算了，还是最简单yum安装吧：
https://bedtools.readthedocs.io/en/latest/content/installation.html
$sudo yum install BEDTools
$ bedtools -version
bedtools 2.15.0
#版本有点老，卸载
$ sudo yum remove BEDTools



重新安装新版本。
$ sudo make 
# 报错 gzstream.C:(.text+0x2a2): undefined reference to `gzopen64'

##https://bedtools.readthedocs.io/en/latest/content/faq.html
## 先确定gzopen64是否存在，再更新链接，使该函数可被调用。
$ locate libz.so | less
$ objdump -T /usr/local/lib/libz.so | grep gzopen64
## 00000000000120f0 g    DF .text  0000000000000304  ZLIB_1.2.3.3 gzopen64

$ sudo make LIBS='/usr/local/lib/libz.so'
$ sudo make install LIBS='/usr/local/lib/libz.so'

##退出再登录还是报错： bedtools: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by bedtools)
$ sudo ln -sf /usr/local/lib/libz.so /lib64/libz.so.1

$ bedtools --version
bedtools v2.25.0



========================================
Control-Freec:检测拷贝数变异的神器
----------------------------------------
1. 简介
http://bioinfo-out.curie.fr/projects/freec/


Control-Freec 既可以检测拷贝数变异CNV，还可以分析杂合性缺失LOH。官网如下
http://boevalab.com/FREEC/

Control-FREEC is a tool for detection of copy-number changes and allelic imbalances (including LOH) using deep-sequencing data originally developed by the Bioinformatics Laboratory of Institut Curie (Paris). Nowdays, Control-FREEC is supported by the team of Valentina Boeva at Institut Cochin, Inserm(Paris). 

在检测拷贝数变异时，支持全基因组测序，全外显子测序，目标区域捕获测序等多种测序方案，
对于全基因组数据，分析是不需要提供对照样本；
对于全外显子测序和目标区域捕获测序，必须提供对照样本。

# Control-FREEC
Copy number and genotype annotation from whole genome and whole exome sequencing data.







2.安装准备
(1)安装必须准备
http://boevalab.com/FREEC/tutorial.html#install

10Gb of RAM
Linux or MACS OS; only older versions of Control-FREEC (up to v6.2) are available for Windows
R installed
	R version 3.5.1
samtools installed if the input files are in .BAM format
	Version: 1.9 (using htslib 1.9)
bedtools installed if you wish to create minipileup files from .BAM
	bedtools v2.25.0
sambamba installed if you wish to speed up reading of .BAM files
	sambamba 0.6.8 by Artem Tarasov and Pjotr Prins (C) 2012-2018



(2)##https://github.com/BoevaLab/FREEC/releases
$ wget -O FREEC-11.5.tar.gz https://github.com/BoevaLab/FREEC/archive/v11.5.tar.gz
$ tar xzvf FREEC-11.5.tar.gz
$ cd FREEC-11.5
在FREEC-11.4下有3个目录：
data目录保存的是配置文件的模板，包含WGS和WES两套模板；
scripts目录下是一些常用的脚本；
src目录下就是软件的源代码，freec可执行文件就位于这个目录。

$ cd src
$ make
$ make install #报错，可能必须手动安装快捷方式了

$ cd ~/bin
$ ln -s ~/Soft/FREEC-11.5/src/freec
$ freec -h
Control-FREEC v11.5 : a method for automatic detection of copy number alterations, subclones and for accurate estimation of contamination and main ploidy using deep-sequencing data





文件配置方式：http://boevalab.com/FREEC/tutorial.html#CONFIG




http://boevalab.com/FREEC/
Starting from Control-FREEC v10.6, Control-FREEC can work on exome-seq data without a control sample. 






3.Control-FREEC v11.5
$ pwd
/home/wangjl/bin
$ ln -s /home/wangjl/Soft/FREEC-11.5/src/freec



3.配置文件很复杂
http://boevalab.com/FREEC/
http://boevalab.com/FREEC/tutorial.html#CONFIG

Starting from Control-FREEC v10.6, Control-FREEC can work on exome-seq data without a control sample. 




========================================
变异注释-基因频率数据库
----------------------------------------
如何挖掘外显子变异频率信息： https://www.jianshu.com/p/4278896661f2
http://www.omicsclass.com/article/463

千人基因组计划 http://www.internationalgenome.org/category/population/



1.为达到变异筛选目的，我们一般会在几个大型的外显子变异数据库中对新发现的突变进行注释，了解其突变频率等情况。常用的数据库有dbSNP、Hapmap、COSMIC、1000Genomes projects千人基因组计划（根据人种来源，分为全部人种、东亚人、美洲人等不同子数据库）、ESP6500外显子计划、ExAC（根据人种来源，分为全部人种、东亚人等不同子数据库）。

Population Code	Population Description	Super Population Code
CHB	Han Chinese in Beijing, China	EAS
CDX	Chinese Dai in Xishuangbanna, China	EAS


2.目标变异筛选（基于变异频率）
结合以上数据库，通过特定的阈值筛选，我们可以过滤很多无效变异。例如，可以过滤千人基因组数据库中频率大于0.01变异位点，以得到真正可能致病的罕见突变（rare）。也可以联合多个数据库对突变频率进行过滤，或者同时参考dbSNP中记录的SNP信息，初步判断数据库中不存在的变异为新发现变异，以增加研究价值。

不过值得注意的是，在dbSNP中没有记录的变异，有可能是新变异，也有可能是旧的符合条件的变异，更有可能是测序错误。因此在判断某一变异的价值的时候，需要结合其位置信息以及蛋白突变有害性等信息进行判断。













========================================
变异功能注释 ANNOVAR
----------------------------------------

将原始fq文件通过FastQC-align-samtools||GATK等流程最终得到vcf文件，也就是记录某些位点变异的文本文件。但只是通过看vcf文件我们是不知道些变异位点到底是位于基因的exon、intron、UTR等的哪些区域的。所以我们需要对vcf文件也就是这些变异位点进行注释。最常用的vcf注释软件有annovar和snpEff。




1. ANNOVAR简介(邮件注册才能下载)
ANNOVAR是由王凯编写的一个注释软件，可以对SNP和indel进行注释，也可以进行变异的过滤筛选。
官网 http://annovar.openbioinformatics.org/en/latest/

ANNOVAR能够利用最新的数据来分析各种基因组中的遗传变异。主要包含三种不同的注释方法，Gene-based Annotation（基于基因的注释）、Region-based Annotation（基于区域的注释）、Filter-based Annotation（基于筛选的注释）。

ANNOVAR由Perl编写。
优点：提供多个数据可直接下载、支持多种格式、注释直观；
缺点：没有数据库的物种无法注释。


软件新闻请关注谷歌小组： 
https://groups.google.com/forum/#!forum/annovar





2.安装 https://www.jianshu.com/p/b2b70202d7f2
$ tar zxvf annovar.latest.tar.gz ##解压 
解压后生成annovar文件夹，里面有6个perl脚本程序和两个文件夹，其中一个是example文件夹，另一个是已经建立好的hg19或者GRCh37的humandb的数据库文件夹，可用于人的注释。

加入到path中
$ vim ~/.bashrc 
##末尾加入
export PATH=/home/wangjl/Soft/annovar/:$PATH 

$ source ~/.bashrc
then typing annotate_variation.pl would be okay instead of typing perl annotate_variation.pl). 


First, we need to download appropriate database files using annotate_variation.pl, 
and next we will run the table_annovar.pl program to annotate the variants in the example/ex1.avinput file.





3.使用Annovar建库
Function: annotate a list of genetic variants against genome annotation databases stored at local disk. 
人的注释方法，官网介绍的很详细，但仅仅有人的数据库肯定是满足不了大家的需求。
下面以小鼠GRCh38为例子，介绍如何自己构建一个 humandb38 数据库。

先在annovar文件夹里面创建 humandb38 文件夹（名字可自取），命令
$ cd /home/wangjl/Soft/annovar/
$ mkdir humandb38

然后一个一个执行命令：使用annovar文件夹下的perl程序annotate_variation.pl
(1)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar refGene humandb38/
这个命令是下载 hg38 的refGene的文件，保存在 humandb38 文件下，自动解压后文件名为 hg38_refGene.txt。
total 235M
-rw-rw-r--. 1 wangjl wangjl  993 Nov 13 16:17 annovar_downdb.log
-rw-rw-r--. 1 wangjl wangjl 216M Jun  2  2017 hg38_refGeneMrna.fa
-rw-rw-r--. 1 wangjl wangjl  19M Jun  2  2017 hg38_refGene.txt
-rw-rw-r--. 1 wangjl wangjl 810K Jun  2  2017 hg38_refGeneVersion.txt

(2)$ annotate_variation.pl -buildver hg38 -downdb cytoBand humandb38/
-rw-rw-r--. 1 wangjl wangjl  45K Aug 11  2014 hg38_cytoBand.txt

(3)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar exac03 humandb38/ 
-rw-rw-r--. 1 wangjl wangjl  23M Nov 30  2015 hg38_exac03.txt.idx
-rw-rw-r--. 1 wangjl wangjl 600M Nov 30  2015 hg38_exac03.txt

(4)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar avsnp147 humandb38/
-rw-rw-r--. 1 wangjl wangjl 5.9G Jun  2  2016 hg38_avsnp147.txt
-rw-rw-r--. 1 wangjl wangjl 884M Jun  2  2016 hg38_avsnp147.txt.idx

(5)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar dbnsfp30a humandb38/
-rw-rw-r--. 1 wangjl wangjl  14G Oct 16  2015 hg38_dbnsfp30a.txt
-rw-rw-r--. 1 wangjl wangjl  18M Nov 17  2015 hg38_dbnsfp30a.txt.idx
前五个命令下载一堆注释文件到 humandb38/。



## --downdb                    download annotation database 
## --webfrom <string>          specify the source of database (ucsc or annovar or URL) (downdb operation)  

## --thread <int>              use multiple threads for filter-based annotation
# --maxgenethread <int>       max number of threads for gene-based annotation (default: 6)



#### 尝试下载其他数据库？官方下载数据库
http://annovar.openbioinformatics.org/en/latest/user-guide/download/

$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar avdblist humandb38/
则在 humandb38 目录下下载一个文本文件，包含现有的 hg38 的数据库92行。


该选择哪些数据库？
https://doc-openbio.readthedocs.io/projects/annovar/en/latest/user-guide/filter/#summary-of-databases


vcf 文件的说明和什么是 Left-normalization ：
http://annovar.openbioinformatics.org/en/latest/articles/VCF/



(7)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar 1000g2015aug humandb38/
-rw-r--r--. 1 wangjl wangjl 3.2G Aug 26  2015 hg38_ALL.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl 1.5G Aug 26  2015 hg38_AFR.sites.2015_08.txt
分为sas 南亚 ，eas 东亚 等好几个文件。 

https://www.jianshu.com/p/4278896661f2
千人基因组计划（http://www.internationalgenome.org/）。通过注释，可以知道该变异在全部参与千人基因组计划人群中的突变频率，参与人群来自于非洲AFR（African），美洲AMR（Admixed American），东亚EAS（EastAsian），欧洲EUR（European），南亚SAS（South Asian）等区域。



(8)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar avsnp150 humandb38/
## hg38	avsnp150	dbSNP150 with allelic splitting and left-normalization	20170929
-rw-rw-r--. 1 wangjl wangjl  13G Sep 30  2017 hg38_avsnp150.txt
-rw-rw-r--. 1 wangjl wangjl 918M Sep 30  2017 hg38_avsnp150.txt.idx


(9)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar clinvar_20180603 humandb38/
CLINVAR database with Variant Clinical Significance (unknown, untested, non-pathogenic, probable-non-pathogenic, probable-pathogenic, pathogenic, drug-response, histocompatibility, other) and Variant disease name
Clinvar version 20180603 with separate columns (CLNALLELEID CLNDN CLNDISDB CLNREVSTAT CLNSIG)
hg38	clinvar_20180603	same as above	20180708

-rw-rw-r--. 1 wangjl wangjl  72M Jul  9 11:50 hg38_clinvar_20180603.txt
-rw-rw-r--. 1 wangjl wangjl 1.3M Jul  9 11:50 hg38_clinvar_20180603.txt.idx


(10)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar cosmic70 humandb38/

COSMIC database version 70? on WGS data
hg38	cosmic70	same as above	20150428

-rw-rw-r--. 1 wangjl wangjl  87M Apr 29  2015 hg38_cosmic70.txt
-rw-rw-r--. 1 wangjl wangjl 6.6M Apr 29  2015 hg38_cosmic70.txt.idx



(11)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar esp6500siv2_all humandb38/
alternative allele frequency in All subjects in the NHLBI-ESP project with 6500 exomes, including the indel calls and the chrY calls. This is lifted over from hg19 by myself.
-rw-rw-r--. 1 wangjl wangjl  88M Dec 23  2014 hg38_esp6500siv2_all.txt


(12)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar ljb26_all humandb38/
ljb26_all: whole-exome SIFT, PolyPhen2 HDIV, PolyPhen2 HVAR, LRT, MutationTaster, MutationAssessor, FATHMM, MetaSVM, MetaLR, VEST, CADD, GERP++, PhyloP and SiPhy scores from dbNSFP version 2.6

-rw-rw-r--. 1 wangjl wangjl  11G May 21  2015 hg38_ljb26_all.txt








4.使用 table_annovar 注释 

(1) 官方句子
$ table_annovar.pl example/ex1.avinput humandb38/ -buildver hg38 -out myanno -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation gx,r,f,f,f -nastring . -csvout -polish --xreffile example/gene_xref.txt #最后用 --xreffile 和 -xref 有啥区别？ 

该一个命令使用 table_annovar ，使用 ExAC version 0.3 (referred to as exac03) dbNFSP version 3.0a (referred to as dbnsfp30a), dbSNP version 147 with left-normalization (referred to as avsnp147) databases 并删除临时文件, 生成的输出文件为 myanno.hg19_multianno.txt.
 
没有注释信息的用点号.填充。
The header line starts with #. The cross-reference file then contains 15 types of annotations for genes. 

You can run the same command above but change -xref file from gene_xref.txt to gene_fullxref.txt, and the result file can be downloaded from here. 

支持直接输入vcf文件 table_annovar.pl can directly support input and output of VCF files (the annotation will be written to the INFO field of the output VCF file). Let's try this:
$ table_annovar.pl example/ex2.vcf humandb38/ -buildver hg38 -out myanno3 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput

Similarly, if you generated variant calls from human GRCh38 coordinate, add -buildver hg38 in every command, 


(2)公众号见到的，使用了更多的库
table_annovar.pl example/ex1.avinput humandb/ -buildver hg19 -out myanno -remove -protocol refGene,cytoBand,genomicSuperDups,esp6500siv2_all,1000g2014oct_all,1000g2014oct_afr,1000g2014oct_eas,1000g2014oct_eur,snp138,ljb26_all -operation g,r,r,f,f,f,f,f,f,f -nastring . -csvout

# -buildver hg38 表示使用hg38版本
# -out myanno 表示输出文件的前缀为myanno
# -remove 表示删除注释过程中的临时文件
# -protocol 表示注释使用的数据库，用逗号隔开，且要注意顺序
# -operation 表示对应顺序的数据库的类型（g代表gene-based、r代表region-based、f代表filter-based），用逗号隔开，注意顺序
# -nastring . 表示用点号替代缺省的值
# -csvout 表示最后输出.csv文件

# --polish   polish the protein notation for indels (such as p.G12Vfs*2)
# --xreffile <file>     specify a cross-reference file for gene-based annotation


(3)诺禾致源给的csv报告中的频率
#[25] "X1000g2015aug_Chinese"(x) "X1000g2015aug_eas"     "X1000g2015aug_all"    
#[28] "esp6500siv2_all"       "ExAC_ALL"(x)              "ExAC_EAS"(x) 

-rw-r--r--. 1 wangjl wangjl 3.2G Aug 26  2015 hg38_ALL.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl 856M Aug 26  2015 hg38_EAS.sites.2015_08.txt
-rw-rw-r--. 1 wangjl wangjl  88M Dec 23  2014 hg38_esp6500siv2_all.txt
-rw-rw-r--. 1 wangjl wangjl 600M Nov 30  2015 hg38_exac03.txt

$ grep 1000g2015 humandb38/hg38_avdblist.txt
## hg38_1000g2015aug.zip   20150826        2732158830

$ grep ljb26 humandb38/hg38_avdblist.txt
## hg38_ljb26_all.txt.gz   20150520        2311601075


自己整合一个句子
报错1: Error in argument: -csvout is not compatible with -vcfinput， 其实还是希望能输出csv的。
$ time table_annovar.pl /home/data/example201710/wangjl_bwa/3_callSNP/1WES01_2_.filter.vcf humandb38/ -buildver hg38 -out 2WES01_ -remove -protocol refGene,cytoBand,esp6500siv2_all,ALL.sites.2015_08,EAS.sites.2015_08,exac03,avsnp147,avsnp150,dbnsfp30a,ljb26_all -operation g,r,f,f,f,f,f,f,f,f -nastring . -vcfinput -xreffile example/gene_fullxref.txt


报错2: annovar Argument "T" isn't numeric in numeric eq (==) at annotate_variation.pl line 2583, <DB> line 2169376.

Try using '-protocol 1000g2015aug_all' in your command, and let us know if that solves your problem or not.

<annotate_variation.pl -filter -dbtype ljb26_all -buildver hg38 -outfile 3WES01csv_ 2WES01_.avinput humandb38/ -otherinfo>








//todo
(1) ## 使用第一次执行的中间文件 37min.
$ time table_annovar.pl 2WES01_.avinput humandb38/ -buildver hg38 -out 3WES01csv_ -remove -protocol refGene,cytoBand,esp6500siv2_all,ALL.sites.2015_08,EAS.sites.2015_08,exac03,avsnp147,avsnp150,dbnsfp30a,ljb26_all -operation g,r,f,f,f,f,f,f,f,f -nastring . -csvout -xreffile example/gene_fullxref.txt



(2)先转化为.avinput
$ convert2annovar.pl -format vcf4 example/ex2.vcf > ex2wjl.avinput
# -format vcf4 指定格式为vcf

$ convert2annovar.pl -format vcf4 1WES01_2_.filter.vcf > 1WES01_2_.filter.avinput
转换失败，没报错，但是结果好像不像。

$ time table_annovar.pl /home/data/example201710/wangjl_bwa/3_callSNP/1WES01_2_.filter.avinput humandb38/ -buildver hg38 -out 4WES01aviinput_ -remove -protocol refGene,cytoBand,esp6500siv2_all,ALL.sites.2015_08,EAS.sites.2015_08,exac03,avsnp147,avsnp150,dbnsfp30a,ljb26_all -operation g,r,f,f,f,f,f,f,f,f -nastring . -csvout -xreffile example/gene_fullxref.txt

















5.Annovar 软件注释流程介绍
https://www.cnblogs.com/zkkaka/p/6146137.html




能用的
$ table_annovar.pl example/ex2.vcf humandb38/ -buildver hg38 -out my3 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput


正常样本
$ table_annovar.pl /home/data/example201710/wangjl_bwa/2_mapping/test.vcf humandb38/ -buildver hg38 -out test1 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput

过滤后的数据样本
$ table_annovar.pl /home/data/example201710/wangjl_bwa/3_callSNP/test.vcf humandb38/ -buildver hg38 -out testF -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput


过滤后的数据 全部
$ time table_annovar.pl /home/data/example201710/wangjl_bwa/3_callSNP/1WES01_2_.filter.vcf humandb38/ -buildver hg38 -out 1WES01_2 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput










#####
$ sed "s/^chr//" test.csv > test2.csv

$ table_annovar.pl /home/data/example201710/wangjl_bwa/2_mapping/test2.vcf humandb38/ -buildver hg38 -out WEStest2_ -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation gx,r,f,f,f -nastring . -csvout -polish -xref example/gene_xref.txt




annotate_variation.pl -geneanno -dbtype refGene -buildver hg38  -out test01_ example/ex1.avinput humandb38/

annotate_variation.pl -buildver hg38 ex1.avinput humandb38/






========================================
gatk--Variant Discovery in High-Throughput Sequencing Data
----------------------------------------
参考全文 https://www.cnblogs.com/nkwy2012/p/6322775.html

1.官网： https://software.broadinstitute.org/gatk/
Genome Analysis Toolkit
Variant Discovery in High-Throughput Sequencing Data


(1)如何安装：https://www.jianshu.com/p/825e7d618838
$ wget https://github.com/broadinstitute/gatk/releases/download/4.0.11.0/gatk-4.0.11.0.zip
$ unzip gatk-4.0.11.0.zip
$ tree -L 1 gatk-4.0.11.0
gatk-4.0.11.0
├── gatk
├── gatk-completion.sh
├── gatkcondaenv.intel.yml
├── gatkcondaenv.yml
├── GATKConfig.EXAMPLE.properties
├── gatkdoc
├── gatk-package-4.0.11.0-local.jar
├── gatk-package-4.0.11.0-spark.jar
├── gatkPythonPackageArchive.zip
├── README.md
└── scripts

解压缩之后，可以看到两个后缀为.jar的文件，local用于本地运行，spark用于在spark集群上运行。实际使用时，直接用gatk这个可执行文件就行了。

建立软链接
$ cd /home/wangjl/bin
$ ln -s ~/Soft/gatk-4.0.11.0/gatk   

通过一个简单的命令，查看程序是否正确安装
$ gatk --list
这个命令能够打印出所有的子命令(估计有几百个)，如果打印出来结果，说明程序安装正确。部分子命令截图如下
USAGE:  <program name> [-h]

Available Programs:
----- ----- ----- ----- ----- ----- ----- ----- ----- 
Base Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters
    CheckIlluminaDirectory (Picard)              Asserts the validity for specified Illumina basecalling data.
    CollectIlluminaBasecallingMetrics (Picard)   Collects Illumina Basecalling metrics for a sequencing run.
    CollectIlluminaLaneMetrics (Picard)          Collects Illumina lane metrics for the given BaseCalling analysis directory.
    ExtractIlluminaBarcodes (Picard)             Tool determines the barcode for each read in an Illumina lane.
    CollectIlluminaLaneMetrics (Picard)          Collects Illumina lane metrics for the given BaseCalling analysis directory.                                                                                                                             [252/1916]
    ExtractIlluminaBarcodes (Picard)             Tool determines the barcode for each read in an Illumina lane.
    IlluminaBasecallsToFastq (Picard)            Generate FASTQ file(s) from Illumina basecall read data.
    IlluminaBasecallsToSam (Picard)              Transforms raw Illumina sequencing data into an unmapped SAM or BAM file.
    MarkIlluminaAdapters (Picard)                Reads a SAM or BAM file and rewrites it with new adapter-trimming tags.

----- ----- ----- ----- ----- ----- ----- ----- ----- 
Copy Number Variant Discovery:                   Tools that analyze read coverage to detect copy number variants.
    AnnotateIntervals                            (BETA Tool) Annotates intervals with GC content, mappability, and segmental-duplication content
    CallCopyRatioSegments                        (BETA Tool) Calls copy-ratio segments as amplified, deleted, or copy-number neutral
    CollectAllelicCountsSpark                    Collects ref/alt counts at sites.
    CombineSegmentBreakpoints                    (EXPERIMENTAL Tool) Combine the breakpoints of two segment files and annotate the resulting intervals with chosen columns from each file.
    CreateReadCountPanelOfNormals                (BETA Tool) Creates a panel of normals for read-count denoising
    DenoiseReadCounts                            (BETA Tool) Denoises read counts to produce denoised copy ratios
    DetermineGermlineContigPloidy                (BETA Tool) Determines the baseline contig ploidy for germline samples given counts data
    FilterIntervals                              (BETA Tool) Filters intervals based on annotations and/or count statistics
...

子命令后面如果有(picard), 说明这个功能是继承于picard软件，从这里也可以看出，GATK4集成了picard软件的功能。再不需要像之前版本一样，混合使用picard 和 gatk 了。

GATK4 的最佳实践给出了5套pipeline
1)Germline SNPs + Indels
2)Somatic SNVs + Indels
3)RNAseq SNPs + Indels
4)Germline CNVs
5)Somatic CNVs

以上五套pipeline 可以根据研究对象是DNA还是RNA进行划分：DNA 测序（包含1,2,4,5）和RNA 测序（3）。可以看到，GATK 更多的是倾向于DNA 测序数据的分析。对于DNA测序而言，主要识别SNP和CNV 两大类型的变异，每种变异类型又有Germline和Somatic的区别。

Germline指的是在胚胎发育早起出现的变异，这种变异会在所有细胞中广泛存在，是可以遗传给后代的变异；Somatic指的是体细胞变异，身体特定区域或者组织中出现的变异。通常不会遗传给后代。

在所有的pipeline之前，都存在一个数据预处理步骤data pre-processing。
GATK4 版本的最佳实践并不是直接给出了每个步骤对应的代码，而是给出了几套它们自己编写的流程，以供参考。这些流程以WDL这种workflow 语言进行编写。官方对于WDL, 也给出了详细的文档，帮助我们了解。

总结
GATK4整合了picard软件，在算法上进行了优化，新增了许多新的功能。
官网给出了基于GATK4的pipeline, 以WDL这种workflow 流程管理语言编写。



(2)下载参考数据
https://gatkforums.broadinstitute.org/gatk/discussion/1215/how-can-i-access-the-gsa-public-ftp-server


GATK在进行BQSR和VQSR的过程中会使用到R软件绘制一些图，因此，在运行GATK之前最好先检查一下是否正确安装了R和所需要的包，所需要的包大概包括ggplot2、gplots、bitops、caTools、colorspace、gdata、gsalib、reshape、RColorBrewer等。如果画图时出现错误，会提示需要安装的包的名称。



用浏览器登陆 ftp://ftp.broadinstitute.org/bundle/
annovar/humandb folder will have its own dbnsfp file eg hg38_dbnsfp*.txt.



2.GATK Best Practices
https://software.broadinstitute.org/gatk/best-practices/



3.Quick Start Guide:Take a brief orientation tour and get started today
https://software.broadinstitute.org/gatk/documentation/quickstart.php


========================================
gatk4.0 实践 (hg19) 当前主流
----------------------------------------
主要参考： https://www.plob.org/article/11698.html
	https://cloud.tencent.com/developer/news/180158
其他参考： http://starsyi.github.io/2016/05/25/%E5%8F%98%E5%BC%82%E6%A3%80%E6%B5%8B%EF%BC%88BWA-SAMtools-picard-GATK%EF%BC%89/


############
[hg19版本]对于外显子测序分析，下载hg19数据吧，GATK官方建议
############


0.准备数据

##0.1 下载参考数据
For Best Practices short variant discovery in exome and other targeted sequencing: b37/hg19
(https://software.broadinstitute.org/gatk/download/bundle 2018.12.5)

链接ftp后下载：
location: ftp.broadinstitute.org/bundle
username: gsapubftp-anonymous
password:


输入如下命令： $ lftp ftp.broadinstitute.org -u gsapubftp-anonymous
回车后键入空格，便可进入resource bundle。进入其中名为bundle的目录，找到最新版的hg19目录。


$ cd /home/data/GATK/resources/bundle/hg19
## ftp://ftp.broadinstitute.org/bundle/hg19/ #11:12
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.gz &
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.dict.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.fai.gz & 

wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/dbsnp_138.hg19.vcf.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/dbsnp_138.hg19.vcf.idx.gz & 

wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx.gz & 

wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/1000G_phase1.snps.high_confidence.hg19.sites.vcf.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/1000G_phase1.snps.high_confidence.hg19.sites.vcf.idx.gz & 

wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/hapmap_3.3.hg19.sites.vcf.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/hapmap_3.3.hg19.sites.vcf.idx.gz & 

注意下载文件均为压缩文件，需要解压才能使用。
解压：
$ gunzip ucsc.hg19.fasta.gz
$ gunzip *gz

##0.2 建立bwa索引文件
$ mkdir bwa_index 
$ cd bwa_index
$ 
$ bwa index -a bwtsw \
	-p gatk_hg19 ../ucsc.hg19.fasta \
	1>hg19.bwa_index.log 2>&1 &
## 耗时 main] Real time: 4507.269 sec; CPU: 4451.448 sec








1.开始比对：map to reference[工具:BWA, MergeBamAlignments]
$ pwd
/home/data/ex23/wangjl_bwa

##1.1设定参考基因组，样本名
$ INDEX="/home/data/GATK/resources/bundle/hg19/bwa_index/gatk_hg19"
$ sample="EX23"


#使用raw reads进行mapping，由于该样本测了2个lane（每个R1、R2两个文件，共4个文件），每个lane分别map，最后合并sam文件。
$ bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L1_1.fq.gz ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L1_2.fq.gz >${sample}_L1.sam
$ bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L2_1.fq.gz ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L2_2.fq.gz >${sample}_L2.sam

也可以先合并文件，https://sourceforge.net/p/bio-bwa/mailman/message/31053122/
示例：bwa mem '<zcat R1_001.fastq.gz R1_002.fastq.gz R1_003.fastq.gz' '<zcat R2_001.fastq.gz R2_002.fastq.gz R2_003.fastq.gz' > out.sam
$ bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX '<zcat ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L1_1.fq.gz ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L2_1.fq.gz' '<zcat ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L1_2.fq.gz ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L2_2.fq.gz' >${sample}.sam
[main] Real time: 4452.334 sec; CPU: 26123.824 sec





2.标记重复：Mark Duplicates[工具: MarkDuplicates, SortSam]

#2.1sort by coordinate, 对sam文件进行进行重新排序（reorder）
$ gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" SortSam -SO coordinate -I $sample.sam -O $sample.bam 1>gatk_SortSam.log 2>&1 &
#生成 EX23.bam #这个新生成的bam文件真小，只有16%(6.9G Bam/41G Sam)
## real    81m29.543s


#2.2 统计比对情况(可选)
做到这一步需要对序列比对情况进行统计，如果比对情况很差需要查找原因。
## sudo yum install gnuplot #需要依赖画图

samtools index -@ 6 $sample.bam  #-@ 6 线程数

samtools flagstat -@ 6 $sample.bam > ${sample}.alignment.flagstat 
samtools stats -@ 6 $sample.bam > ${sample}.alignment.stat
plot-bamstats -p ${sample}_QC ${sample}.alignment.stat #好几个图


#2.3 标记重复，主要是去掉PCR带来的重复。
nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" MarkDuplicates -I $sample.bam -O ${sample}_marked.bam -M $sample.metrics 1>log.mark 2>&1 &
##11:19->11:38

查看被标记重复的reads数。网站上sam文件flag的解释：1024表示是PCR重复Reads。
$ samtools view -f 1024 EX23_marked.bam|wc
17753967 320890672 8235579827

为wes.sorted.MarkDuplicates.bam创建索引文件，它的作用能够让我们可以随机访问这个文件中的任意位置，而且后面的步骤也要求这个BAM文件一定要有索引.
samtools index -@ 6 ${sample}_marked.bam  #-@ 6 线程数


//todo 
#步骤FixMateInformation有必要吗？貌似不必要了。
nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" FixMateInformation -I ${sample}_marked.bam -O ${sample}_marked_fixed.bam -SO coordinate 1>log.fix 2>&1 &




#2.4 Base (Quality Score) Recalibration[工具: BaseRecalibrator, Apply Recalibration, AnalyzeCovariates (optional)]
GATK4在核心算法层面并没太多的修改，但参数设置还是有些改变的，并且取消了RealignerTargetCreator、IndelRealigner，应该是HaplotypeCaller继承了这部分功能。


局部比对 Local relignment(可选)
(https://gatkforums.broadinstitute.org/gatk/discussion/1247/what-should-i-use-as-known-variantssites-for-running-tool-x)
It is recommended on GATK website that indelrealigner step could safely be ignored if haplotype caller is to be used for variant calling. 
也就是说接下来一步中用haplotype caller的话，就不需要再用indelrealigner步骤了。

1000G_phase1.indels.b37.vcf和Mills_and_1000G_gold_standard.indels.b37.vcf 这两个文件来自千人基因组和Mills项目，里面记录了那些项目中检测到的人群Indel区域。





3.变异检测：使用GATK的HaplotypeCaller命令
HaplotypeCaller: Call germline SNPs and indels via local re-assembly of haplotypes

The HaplotypeCaller is capable of calling SNPs and indels simultaneously via local de-novo assembly of haplotypes in an active region.


HaplotypeCaller和那些直接应用贝叶斯推断的算法有所不同，它会先推断群体的单倍体组合情况，计算各个组合的几率，然后根据这些信息再反推每个样本的基因型组合。因此它不但特别适合应用到群体的变异检测中，而且还能够依据群体的信息更好地计算每个个体的变异数据和它们的基因型组合。

一般来说，在实际的WGS流程中对HaplotypeCaller的应用有两种做法，差别只在于要不要在中间生成一个gVCF：

（1）直接进行HaplotypeCaller，这适合于单样本，或者那种固定样本数量的情况，也就是执行一次HaplotypeCaller之后就老死不相往来了。否则你会碰到仅仅只是增加一个样本就得重新运行这个HaplotypeCaller的坑爹情况（即，N+1难题），而这个时候算法需要重新去读取所有人的BAM文件，这将会是一个很费时间的痛苦过程；

（2）每个样本先各自生成gVCF，然后再进行群体joint-genotype。这其实就是GATK团队为了解决（1）中的N+1难题而设计出来的模式。gVCF全称是genome VCF，是每个样本用于变异检测的中间文件，格式类似于VCF，它把joint-genotype过程中所需的所有信息都记录在这里面，文件无论是大小还是数据量都远远小于原来的BAM文件。这样一旦新增加样本也不需要再重新去读取所有人的BAM文件了，只需为新样本生成一份gVCF，然后重新执行这个joint-genotype就行了。


本例就一个病人样本，就使用单样本模式。

## 首先设置好软件地址
GENOME=/home/data/GATK/resources/bundle/hg19/ucsc.hg19.fasta
DBSNP=/home/data/GATK/resources/bundle/hg19/dbsnp_138.hg19.vcf

nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" HaplotypeCaller \
	-R $GENOME -I ${sample}_marked.bam --dbsnp $DBSNP -O ${sample}_raw.vcf 1>log.HC 2>&1 &
##18:50-2:39 Elapsed time: 468.53 minutes(7.8h).
Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -Djava.io.tmpdir=./ -jar /home/wangjl/Soft/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar HaplotypeCaller -R /home/data/GATK/resources/bundle/hg19/ucsc.hg19.fasta -I EX23_marked.bam --dbsnp /home/data/GATK/resources/bundle/hg19/dbsnp_138.hg19.vcf -O EX23_raw.vcf


最后得到的vcf文件如下：
$ ls -lth
total 55G
-rw-rw-r--. 1 wangjl wangjl 278K Dec  8 02:39 log.HC
-rw-rw-r--. 1 wangjl wangjl 5.7M Dec  8 02:39 EX23_raw.vcf.idx
-rw-rw-r--. 1 wangjl wangjl 116M Dec  8 02:39 EX23_raw.vcf





4.变异质控和过滤
Filter Variants by Variant (Quality Score) Recalibration[工具: VariantRecalibrator, ApplyRecalibration]

(1)
https://cloud.tencent.com/developer/news/180158
https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145

高灵敏度来避免漏掉稀有突变，但是需要过滤来减低大量的假阳性。
已经建立的功率raw variant的方法是VQSR（variant quality score recalibration），用机器学习识别稀有位点，为其分配一个比QUAL靠谱的VQSLOD分。
然后就可以用这个分来过滤结果，产生一个符合我们期待质量水品的子集，平衡特异性和敏感性。

接下来的recalibration 算法需要高质量的变异集合来训练和作为真集合，很多物种没有该数据。
也需要很多数据来学习profiles of good vs. bad variants，所以对于一个或少量样本不适用，也不适用于on targeted sequencing data, on RNAseq, and on non-model organisms。
这样就要使用hard-filtering了。为具体的注释设置阈值，并平等的应用到所有的变异上。查看方法学文章和相关FAQ。

我们现在正在尝试基于神经网络的更强大、更灵活的方法，来替代VQSR。


(2)执行硬过滤
https://gatkforums.broadinstitute.org/gatk/discussion/39/variant-quality-score-recalibration-vqsr
要注意的是，VQSR只适用于samples数目大于30的exome sequencing或者whole genome sequenicing。如果sample数目不够或者capture region过小，请参考http://www.broadinstitute.org/gatk/guide/topic?name=best-practices中对于此类问题的解决方法建议。一般是使用VariantFiltration进行hard filter。

$ pwd
/home/data/ex23/wangjl_bwa/3_callSNP

(1)# 使用SelectVariants，选出SNP(time=1.2min)
time gatk SelectVariants \
    -select-type SNP \
    -V ../2_mapping/${sample}_raw.vcf \
    -O ${sample}.snp.vcf.gz

# 为SNP作硬过滤(time=1.26min)
time gatk VariantFiltration \
    -V ${sample}.snp.vcf.gz \
    --filter-expression "QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "PASS" \
    -O ${sample}.snp.filter.vcf.gz



(2)# 使用SelectVariants，选出Indel(time=1.06min)
time gatk SelectVariants \
    -select-type INDEL \
    -V ../2_mapping/${sample}_raw.vcf \
    -O ${sample}.indel.vcf.gz

# 为Indel作过滤(time=1.03min)
time gatk VariantFiltration \
    -V ${sample}.indel.vcf.gz \
    --filter-expression "QD < 2.0 || FS > 200.0 || SOR > 10.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "PASS" \
    -O ${sample}.indel.filter.vcf.gz



(3) # 重新合并过滤后的SNP和Indel
time gatk MergeVcfs \
    -I ${sample}.snp.filter.vcf.gz \
    -I ${sample}.indel.filter.vcf.gz \
    -O ${sample}.filter.vcf.gz

# 删除无用中间文件
## $ rm -f ${sample}.snp.vcf.gz* ${sample}.snp.filter.vcf.gz* ${sample}.indel.vcf.gz* ${sample}.indel.filter.vcf.gz*


(4) 查看结果文件
$ zcat EX23.filter.vcf.gz|grep -v "#"|head

$ zcat EX23.filter.vcf.gz|grep -v "#"|wc
627989 6279890 123423242





========================================
gatk4.0 实践 (hg38) 未来趋势
----------------------------------------
主要参考： http://www.bio-info-trainee.com/3144.html
其他参考： https://www.plob.org/article/7009.html
GATK4最佳实践-数据预处理篇 https://www.jianshu.com/p/e306dc2307e5

GATK4 最佳实践-生殖细胞突变的检测与识别 https://www.jianshu.com/p/6f3198b7a070

官方推的GATK4教程：https://drive.google.com/drive/folders/1U6Zm_tYn_3yeEgrD1bdxye4SXf5OseIt






0.准备数据
##0.1 下载参考数据
$ mkdir -p /home/data/GATK/resources/bundle/hg38
$ cd /home/data/GATK/resources/bundle/hg38
## ftp://ftp.broadinstitute.org/bundle/hg38/
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/dbsnp_146.hg38.vcf.gz 2>dbsnp_146.hg38.vcf.gz.log & 
#nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/dbsnp_146.hg38.vcf.gz.tbi & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz 2>Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.log & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi 2>Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi.log & 
#nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.gz & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.fai 2>Homo_sapiens_assembly38.fasta.fai.log & 
#nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz 2>1000G_phase1.snps.high_confidence.hg38.vcf.gz.log & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi 2>1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi.log & 

##0.2 建立bwa索引文件
$ mkdir bwa_index 
$ cd bwa_index
$ bwa index -a bwtsw \
	-p gatk_hg38 ../Homo_sapiens_assembly38.fasta \
	1>hg38.bwa_index.log 2>&1 &
##-p STR    prefix of the index [same as fasta name]
##[main] Real time: 4686.724 sec; CPU: 4628.838 sec









1.开始比对：map to reference

##1.1设定参考基因组，样本名
$ INDEX="/home/data/GATK/resources/bundle/hg38/bwa_index/gatk_hg38"
$ sample="WES01_2"

#1.2开始比对
#使用clean data进行mapping
## bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX ../1_cleanData/outN.1.fastq ../1_cleanData/outN.2.fastq >$sample.sam #WES01.sam


是否需要在第一步做fastqc和cutadapt过滤？
如果过滤，则不容易mark duplicate。
因为mark是比对5'和3'端，一样就认为重复。


#使用raw reads进行mapping
$ bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX ../0_fastqc/B1709060700_NDHE00383-A84-A68_AH7277CCXY_L7_1.fq ../0_fastqc/B1709060700_NDHE00383-A84-A68_AH7277CCXY_L7_2.fq >$sample.sam ##WES01_2.sam
[main] Real time: 3044.140 sec; CPU: 17879.608 sec









2.对bam进行clean：Mark Duplicates
#2.1sort by coordinate
$ gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" SortSam -SO coordinate -I $sample.sam -O $sample.bam 1>log.sort 2>&1 &
#生成 WES01_2.bam #这个新生成的bam文件真小，只有10%多点(4.1G Bam/34G Sam)


#2.2 索引和统计
$ sudo yum install gnuplot #需要依赖画图

samtools index $sample.bam
# -@ 6 线程数
samtools flagstat -@ 6 $sample.bam > ${sample}.alignment.flagstat 
samtools stats -@ 6 $sample.bam > ${sample}.alignment.stat
echo plot-bamstats -p ${sample}_QC ${sample}.alignment.stat ##？没有看到图//todo


#2.3 标记重复
nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" MarkDuplicates -I $sample.bam -O ${sample}_marked.bam -M $sample.metrics 1>log.mark 2>&1 &
##11:19->11:38

nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" FixMateInformation -I ${sample}_marked.bam -O ${sample}_marked_fixed.bam -SO coordinate 1>log.fix 2>&1 &

samtools index ${sample}_marked_fixed.bam


为了节省磁盘空间，最后只需要留学一个bam文件即可.


3.Base(Quality Score) Recalibration(optional)
??




4.直接找变异：使用GATK的HaplotypeCaller命令

## 首先设置好软件地址
GENOME=/home/data/GATK/resources/bundle/hg38/Homo_sapiens_assembly38.fasta
DBSNP=/home/data/GATK/resources/bundle/hg38/dbsnp_146.hg38.vcf.gz

nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" HaplotypeCaller \
	-R $GENOME -I ${sample}_marked_fixed.bam --dbsnp $DBSNP -O ${sample}_raw.vcf 1>log.HC 2>&1 &
##14:03->
## Elapsed time: 492.58 minutes.(8Hours)

Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -Djava.io.tmpdir=./ -jar /home/wangjl/Soft/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar HaplotypeCaller -R /home/data/GATK/resources/bundle/hg38/Homo_sapiens_assembly38.fasta -I WES01_2_marked_fixed.bam --dbsnp /home/data/GATK/resources/bundle/hg38/dbsnp_146.hg38.vcf.gz -O WES01_2_raw.vcf


最后得到的vcf文件如下；
$ ls -lth
total 110G
-rw-rw-r--. 1 wangjl wangjl 323K Nov  9 22:15 log.HC
-rw-rw-r--. 1 wangjl wangjl 2.3M Nov  9 22:15 WES01_2_raw.vcf.idx
-rw-rw-r--. 1 wangjl wangjl 185M Nov  9 22:15 WES01_2_raw.vcf

这些VCF格式的变异记录该如何理解，以及GATK的每一个步骤是否仍然必须?





############################
对vcf文件的常用统计和处理
############################
1)Count variants:
$ grep -v "#" WES01_2_raw.vcf | wc

2)Extract sites located between positons 10000 and 20000 on chromosome chr2 and save them in a new VCF file:
$ head -1000 WES01_2_raw.vcf | grep "#" > new_file.vcf 
$ grep -v "#" WES01_2_raw.vcf | \
	awk '{if($1=="chr2" && $2 >=10000 && $2 <=20000) print}' >> new_file.vcf

3)Extract variants with quality (QUAL) greater than 100 (the resulting file will have no header!):
$ grep -v "#" WES01_2_raw.vcf | awk '{if($6>100) print}' > good_variants








5.获取的snp和indel文件，并使用硬过滤的策略
GATK4.0和全基因组数据分析实践（下）
原创： 矿工  碱基矿工  2018.3月23日

(1)
# 使用SelectVariants，选出SNP
time gatk SelectVariants \
    -select-type SNP \
    -V ../2_mapping/WES01_2_raw.vcf \
    -O WES01_2_.snp.vcf.gz

# 为SNP作硬过滤
time gatk VariantFiltration \
    -V WES01_2_.snp.vcf.gz \
    --filter-expression "QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "PASS" \
    -O WES01_2_.snp.filter.vcf.gz
	
(2)
# 使用SelectVariants，选出Indel
time gatk SelectVariants \
    -select-type INDEL \
    -V ../2_mapping/WES01_2_raw.vcf \
    -O WES01_2_.indel.vcf.gz

# 为Indel作过滤
time gatk VariantFiltration \
    -V WES01_2_.indel.vcf.gz \
    --filter-expression "QD < 2.0 || FS > 200.0 || SOR > 10.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "PASS" \
    -O WES01_2_.indel.filter.vcf.gz

(3)
# 重新合并过滤后的SNP和Indel
time gatk MergeVcfs \
    -I WES01_2_.snp.filter.vcf.gz \
    -I WES01_2_.indel.filter.vcf.gz \
    -O WES01_2_.filter.vcf.gz

# 删除无用中间文件
$ rm -f WES01_2_.snp.vcf.gz* WES01_2_.snp.filter.vcf.gz* WES01_2_.indel.vcf.gz* WES01_2_.indel.filter.vcf.gz*



https://biohpc.cornell.edu/lab/doc/Variant_workshop_Part1.pdf
https://biohpc.cornell.edu/lab/doc/Variant_workshop_Part2.pdf





========================================
Workflow Description Language (WDL)语言
----------------------------------------
1.简介
http://www.openwdl.org/

语法方便人读写的指定数据处理流程的方法。The Workflow Description Language (WDL) is a way to specify data processing workflows with a human-readable and -writeable syntax. 

直接定义任务，链接起来，并行处理。WDL makes it straightforward to define analysis tasks, chain them together in workflows, and parallelize their execution. 

The language makes common patterns simple to express, while also admitting uncommon or complicated behavior; and strives to achieve portability not only across execution platforms, but also different types of users. Whether one is an analyst, a programmer, an operator of a production system, or any other sort of user, WDL should be accessible and understandable.


2.Getting Started 快速入门教程
https://software.broadinstitute.org/wdl/documentation/quickstart

(1)基本结构 Base Structure
https://software.broadinstitute.org/wdl/documentation/structure.php
https://blog.csdn.net/theomarker/article/details/79627804

WDL有5个基本成分： workflow, task, call, command and output. 
没有明确的input定义组件；输入参数（指定参数，包括输入和输出文件名）。
还有些可选组件可以指定运行时参数（环境条件，比如Docker image），作者和邮件等meta信息，输入和输出的parameter_meta 描述——但现在我们已经不再担心这些了。

在一个小型的WDL脚本中，一个叫做 myWorkflowName 的workflow，和两个tasks(task_A和task_B)







(2)添加变量 Add Variables: 外部设定输入文件、参数
(3)添加管道 Add Plumbing: 把组件组合成管线pipeline
(4)验证语法 Validate Syntax: 防止少了一个分号之类的错误


3.Running a WDL
产生一个JSON模板，指定输入（很简单）。
执行WDL脚本的选项，我们用的引擎是Cromwell.

(1)Specify Inputs

(2)Execute!



4.When you're ready
(1) WDLTool: https://github.com/broadinstitute/wdltool/releases
wget https://github.com/broadinstitute/wdltool/releases/download/0.14/wdltool-0.14.jar
(2) Text editor:用的 vim
(3) Cromwell: 可以执行WDL，描述数据处理和分析流程，包含命令行工具(比如执行 Variant Discovery的GATK管线)
如果你熟悉GATK，可能听过用Qscripts写的叫做Queue的执行引擎。Cromwell and WDL 是 Queue and Qscripts的一个用户友好的替代.

https://github.com/broadinstitute/cromwell/releases
$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar
$ java -jar cromwell.jar #Cromwell是一个开源的由Java编写支持WDL的执行工具，需要java8环境。

下载jar包，并放到路径中。

Running WDL on Cromwell locally
$ java -jar Cromwell.jar run myWorkflow.wdl --inputs myWorkflow_inputs.json



(4) java8
https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html

$ java --version
openjdk 10.0.2 2018-07-17
OpenJDK Runtime Environment (build 10.0.2+13-Ubuntu-1ubuntu0.18.04.3)
OpenJDK 64-Bit Server VM (build 10.0.2+13-Ubuntu-1ubuntu0.18.04.3, mixed mode)

(5) Docker (optional)
https://docs.docker.com/install/


(6) Programs to be pipelined 需要流程化的程序 
我们的教程使用GATK和Picard工具展示如何写WDL脚本。首先安装这俩工具，按照GATK即可。
此外还推荐安装R包gsalib。







========================================
用vcftools来处理vcf文件（centOS6.8安装失败）
----------------------------------------
1.概述
https://vcftools.github.io/index.html
VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project. The aim of VCFtools is to provide easily accessible methods for working with complex genetic variation data in the form of VCF files.

http://www.1000genomes.org/ 改名为 http://www.internationalgenome.org/


This toolset can be used to perform the following operations on VCF files:

过滤 Filter out specific variants
比较 Compare files
汇总 Summarize variants
转换 Convert to different file types
验证合并 Validate and merge files
求交集 Create intersections and subsets of variants

VCFtools consists of two parts, a perl module and a binary executable. 
The perl module is a general Perl API for manipulating VCF files, whereas the binary executable provides general analysis routines.




2.下载与安装
https://vcftools.github.io/downloads.html

(1)用yum安装吧：
$ sudo yum install vcftools
$ vcftools
VCFtools (v0.1.11)

使用的时候报错
Error:VCF version must be v4.0 or v4.1:
You are using version VCFv4.2
说明必须安装新版本
sudo yum remove vcftools



(2)尝试安装
$ git clone https://github.com/vcftools/vcftools.git
需要环境中有perl
cd vcftools
./autogen.sh  报错1
$ sudo yum install install autoconf automake libtool #解决报错1

./configure
make 
make install #依旧报错


$ wget https://github.com/vcftools/vcftools/releases/download/v0.1.16/vcftools-0.1.16.tar.gz
$ tar zxvf vcftools-0.1.16.tar.gz
$ cd vcftools-0.1.16 
./configure
make ##报错 bgzf.h:33:10: fatal error: zlib.h: No such file or directory
make install


## 无效 download from : 
wget https://github.com/madler/zlib/blob/master/zlib.h
## put the file in the same folder as your project file. 放在平级目录中。



(3)重启，再次make时又有新的报错：
vcftools-bcf_file.o: In function `bcf_file::open_gz()':                                                                                  │
/home/wangjl/Soft/vcftools/src/cpp/bcf_file.cpp:88: undefined reference to `gzbuffer'
解决不了



(4)回到压缩包内
[wangjl@NGSZD vcftools-0.1.16]$ whereis zlib.h
zlib: /usr/include/zlib.h /usr/share/man/man3/zlib.3.gz


这里说 https://github.com/alexdobin/STAR/issues/142 可能是gcc找不到.h文件，
it looks like gcc cannot find zlib.h . Please check that /usr/include/ appears in the list of standard include directories with
$ gcc -xc -E -v -
$ gcc -xc++ -E -v -



看来是需要设置gcc的头文件自动搜索路径
【1】include头文件路径
除了默认的/usr/include, /usr/local/include等include路径外，还可以通过设置环境变量来添加系统include的路径：
# C
export C_INCLUDE_PATH=XXXX:$C_INCLUDE_PATH
# CPP
export CPLUS_INCLUDE_PATH=XXX:$CPLUS_INCLUDE_PATH
以上修改可以直接命令行输入（一次性），可以在/etc/profile中完成（对所有用户生效），也可以在用户home目录下的.bashrc或.bash_profile中添加（针对某个用户生效），修改完后重新登录即生效。
原文：https://blog.csdn.net/allenlinrui/article/details/21483617 



ghc-zlib-devel.x86_64 : Haskell compression and decompression library development files
zlib-devel.i686 : Header files and libraries for Zlib development
zlib-devel.x86_64 : Header files and libraries for Zlib development


结论：最终在centOS6.8上也没有安装成功新版本的vcftools，旧版本已经不支持该vcf 4.2标准。




(5)使用ubuntu1604安装试试(192.168.1.115)
很顺利的make通过了。
$ vcftools --version
VCFtools (0.1.16)






3.使用案例
bcftools或vcftools提取指定区段的vcf文件（extract specified position）
http://www.cnblogs.com/chenwenyan/p/9213394.html
https://www.biostars.org/p/162872/


如果只想提取指定位置（specific position）的基因型（genotypes），则可以用到vcftools工具

$ vcftools --gzvcf file.vcf.gz --positions specific_position.txt --recode --out specific_position.vcf

specific_position.txt的输入格式如下：
1 842013
1 891021
1 903426
1 949654
1 1018704




========================================
用bcftools来call variation
----------------------------------------

1.安装
Variant Call Format (VCF) and its binary counterpart BCF.
https://mp.weixin.qq.com/s?__biz=MzAxMDkxODM1Ng==&mid=2247483812&idx=1&sn=c3f7ce44a4f164f0d5549e11a22e702f&chksm=9b48411fac3fc8093925fbad417636544ea9af0c58ee9f4f31d1fa1fc2c80af610a835d09311&scene=21#wechat_redirect




http://samtools.github.io/bcftools/
## 下载安装
git clone git://github.com/samtools/htslib.git
git clone git://github.com/samtools/bcftools.git
cd bcftools
## The following is optional: 
## autoheader && autoconf && ./configure --enable-libgsl --enable-perl-filters
sudo make #第一次出错，用 make clean后再次sudo make通过了。
sudo make install 


$ bcftools
## Version: 1.9-67-g626e46b (using htslib 1.9-63-gd7922af) 


为了使用BCFtools plugins，需要设置环境变量：
export BCFTOOLS_PLUGINS=/path/to/bcftools/plugins




2.使用
bcftools — utilities for variant calling and manipulating VCFs and BCFs.
https://samtools.github.io/bcftools/bcftools.html

对排序好的bam数据用samtools生成bcf文件：
$ samtools mpileup -ugf ../hs38DH.fa hs2.sort.bam  >hs2.bcf

由于生成的是二进制格式的数据，需要进行解析或者转换成vcf：
$ bcftools view hs2.bcf >hs2.vcf


(2)bcftools合并vcf文件
$ bcftools merge A.vcf.gz B.vcf.gz C.vcf.gz -Oz -o ABC.vcf.gz

https://www.cnblogs.com/chenwenyan/p/9273726.html



========================================
Picard: 对sam文件进行进行重新排序（reorder）
----------------------------------------
1.官网： A set of command line tools (in Java) for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.

http://broadinstitute.github.io/picard/


2.下载
$ wget https://github.com/broadinstitute/picard/releases/download/2.18.15/picard.jar
$ vim ~/.bashrc #添加一行
	PICARD="/home/wangjl/Soft/picard.jar"
$ source ~/.bashrc


3.使用Picard排序：比对是按照字典顺序1,10,2,改为1,2,10

$ java -Xmx20g -jar $PICARD SortSam SORT_ORDER=coordinate INPUT=${WORKING_DIR}/2018rerun/processed_bam/${name}.sam OUTPUT=${WORKING_DIR}/2018rerun/processed_bam/${name}.bam




========================================
[大纲]第十八期临床遗传病生信实操培训班（上海班）
----------------------------------------
基因游侠  基因检测与解读  2018.10月15日


“在课上系统学习了质控的三大要点，拿到检测机构的原始数据后我就可以自己评估谁的实验做的好不好了，对于阴性结果的案例，我也学会在IGV中查看感兴趣的基因有没有完全覆盖，我还知道判断一个基因是否为患者的致病基因，需要从临床、遗传模式及生物信息三个方面考虑，缺一不可！”

1.原始数据fastq到bam到vcf
2.vcf与表型相关变异关系的分析过程。



内容：

基因侦探王剑
1.NGS实验基本流程
2.新致病基因的鉴定
3.NGS数据质量控制
4.致病基因（变异）的筛选与评估
5.变异精准解读影响因素
6.NGS分析CNV
7.案例分析


基因游侠
1、CentOS系统的介绍
2、实际操作fastQC查看数据质量
3、speedseq比对原始数据
4、如何分析覆盖度与测序深度
5、GATK call突变及位点过滤
6、annovar注释及筛选
7、IGV查看reads比对情况
8、怎么分析trio新发突变
9、怎么使用Exomiser分析复合杂合变异
10、怎么筛选大家系多个样本候选位点
11、以DMD为例，利用NGS分析外显子缺失与重复
12、XYAutoFilter软件的使用
13、利用WES分析SMN1拷贝数

14、cnv分析原理与案例



========================================
[大纲]遗传病测序数据分析解读科普系列 第2期 生信分析基础：了解数据处理过程
----------------------------------------
原创： 熊吉的熊  瀚垚生物医学  2018.8月2日

1.数据前处理
	fastq ->BQSR.bam

第一部分主要讲述从fastq到可以call变异的bam文件之间主要发生了什么。
重点理解，为什么比对目前最常用的是bwa，为什么要对bam进行sort，markdup和BQSR操作。

bwa优势：快和准

bam文件对很多人的印象可能是
第一步：打开igv
第二步：载入bam文件
第三步：输入看变异



2.如何得到变异vcf文件
	bam -> vcf

重点理解GATK的HaplotypeCaller都做了什么，为什么在众多的call variant软件中，HC使用范围最广。
适当了解变异后处理过程中推荐使用人群数据和VQSR的目的。



3.qc和统计
	fastq/bam/vcf的质量评估
通过质控或统计数据判断数据的质量

当拿到处理的数据后，第一时间其实并不是进行数据解读，其实首先需要检验数据的质量，即判断是否合格，否则对一个质量不达标的数据进行下游处理，很大可能会造成时间和精力的浪费。

主要理解fastq/bam/vcf的数据质量基本判断原则。


========================================
[大纲]逸仙生信临床解读培训班第二天
----------------------------------------
廖健伟  逸仙细胞分子论坛  2018.9月9日

2018年9月9日，逸仙细胞分子论坛的首届生物信息临床解读培训班迎来了第二天的课程

1.上午首先由颜彬博士介绍人群数据库和疾病数据库的使用。人群频率数据与疾病数据是胚系变异评级分类的重要证据，人群数据库可用于获得某变异在人群中的发生频率，而发生频率的高低可作为一个预测指标，区分良性变异和致病性变异；疾病数据库可用于查找疾病相关变异并对变异的致病性进行评估。颜博士重点介绍了1000 genomes及ExAC人群数据库和ClinVar、HGMD、OMIM等疾病数据库。

2.接着，温晓君老师介绍了参考基因组及计算机预测软件。参考基因组是人类基因组计划的产物，记录基因完整的信息包括编码序列、调控序列等，随着研究的进行，参考基因组也不断更新。温老师主要介绍了参考基因组NCBI、UCSC，ensemble等数据库在基因变异解读中重要性及参考基因组的下载方式，并结合ACMG指南，以变异解读实例详细地介绍了预测软件如错义变异的预测软件SIFT、Polyphen-2，剪切预测软件MaxExtScan、NetGen2等软件的应用。

3.然后，温晓君与廖健伟博士依次作了遗传病基因变异与肿瘤基因变异的解读与病例分享。遗传病非常复杂多样，根据其变异范围大小可分为单基因多基因疾病、基因组疾病、染色体疾病。温老师首先介绍了根据不同的疾病，检测方法及策略的不同，包括一代测序、靶向测序、医学外显子、全外显子、全基因组测序，其中，重点介绍了利用医学外显子测序检测单基因遗传病的适用范围及检测流程。最后她以两个病例详细介绍了医学外显子测序辅助诊断单基因遗传病的整个流程，包括检测前遗传咨询、实验流程及数据分析、变异评级。

然而，肿瘤基因检测与遗传疑难病有所区别，除了检测胚系变异，体细胞变异的检测同样重要。而对肿瘤组织的体细胞变异，目前有推荐以用药指导作为分类标准，目的主要是指导靶向治疗。廖博士介绍了药物相关数据库如oncoKB、My cancer Genome、Clinical Trials，可作为肿瘤基因变异分类的参照。


4.下午，先由欧阳能太教授分享了NGS实验室的设计经验，讲述了实验室的功能分区、气压、温湿度等对NGS检测的重要性，并就控制实验室压差和温湿度的暖通系统作了详细介绍。

5.接着，萧晓琴老师讲解了qPCR及一代测序在临床的应用。这两种方法各有优劣，获得的信息量也大不相同，根据临床的需求进行选择，qPCR法主要用于特定基因已知热点突变的检测；一代测序主要用于单个基因的测序及基因多态性分析；也常应用于二代测序得到的变异位点的验证及家属验证。

6.然后，李晓娟博士介绍了基因芯片及FISH技术的临床应用。临床检测染色体数量及结构，FISH和基因芯片是目前仍在使用的重要方法，并且各有优势。如果是针对已知的染色体数量和结构（3-20Mb）的检测，则FISH方法可以在显示受检细胞内部染色体变化，快速、方便。如果检测更微小的结构变异，则可通过基因芯片的方法对全基因组水平中发生的5kb以上的拷贝数扩增或缺失进行检测。但针对平衡易位还是需要传统的核型分析及FISH的方法来进行原位的观察。

7.随后，廖健伟博士负责解答临床基因检测的相关咨询问题。问题涉及基因检测项目的选择、发生退费的原因、检测部位选择、报告时效差别、基因检测报告的解析等。

8.最后，蒋圆玲老师为我们介绍了NIPT的检测原理及临床应用。染色体异常疾病是新生儿出生缺陷中常见的疾病，现阶段并没有有效的治疗方法，只能通过产前诊断进行出生干预才能有效地降低出生缺陷。讲者简单介绍了传统产前筛查和产前诊断方式；然后介绍了基于高通量测序的无创产前筛查（Noninvasive Prenatal Testing, NIPT）技术，并详细从技术的的发展起源、检测原理以及在临床中应用等方面进行了介绍。



========================================
[大纲]儿童遗传分子诊断技术质控及数据一体化信息管理——新华医院的经验探索尝试
----------------------------------------
聚道科技GeneDock  聚道科技GeneDock  2018.6月22日

2018分子及遗传诊断高峰论坛于6月14-15日在上海成功举办。上海交通大学医学院附属新华医院余永国主任为大家带来题为《复杂多样的儿童遗传分子诊断技术质控及数据一体化信息管理——新华医院的经验探索尝试》的精彩报告。

随着现代生物医学技术的发展，分子诊断是精准医学实践中不可或缺的一个重要手段。遗传病的三级预防措施能够有效降低出生缺陷：
一级预防：孕前禁止近亲结婚，杂合子检出
二级预防：怀孕期羊水细胞、绒毛膜细胞的酶学、基因产前诊断
三级预防：新生儿筛查


此外，儿童疑难罕见遗传病的常规基因诊断报告，经历从发起申请、进行实验、生信分析、到出遗传报告等过程，需要45-60天的周期。为遗传病患儿的救治赢得更多宝贵时间，缩短检测各个环节的时间尤为重要。过去，检测申请、分子实验过程记录、检测报告签发等信息记录主要通过从医院系统下载上传表格，通过EXCEL记录实现。这给项目沟通、数据流通、人员协作都带来了很大的不便。

分子检测需要不同团队、各个成员沟通协作，为了确保检测结果的准确性，新华医院采取三级审核的制度。比如基因诊断报告流程：
第一review：要求产生精准的数据并运行工作流
第二review：要求分析数据，双人核对，列出依据，讨论结果
第三review：医生审核位点，一代验证，撰写报告
实验室信息化平台的搭建对促进实验室信息同步、协作带来了很大的便利，能够提高协作效率，缩短检测周期。

1.分子平台临床信息和检测自动化搭建（LIMS系统）




========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



