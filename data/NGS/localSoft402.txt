本地生信软件
	- 本地运行的生信软件
	- 包括可以下载安装的tool/VM/docker等

生物信息学常见1000个软件的安装代码
http://www.360doc.com/content/17/1014/11/42030643_694826019.shtml


Hemberg-lab单细胞转录组数据分析（五）
http://www.360doc.com/content/19/0403/11/51784026_826127283.shtml




========================================
Galaxy生信分析平台-搭建（本地化）
----------------------------------------
1.简介
https://usegalaxy.org/

Galaxy is an open source, web-based platform for data intensive biomedical research. If you are new to Galaxy start here or consult our help resources. You can install your own Galaxy by following the tutorial and choose from thousands of tools from the Tool Shed.




2.在本地搭建
http://www.bioinfo-scrounger.com/archives/683








========================================
测序数据质控 fastqc 及报告解读
----------------------------------------
1.download and install(for centOS7 no root)
http://www.bioinformatics.babraham.ac.uk/projects/fastqc/
http://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqc

(1)
$ wget http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.7.zip
$ unzip fastqc_v0.11.7.zip
$ cd FastQC
#添加运行权限
$ sudo chmod 755 fastqc
#添加软连接
$ ln -s /home/wangjl/software/FastQC/fastqc /home/wangjl/bin/fastqc

$ fastqc -v
FastQC v0.11.7
OK now.

安装：http://www.bioinformatics.babraham.ac.uk/projects/fastqc/INSTALL.txt
README：http://www.bioinformatics.babraham.ac.uk/projects/fastqc/README.txt



(2) 更多选项
$ fastqc --help  帮助
FastQC - A high throughput sequence QC analysis tool

fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN

-o用来指定输出文件的所在目录，注意是不能自动新建目录的。
	输出的结果是.zip文件，默认自动解压缩，命令里加上--noextract则不解压缩。
	--outdir=/some/other/dir/ 指定输出文件位置
-f用来强制指定输入文件格式，默认会自动检测。
-c用来指定一个contaminant文件，fastqc会把overrepresented sequences往这个
contaminant文件里搜索。contaminant文件的格式是"Name\tSequences"，#开头的行是注释。
加上 -q 会进入沉默模式，即不出现下面的提示：
Started analysis of target.fq
Approx 5% complete for target.fq
Approx 10% complete for target.fq


如果输入的fastq文件名是target.fq，fastqc的输出的压缩文件将是target.fq_fastqc.zip。解压后，查看html格式的结果报告


(3) 常用命令
$ nohup fastqc -o . -t 5 -f fastq SRR3101251.fastq &
-o . 表示输出文件位置，输出到fastq文件所在目录时可以忽略该参数
-t 5：表示开5个线程运行。每个thread分配250MB内存。32位系统最多6threads。
-f fastq 表示使用的输入文件格式，支持Valid formats are bam,sam,bam_mapped,sam_mapped and fastq

简单用法：
$ fastqc untreated.fq -o fastqc_out_dir/

当fastq文件比较多时，也可以批量执行：
$ fastqc /path_to_fq/*.fq -o fastqc_out_dir/

#多线程
$ fastqc -t 20 /home/wangjl/data/apa/fq_files/bc/c19_ROW13_R2.fastq -o /home/wangjl/data/apa/190610APA/QC_before_trim/

或者后台执行
$ nohup fastqc -t 10 ../fastq/19C001854_WES001_CapNGS_R1.fq -o fastqc_out/ >R1.log 2>&1 &




2)输入bam文件呢？
$ fastqc  -t 5 -o hg19_mm10_rmdup_QC/ -f bam hg19_mm10_rmdup/c01_ROW01.rmdup.bam








2.fastqc结果报告怎么看？
fastqc结果该怎么看及切割接头：
http://www.huangshujia.me/2017/08/25/2017-08-25-Begining-WGS-Data-Analysis-Fastq-Data-Quality-Control.html
https://www.plob.org/article/5987.html



(4) Per Base Sequence Content
对所有reads的每一个位置，统计ATCG四种碱基（正常情况）的分布：

横轴为位置，纵轴为百分比。 正常情况下四种碱基的出现频率应该是接近的，而且没有位置差异。因此好的样本中四条线应该平行且接近。当部分位置碱基的比例出现bias时，即四条线在某些位置纷乱交织，往往提示我们有overrepresented sequence的污染。当所有位置的碱基比例一致的表现出bias时，即四条线平行但分开，往往代表文库有bias (建库过程或本身特点)，或者是测序中的系统误差。
当任一位置的A/T比例与G/C比例相差超过10%，报"WARN"；当任一位置的A/T比例与G/C比例相差超过20%，报"FAIL"。


(9) Duplicate Sequences
统计序列完全一样的reads的频率。测序深度越高，越容易产生一定程度的duplication，这是正常的现象，但如果duplication的程度很高，就提示我们可能有bias的存在（如建库过程中的PCR duplication）。


(10) Overrepresented Sequences
如果有某个序列大量出现，就叫做over-represented。fastqc的标准是占全部reads的0.1%以上。和上面的duplicate analysis一样，为了计算方便，只取了fq数据的前200,000条reads进行统计，所以有可能over-represented reads不在里面。而且大于75bp的reads也是只取50bp。如果命令行中加入了-c contaminant file，出现的over-represented sequence会从contaminant_file里面找匹配的hit（至少20bp且最多一个mismatch），可以给我们一些线索。
当发现超过总reads数0.1%的reads时报”WARN“，当发现超过总reads数1%的reads时报"FAIL"。

建库接头序列 https://github.com/csf-ngs/fastqc/blob/master/Contaminants/contaminant_list.txt








3.Ubuntu1804 报错处理
(1)$ fastqc *.fastq &
Analysis complete for 19C001854_WES001_CapNGS_R1.fq
Exception in thread "Thread-1" java.awt.AWTError: Assistive Technology not found: org.GNOME.Accessibility.AtkWrapper
        at java.awt.Toolkit.loadAssistiveTechnologies(Toolkit.java:807)
        at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:886)
        at sun.swing.SwingUtilities2.getSystemMnemonicKeyMask(SwingUtilities2.java:2020)
        at javax.swing.plaf.basic.BasicLookAndFeel.initComponentDefaults(BasicLookAndFeel.java:1158)

解决方法:
$ sed -i 's/^assistive_technologies=/#&/' /etc/java-8-openjdk/accessibility.properties
Or just comment out below line in  /etc/java-8-openjdk/accessibility.properties
assistive_technologies=org.GNOME.Accessibility.AtkWrapper











========================================
|-- MultiQC: 批量显示QC结果的利器
----------------------------------------

1.安装
pip3 install multiqc
multiqc --help

$ fastqc --version
FastQC v0.11.7

$ multiqc --version
multiqc, version 1.7






2.使用 
(1) fastqc获得每个文件的QC报告
#单个文件：
$ fastqc untreated.fq -o fastqc_out_dir/

#批量化 先获取QC结果 *gz
ls ../R2_left/*.fastq | while read id; do fastqc -t 4 $id -o .; done


(2)multiqc 批量汇总QC报告
# multiqc
multiqc *fastqc.zip --pdf #需要提前安装某些包
# or
multiqc *fastqc.zip -o /path/to/store/




例子:
$ ls ../R2_left/c2_ROW01_R2.fastq | while read id; do fastqc -t 40 $id -o ./; done
$ multiqc *fastqc.zip -o /data/jinwf/wangjl/c1_2019APA/sc/combine/sc_fq/



refer:
https://blog.csdn.net/ada0915/article/details/77201871




========================================
测序文件预处理:切除接头adapter/primer等
----------------------------------------
1.概述
In addition, poly(A) (or poly(T) on the reverse strands) could also be removed by cutadapt or FASTXToolkit. 

Tools to remove adapter sequences from next-generation sequencing data
http://bioscholar.com/genomics/tools-remove-adapter-sequences-next-generation-sequencing-data/
https://www.biostars.org/p/98707/

1) Remove adapter sequences using fastX toolkit
2) Run fastQC to identify read quality and trim accordingly
3) Run Deconseq to remove contaminants
4) Remove short reads (<10nt)
5) Assemble using bowtie2 against reference (nb our strain is not exactly the reference but should similar enough)


去接头的软件包括 Trimmomatic、cutadapt、fastx_toolkit、fastp 等。


Q&A:
1.没必要去除polyA，因为不比对到任何地方?
https://www.biostars.org/p/148743/
Have you tried mapping the RNA-seq data yet? It may not be necessary to remove polyA stretches because they won't map to a unique location and you may already have enough reads to not worry about it. 




========================================
|-- 高通量测序数据质控神器—Trimmomatic （也是一个java程序）
----------------------------------------
1. 简介
(1) 文章
Trimmomatic: A flexible read trimming tool for Illumina NGS data
Citations
Bolger, A. M., Lohse, M., & Usadel, B. (2014). Trimmomatic: A flexible trimmer for Illumina Sequence Data. Bioinformatics, btu170.

高通量测序数据质控神器——Trimmomatic。这个于 2014 年发表在 Bioinformatics 上的软件，至今为止在 Web of Science 上可以检索到 2,098 次引用，而在谷歌学术上更是达到了惊人的 3,391 次：


(2) 这个软件为什么深受大家的喜爱呢？今天分析一下它在质控方面的强大之处。

1). 无脑安装、使用"简单"、运行速度可观
这个软件是用 Java 写的，运行效率比较高.

2). 强大的去接头能力
一般的质控软件在处理含有接头序列的 reads 时，通常采用 "在允许错配的情况下，如果分析的 read 匹配一定数量的接头序列即去除这条 read 或从匹配开始的位置截断 read，仅保留匹配位置之前的部分序列" 的方式。

如果采取 "去除含有接头序列的 reads" 的方式，会造成测序数据的浪费 (如果片段选择没有控制好，整个 lane 会有很大一部分数据含有接头序列，怎么办？);

如果采取 "从匹配开始的位置截断 read，仅保留匹配位置之前的部分序列" 的方式，对于只含有少数几个碱基的 reads，普通的质控软件是处理不了的（又该怎么办？）。

But，Trimmomatic 有两种模式：Single End Mode 和 Paired End Mode，对于单端测序数据，它和其它软件相比没有明显的优势；但如果是双端测序的数据，Trimmomatic 采用两种去接头方式，更强大，更彻底！




(3) 优势 

Trimmomatic 有两种模式：Single End Mode 和 Paired End Mode，对于单端测序数据，它和其它软件相比没有明显的优势；但如果是双端测序的数据，Trimmomatic 采用两种去接头方式，更强大，更彻底！

图略：https://www.plob.org/article/12130.html

i)普通模式：匹配一定数量的接头序列即截断序列，保留匹配起始位置之前的序列，
A、如果从 reads 的开始就匹配到接头序列的话，整条 reads 会被去除；
B、如果是从 reads 的其它部分匹配到接头序列，则从匹配的位置截断序列，保留包含接头的部分。

ii)超级强大的回文模式，如上图 Ｃ和 D 所示
想要了解回文模式去接头的原理，我们需要先熟悉一下：测序结果中的接头序列来自哪里？ 由于只有当插入片段的长度小于测序的读长时才会在测序结果中出现接头序列。那么对于含有接头的片段，正反向的 reads 在除接头之外的部分应该是反向互补的。因此，对于双端测序数据的处理上，Trimmomatic 在考虑接头匹配情况的同时也检查正反向 reads 的序列，从而更加有效的去掉接头序列。理论上，即使 read 仅含有 1 个碱基的接头序列，这 1 个碱基也能被切除！

细节见文档 http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf









2. 使用: 优点是内置有接头数据
(0) 安装
https://github.com/usadellab/Trimmomatic
http://www.usadellab.org/cms/?page=trimmomatic

需要安装过 java(略)
$ wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.39.zip
$ unzip Trimmomatic-0.39.zip
$ ll /home/wangjl/soft/Trimmomatic-0.39/trimmomatic-0.39.jar



(1) Paired End: 双端测序 
With most new data sets you can use gentle quality trimming and adapter clipping.

通常我们不需要做首尾截掉。 对PE数据通常使用 keepBothReads 参数，你可以留下甚至冗余的序列，以便让流程更好管理。
You often don't need leading and traling clipping. Also in general keepBothReads can be useful when working with paired end data, you will keep even redunfant information but this likely makes your pipelines more manageable. 
注意 keepBothReads 前面的:2，这是回文模式最少的 adapter 长度，你甚至可以设置为1。（默认是保守的8）
Note the additional :2 in front of keepBothReads this is the minimum adapter length in palindrome mode, you can even set this to 1. (Default is a very conservative 8)

$ java -jar trimmomatic-0.39.jar PE \
	input_forward.fq.gz input_reverse.fq.gz \
	output_forward_paired.fq.gz output_forward_unpaired.fq.gz \
	output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz \
	ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36


for reference only (less sensitive for adapters) 只有首尾两行有差异。
$ java -jar trimmomatic-0.35.jar PE -phred33 \
	input_forward.fq.gz input_reverse.fq.gz \
	output_forward_paired.fq.gz output_forward_unpaired.fq.gz \
	output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz \
	ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36


This will perform the following: 最后一行的质控都干了啥？(括号里对应着命令的最后一行)
- 去接头 Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10)
- 去掉前面低质量序列 Remove leading low quality or N bases (below quality 3) (LEADING:3)
- 去掉后面低质量序列 Remove trailing low quality or N bases (below quality 3) (TRAILING:3)
- 按照4碱基滑窗，切除平均低于15的碱基。Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15)
- 低于最低长度的去掉 Drop reads below the 36 bases long (MINLEN:36)



测试:
$ cd /data/wangjl/ATAC/fq/test4
$ java -jar /home/wangjl/soft/Trimmomatic-0.39/trimmomatic-0.39.jar PE  \
	raw/SRR7629152_1.fastq.gz raw/SRR7629152_2.fastq.gz \
	clean2/SRR7629152_1.paired.fq.gz clean2/SRR7629152_1.unpaired.fq.gz \
	clean2/SRR7629152_2.paired.fq.gz clean2/SRR7629152_2.unpaired.fq.gz \
	ILLUMINACLIP:/home/wangjl/soft/Trimmomatic-0.39/adapters/TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36

查看接头文件的内容
$ cat /home/wangjl/soft/Trimmomatic-0.39/adapters/TruSeq3-PE.fa
>PrefixPE/1
TACACTCTTTCCCTACACGACGCTCTTCCGATCT
>PrefixPE/2
GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT

关于adapter： 目前绝大部分的illumina的Hiseq和Miseq系列使用的都是Truseq3，过去的GA2测序仪使用的是Truseq2，PE/SE对应单端还是双端测序 ， 如果使用的不是illumina测的，可以按照里面的格式自己新建一个接头文件，但其中的命名要注意。详情见官网主页



try1: 报错 java.io.FileNotFoundException: /data/wangjl/ATAC/fq/test4/TruSeq3-PE.fa (No such file or directory) 
try2: 在软件安装目录 adapters/ 下，添加绝对地址后OK。
check: 原文3M，生成文件
-rw-rw-r-- 1 wangjl wangjl 3.0M Jul 19 14:39 SRR7629152_2.paired.fq.gz
-rw-rw-r-- 1 wangjl wangjl 2.3K Jul 19 14:39 SRR7629152_2.unpaired.fq.gz
没有质控报告。
速度很快！几秒。



(2) Single End: 单端测序 
$ java -jar trimmomatic-0.35.jar SE -phred33 input.fq.gz output.fq.gz ILLUMINACLIP:TruSeq3-SE:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36
This will perform the same steps, using the single-ended adapter file

测试: 
$ java -jar /home/wangjl/soft/Trimmomatic-0.39/trimmomatic-0.39.jar SE -phred33 \
	raw/SRR7629152_1.fastq.gz clean2/SRR7629152_1.clean.fq.gz \
	ILLUMINACLIP:/home/wangjl/soft/Trimmomatic-0.39/adapters/TruSeq3-SE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36

check: 笔者上面的双端，去掉的有点多。
-rw-rw-r-- 1 wangjl wangjl 2.9M Jul 19 14:52 SRR7629152_1.clean.fq.gz













========================================
|-- FASTXToolkit(http://hannonlab.cshl.edu/fastx_toolkit/)
----------------------------------------
https://github.com/DawnEve/NGS_training/blob/master/day3.markdown
说明书： http://hannonlab.cshl.edu/fastx_toolkit/commandline.html


下载和安装(http://hannonlab.cshl.edu/fastx_toolkit/install_centos.txt) 
http://hannonlab.cshl.edu/fastx_toolkit/download.html
$ axel -n 20 http://hannonlab.cshl.edu/fastx_toolkit/fastx_toolkit_0.0.13_binaries_Linux_2.6_amd64.tar.bz2
$ tar -xjvf fastx_toolkit_0.0.13_binaries_Linux_2.6_amd64.tar.bz2
添加路径：将fastx_toolkit的路径加入.bashrc最后一行。并执行source ~/.bashrc 使配置立即生效
export PATH=/home/wangjl/software/fastx_toolkit/bin/:$PATH



#数据过滤
$ fastq_quality_filter -q 30 -p 100 -i test_1.fq -o test_1_filter.fq -Q 33
$ fastq_quality_filter -q 30 -p 100 -i test_2.fq -o test_2_filter.fq -Q 33
-q 30 最低质量分数是30才保留该碱基
-p 80 是最低合格的百分比，Minimum percent of bases that must have [-q] quality.这个地方有问题//todo

-Q 33防止报错。因为默认使用的phred64，而很多使用的是phred33。

不能输入.gz格式的文件！！变通方式：
$ zcat /home/wangjl/data/scFQ/c12_A1.fa.gz | fastq_quality_filter -q 25 -p 60 -z -o /home/wangjl/data/scFQ_filtered/c12_A1_filtered.fq.gz -Q 33

再次fastqc，
$ fastqc -o ./ -t 15 c12_A1_filtered.fq.gz
发现超过100区域碱基不平衡。
问题：Total Sequences（也就是reads数）从3132226减少到1591872。

怎么去除polyA尾巴？





========================================
|-- cutadapt: 一个python包
----------------------------------------
https://cutadapt.readthedocs.io/en/stable/

安装python3: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh

(1)使用pip安装：
$ pip install --user --upgrade cutadapt
检查版本号：
$ cutadapt --version
1.14



$ cutadapt --help
cutadapt version 2.3 #2019.6.15
#示例
$ cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq
-j CORES, --cores CORES   指定cpu数量
--trim-n              Trim N's on ends of reads. 感觉应该先去两端的N

-a ADAPTER, --adapter ADAPTER
	Sequence of an adapter ligated to the 3' end (paired data: of the first read). 
	The adapter and subsequent bases are trimmed. 
	If a '$' character is appended ('anchoring'), the adapter is only found if it is a suffix of the read.
	去除3'端接头及之后的序列。如果加上$后缀，则只在接头是后缀时去除。
-g ADAPTER, --front ADAPTER
	Sequence of an adapter ligated to the 5' end (paired data: of the first read). 
	The adapter and any preceding bases are trimmed. Partial matches at the 5' end are allowed. 
	If a '^' character is prepended ('anchoring'), the adapter is only found if it is a prefix of the read.
	去除5'端接头及之前的序列。允许5'端部分匹配。如果有^前缀，则只在接头是前缀时去除。
-b ADAPTER, --anywhere ADAPTER
	如果第一个碱基和接头匹配，则和-g参数类似，否则和-a参数类似。常用语挽救失败的文库，如果你知道接头位置，不要使用该参数！

-e RATE, --error-rate RATE
	Maximum allowed error rate as value between 0 and 1	(no. of errors divided by length of matching region).Default: 0.1 (=10%)
	最大允许错误，0到1之间，默认0.1
-n COUNT, --times COUNT
    Remove up to COUNT adapters from each read. Default: 1
	每个序列最多去除几个接头？默认是1

-q [5'CUTOFF,]3'CUTOFF, --quality-cutoff [5'CUTOFF,]3'CUTOFF
	Trim low-quality bases from 5' and/or 3' ends of each read before adapter removal. Applied to both reads if data is paired. 
	If one value is given, only the 3' end is trimmed. 
	If two comma-separated cutoffs are given, the 5' end is trimmed with the first cutoff, the 3' end with the second.
	去除质量分数低的序列。如果只给一个值，则只去除3'端。如果逗号隔开的2个值，则第一个过滤5'端，第二个过滤3'端。

-m LEN[:LEN2], --minimum-length LEN[:LEN2]
    Discard reads shorter than LEN. Default: 0
	丢弃长度太短的序列，默认0不丢弃



(2)按照碱基质量剪切：
cutadapt要求输入文件结尾必须是fq，fastq或者fq.gz，fastq.gz，不能是fa.gz，否则报错。
http://cutadapt.readthedocs.io/en/stable/guide.html
$ cutadapt -q 10 -o output.fastq input.fastq

$ cutadapt -q 25 -m 90 -o /home/wangjl/data/scFQ_filtered/c12_A1_filtered.fastq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
#3'端去掉质量分数小于25的碱基，舍弃小于90bp的序列。


(3)测试 cutadapt 中去除polyA尾巴的参数
去除polyA尾巴 For poly-A trimming, for example, you would write:
$ cutadapt -a "A{20}" -o output.fastq input.fastq

$ cutadapt -a "A{10}" -q 25 -m 30 -o filteredA10_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{20}" -q 25 -m 30 -o filteredA20_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{30}" -q 25 -m 30 -o filteredA30_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{100}" -q 25 -m 30 -o filteredA100_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz

NofA	reads
10	2930148
20	2942041
30	2943094
100	2985093

这个N越小保留下的reads越少。



(4) 去除5'端接头
$ cutadapt -g AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT -q 25 -m 20 -o dAdapter1_CutA_c16_ROW17.fq ../polyA_fq/CutA_c16_ROW17.fq >c16_ROW17.log
如果是另一端的PCR引物测穿了，需要使用其反向互补序列。







========================================
|-- 建库接头序列 contaminant_list.txt
----------------------------------------

This file contains a list of potential contaminants which are frequently found in high throughput sequencing reactions.  
These are mostly sequences of adapters / primers used in the various sequencing chemistries.
该文件包含高通量测序常见的污染源，通常是各种测序试剂中的adapters 和 primers。

Please DO NOT rely on these sequences to design your own oligos, some of them are truncated at ambiguous positions, and none of them are definitive sequences from the manufacturers so don't blame us if you try to use them and they don't work.
不要依靠这些序列设计自己的oligo，有些在不明确的位置被删减了，这些都不是厂家给定义的序列，所以如果做不出来不要责怪我们。

You can add more sequences to the file by putting one line per entry and specifying a name[tab]sequence.  
If the contaminant you add is likely to be of use to others please consider sending it to the FastQ authors, either via a bug report at www.bioinformatics.bbsrc.ac.uk/bugzilla/ or by directly emailing simon.andrews@bbsrc.ac.uk so other users of the program can benefit.

Illumina Single End Adapter 1  ACACTCTTTCCCTACACGACGCTGTTCCATCT
Illumina Single End Adapter 2  CAAGCAGAAGACGGCATACGAGCTCTTCCGATCT
Illumina Single End PCR Primer 1  AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Single End PCR Primer 2  CAAGCAGAAGACGGCATACGAGCTCTTCCGATCT
Illumina Single End Sequencing Primer  ACACTCTTTCCCTACACGACGCTCTTCCGATCT
	
Illumina Paired End Adapter 1   ACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End Adapter 2   CTCGGCATTCCTGCTGAACCGCTCTTCCGATCT
Illumina Paried End PCR Primer 1   AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End PCR Primer 2   CAAGCAGAAGACGGCATACGAGATCGGTCTCGGCATTCCTGCTGAACCGCTCTTCCGATCT
Illumina Paried End Sequencing Primer 1		ACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End Sequencing Primer 2		CGGTCTCGGCATTCCTACTGAACCGCTCTTCCGATCT


more: https://github.com/csf-ngs/fastqc/blob/master/Contaminants/contaminant_list.txt





========================================
|-- 去除接头 trim_galore: 自动检测adapter的质控软件
----------------------------------------
1.简介

_Trim Galore_ is a wrapper around [Cutadapt](https://github.com/marcelm/cutadapt) and [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data.

Trim Galore是对FastQC和Cutadapt的包装。适用于所有高通量测序，包括RRBS(Reduced Representation Bisulfite-Seq ), Illumina、Nextera 和smallRNA测序平台的双端和单端数据。主要功能包括两步：
第一步首先去除低质量碱基，然后去除3' 末端的adapter, 如果没有指定具体的adapter，程序会自动检测前1million的序列，然后对比前12-13bp的序列是否符合以下类型的adapter:

Illumina:   AGATCGGAAGAGC
Small RNA:  TGGAATTCTCGG
Nextera:    CTGTCTCTTATA






2. 下载和安装
(1)#不推荐！会在路径提示符前面添加(base)，洁癖的人不要尝试: conda install -c bioconda trim-galore 


(2)[推荐] 下载安装包安装，解压，配置下环境变量就可以使用：https://github.com/FelixKrueger/TrimGalore
$ git clone https://github.com/FelixKrueger/TrimGalore.git
然后里面有二进制文件，就可以用了。
在~/bin下新建软链接：
$ ln -s /home/wangjl/software/TrimGalore/trim_galore
$ trim_galore -v
version 0.6.2


官方安装步骤:
```bash
# Check that cutadapt is installed
cutadapt --version
# Check that FastQC is installed
fastqc -v
# Install Trim Galore
curl -fsSL https://github.com/FelixKrueger/TrimGalore/archive/0.6.0.tar.gz -o trim_galore.tar.gz
tar xvzf trim_galore.tar.gz
# Run Trim Galore
~/TrimGalore-0.6.0/trim_galore











3.使用

(1)示例
# 处理双端测序结果
echo " trim_galore cut adapters started at $(date)"
trim_galore -q 20 --phred33 --stringency 3 --length 20 -e 0.1 \
            --paired $dir/cmp/01raw_data/$fq1 $dir/cmp/01raw_data/$fq2  \
            --gzip -o $input_data
echo "trim_galore cut adapters finished at $(date)"


# 单端测序结果
$ trim_galore --quality 25 --phred33 --stringency 3 --length 30 --gzip  --fastqc_args "-t 15" --output_dir trim_galore xx.fastq

参数解释(部分参数解释见(2))
$ trim_galore --help
--adapter：输入adapter序列。也可以不输入，Trim Galore!会自动寻找可能性最高的平台对应的adapter。自动搜选的平台三个，也直接显式输入这三种平台，即--illumina、--nextera和--small_rna。

--paired：对于双端测序结果，一对reads中，如果有一个被剔除，那么另一个会被同样抛弃，而不管是否达到标准。
--retain_unpaired：对于双端测序结果，一对reads中，如果一个read达到标准，但是对应的另一个要被抛弃，达到标准的read会被单独保存为一个文件。
--gzip和--dont_gzip：清洗后的数据zip打包或者不打包。

-- trim-n : 移除read一端的reads
-e：允许的错误率




(2) 批量模式
echo "Trim galore reads"
# ls *.gz | while read id; do echo "Trim ${id}"; trim_galore --quality 25 --phred33 --stringency 3 --length 30 --gzip  --fastqc_args "-t 15" --output_dir trim_galore $id; done
参数解释
	--quality 25 #设定Phred quality score阈值，默认为20。
	--phred33  #选择-phred33或者-phred64，表示测序平台使用的Phred quality score。
	--stringency 3 #设定可以忍受的前后adapter重叠的碱基数，默认为1（非常苛刻）。可以适度放宽，因为后一个adapter几乎不可能被测序仪读到。
	--length 30 #设定输出reads长度阈值，小于设定值会被抛弃。
	--gzip  #--gzip和--dont_gzip：清洗后的数据zip打包或者不打包。
	--fastqc_args "-t 15" # 线程数量
	--output_dir #输出目录。需要提前建立目录，否则运行会报错。
		-o：输出文件路径
#




4.质控
$ cd /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/trim/galore
$ multiqc *fastqc.zip -o /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/



refer:
https://www.jianshu.com/p/7a3de6b8e503












========================================
比对工具: RNA Mapper
----------------------------------------

Popular short read aligners
http://homer.ucsd.edu/homer/basicTutorial/mapping.html
more: https://en.wikipedia.org/wiki/List_of_sequence_alignment_software#Short-Read_Sequence_Alignment

Most Popular:
bowtie : fast, works well
bowtie2 : fast, can perform local alignments too

Subread - Very fast, (also does splice alignment)
STAR - Extremely fast (also does splice alignment, requires at least 30 Gb memory)
To be honest, I would probably recommend STAR for almost any application at this point if you have the memory (see below)

BWA - Fast, allows indels, commonly used for genome/exome resequencing




========================================
|-- Bowtie2: A fast and sensitive gapped read aligner
----------------------------------------
What is Bowtie?
http://www.cnblogs.com/emanlee/archive/2011/11/12/2246358.html

Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index (based on the Burrows-Wheeler Transform or BWT) to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. Multiple processors can be used simultaneously to achieve greater alignment speed. Bowtie 2 outputs alignments in SAM format, enabling interoperation with a large number of other tools (e.g. SAMtools, GATK) that use SAM. Bowtie 2 is distributed under the GPLv3 license, and it runs on the command line under Windows, Mac OS X and Linux.

Bowtie 2 is often the first step in pipelines for comparative genomics, including for variation calling, ChIP-seq, RNA-seq, BS-seq. Bowtie 2 and Bowtie (also called "Bowtie 1" here) are also tightly integrated into some tools, including TopHat: a fast splice junction mapper for RNA-seq reads, Cufflinks: a tool for transcriptome assembly and isoform quantitiation from RNA-seq reads, Crossbow: a cloud-enabled software tool for analyzing reseuqncing data, and Myrna: a cloud-enabled software tool for aligning RNA-seq reads and measuring differential gene expression.


http://genomebiology.com/2009/10/3/R25  (paper)
http://bowtie-bio.sourceforge.net/index.shtml (source, bin)
http://www.genome.iastate.edu/bioinfo/resources/manuals/rna_bowtie.txt
http://blog.csdn.net/cherylnatsu/article/details/6801997


官网 http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml


1.安装
https://github.com/BenLangmead/bowtie2


(1) conda安装（不推荐，会有不可测的副作用）
https://anaconda.org/bioconda/bowtie2

$ conda install -c bioconda bowtie2 #需要安装和更新很多包。
# 安装了perl，会导致冲突，又删除了
$ conda remove bowtie2 #卸载失败
$ conda uninstall -c bioconda bowtie2
## 必须同意更新好几个包，才能删除
$ which bowtie2
/usr/bin/which: no bowtie2 in (...



(2) binaries安装（推荐）
http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#building-from-source

1) 安装依赖包
Operating System	/Sync Package List	/Search	/Install
Ubuntu, Mint, Debian	/apt-get update	/apt-cache search tbb	/apt-get install libtbb-dev
Fedora, CentOS	/yum check-update	/yum search tbb	/yum install tbb-devel.x86_64

## yum install tbb-devel.x86_64

$ yum list installed | grep tbb
tbb.x86_64                             4.1-9.20130314.el7              @anaconda
tbb-devel.x86_64                       4.1-9.20130314.el7              @anaconda

2) 下载 binaries package
$ wget https://github.com/BenLangmead/bowtie2/releases/download/v2.3.5.1/bowtie2-2.3.5.1-linux-x86_64.zip
$ unzip bowtie2-2.3.5.1-linux-x86_64.zip 

3) 添加到路径[推荐]
$ vim ~/.bashrc #末尾添加一行
export PATH=/home/wangjl/software/bowtie2-2.3.5.1-linux-x86_64:$PATH
$ source ~/.bashrc

（或者把可执行文件复制到路径中的目录中： bowtie2, bowtie2-align-s, bowtie2-align-l, bowtie2-build, bowtie2-build-s, bowtie2-build-l, bowtie2-inspect, bowtie2-inspect-s and bowtie2-inspect-l.）


4) 检查路径和版本号
$ which bowtie2
~/software/bowtie2-2.3.5.1-linux-x86_64/bowtie2

$ bowtie2 --version
/home/wangjl/software/bowtie2-2.3.5.1-linux-x86_64/bowtie2-align-s version 2.3.5.1
64-bit
Built on 
Wed Apr 17 02:50:12 UTC 2019
Compiler: gcc version 7.3.1 20180303 (Red Hat 7.3.1-5) (GCC) 
Options: -O3 -m64 -msse2 -funroll-loops -g3 -g -O2 -fvisibility=hidden -I/hbb_exe_gc_hardened/include -ffunction-sections -fdata-sections -fstack-protector -D_FORTIFY_SOURCE=2 -fPIE -std=c++98 -DPOPCNT_CAPABILITY -DWITH_TBB -DNO_SPINLOCK -DWITH_QUEUELOCK=1
Sizeof {int, long, long long, void*, size_t, off_t}: {4, 8, 8, 8, 8, 8}






2. 文档 https://github.com/BenLangmead/bowtie2
https://blog.csdn.net/u011262253/article/details/79833969

(1)建立索引
# Building a small index
bowtie2-build example/reference/lambda_virus.fa example/index/lambda_virus
# Building a large index
bowtie2-build --large-index example/reference/lambda_virus.fa example/index/lambda_virus

建立6文件索引：
$ bowtie2-build mm9.fa bowtie2/mm9 --threads 50 #bowtie2/mm9 中mm9是前缀，留空不好查看文件

/home/wangjl/data/ref/human
$ bowtie2-build hg19.fa bowtie2/hg19 --threads 90 #使用90个线程



(2)开始mapping
# Aligning unpaired reads
bowtie2 -x example/index/lambda_virus -U example/reads/longreads.fq
# Aligning paired reads
bowtie2 -x example/index/lambda_virus -1 example/reads/reads_1.fq -2 example/reads/reads_2.fq


我的是单端测序：
$ bowtie2 -p 6 --local -x /home/wangjl/data/ref/mouse/bowtie2/mm9 -U /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/R2_left/c2_ROW01_R2.fastq -S /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bowtie2_mouse/c2_ROW01_R2.sam
 -p 6 使用6个线程
 -S 指定输出文件
 --local	在这种模式下，Bowtie 2不要求整个读取从一端到另一端对齐。相反，为了达到最大可能的对齐分数，可以从末端省略一些字符（“软裁剪”）

$ bowtie2 -p 60 --local -x /home/wangjl/data/ref/human/bowtie2/hg19 -U /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/R2_left/c2_ROW01_R2.fastq -S /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bowtie2_human/c2_ROW01_R2.sam


结论：bowtie2的--local选项，有助于提高比对比率。可能是能去除两端残留的建库index。








========================================
|-- STAR: ultrafast universal RNA-seq aligner 
----------------------------------------

STAR is recommended by the ENCODE project, definitely should be your best option(among Bowtie2,).


RNA-seq比对算法开发了STAR（Spliced Transcripts Alignments to a Reference，STAR）.
该算法使用了未压缩后缀阵列中的连续最大可比对种子搜索，接着种子聚类和缝合过程。

1.paper:https://academic.oup.com/bioinformatics/article/29/1/15/272537
Published: 25 October 2012
STAR被实现为一个单机C++代码。

rna call varients时gatk推荐工具，broad institute都推荐了，还是encode计划时冷泉港内部开发的，特点：超级快速（8min map完6gb的reads）、as支持性好、支持长reads、全转录本、发现嵌合转录本等，有理由看一下。

STAR, 很犀利, ENCODE专属RNA-seq工具. 在准度和时间消耗上, 效果拔群. 因为STAR, 了解一下啥是suffix array. 原来做mapping的, 真的就是ctrl+F的工作... 只是, 这个字符串寻找比较麻烦, 既要容错(insertion/deletion/mismatch), 又要考虑到RNA splicing而导致的genomic gap. 

容错的这个还好, 因为DNA-seq已经打下了夯实基础; 反倒是splicng带来的splicing junction detection问题, 是RNA-seq里专属. 所以在处理DNA-seq和RNA-seq数据做mapping时, 真的不一样. 比如bowtie是unspliced mapper, tophat是spliced mapper, 也难怪bowtie多用于DNA-seq而tophat多用于RNA-seq的mapping步骤. 

STAR 吐槽现有的RNA-seq工具都是DNA-seq工具的延伸, 并非量身打造. 从其算法里的mapping一步来看, 应该属于spliced mapper, 但又不同于把reads打断成k-mer形式. 








2. download and install 安装比对软件 STAR

(1)安装star 2.7：https://github.com/alexdobin/STAR
# Get latest STAR source from releases
wget https://github.com/alexdobin/STAR/archive/2.7.0f.tar.gz
tar -xzf 2.7.0f.tar.gz
cd STAR-2.7.0f

#编译 Compile under Linux
cd STAR/source
make STAR

#然后在~/bin下添加快捷方式
$ ln -s /home/wangjl/data/software/STAR-2.7.0f/source/STAR

#查看版本号
$ STAR --version
2.7.0f


(2)最后使用的是(lab上一致的)老版本2.5.2：
https://github.com/alexdobin/STAR/archive/2.5.2b.tar.gz
https://github.com/alexdobin/STAR

find one on the server:
$ find / -name '*STAR*'
/share/apps/genomics/STAR-2.5.2b

use it directly:
$ cd /home/wangjl/bin/
$ ln -s /share/apps/genomics/STAR-2.5.2b/bin/Linux_x86_64/STAR
$ STAR --version
STAR_2.5.2b




(3)下载参考基因组
1).资源:下载hg19基因组的fasta文件和gtf注释文件
参考基因组 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/
jinlab: $ ls /data1/hou/RNA/refs/hg19


a1) 下载方式1
axel -n 30 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz

a2) 下载方法2
axel -n 40 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz
tar -zxvf chromFa.tar.gz
cd chroms
cat *.fa >hg19.fa



2)下载老鼠mm9基因组
axel -n 20 http://hgdownload.cse.ucsc.edu/goldenPath/mm9/bigZips/mm9.2bit


3)怎么把2bit转变成fasta？使用twoBitToFa工具
https://blog.csdn.net/weixin_40099163/article/details/86151988

mm9.2bit - contains the complete mm9 Mouse Genome
    in the 2bit format.  A utility program, twoBitToFa (available from our src tree), can be used to extract .fa file(s) from this file.  See also:
        http://genome.ucsc.edu/admin/cvs.html - CVS access to the source tree
        http://genome.ucsc.edu/admin/jk-install.html - building the utilities
A pre-compiled version of the command line tool can be found at: http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/

下载
$ axel -n 30 http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa
$ chmod +x twoBitToFa
添加到~/bin下。

运行该软件：
$ twoBitToFa mm9.2bit mm9.fa


4)下载mm10  http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/
cd /home/wangjl/data/ref/mm10
$ axel -n 80 http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.2bit
#下载mm10 gtf和bed文件



(4)从UCSC下载gtf注释文件

选择最新的gtf注释文件，人类和小鼠常在http://www.gencodegenes.org下载，植物的可信基因组见http://plants.ensembl.org





(5) 使用genecode的一套fa和gtf文件: Release M25 (GRCm38.p6)
$ cd /home/wangjl/data/ref/mouse_M25
$ wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.annotation.gtf.gz
$ wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/GRCm38.p6.genome.fa.gz

ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.transcripts.fa.gz

$ gunzip *








3.generating genome index 生成索引(很耗时)
STAR command line format: STAR --option1-name option1-value(s)--option2-name option2-value(s)

(1)生成hg19的STAR index（放在 nohup CMD & 中运行的）
STAR --runMode genomeGenerate  \
	--runThreadN 20  \
	--genomeDir /home/wangjl/index/STAR/  \
	--genomeFastaFiles /share/reference/genome/hg19/hg19.fa  \
	--sjdbGTFfile /share/reference/genome/hg19/hg19_ucsc_genes.gtf  \
	--sjdbOverhang 100
# 大服务器上:
STAR --runMode genomeGenerate \
	--runThreadN 50 \
	--genomeDir /home/wangjl/data/ref/human/STAR/  \
	--genomeFastaFiles /home/wangjl/data/ref/human/hg19.fa \
	--sjdbGTFfile /home/wangjl/data/ref/human/hg19_ucsc_genes-20190506.gtf \
	--sjdbOverhang 100
参数解释：
	runThreadN 线程数
	runMode 运行模式，genomeGenerate 选项用来产生index;
	genomeDir 指定生成的index保存的位置
	genomeFastaFiles 输入的参考基因组文件fasta。
	sjdbGTFfile 注释gtf文件
	sjdbOverhang : specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. max(ReadLength)-1 或者默认100.

很费时，占用CPU很严重，建议晚上进行。小心被骂。
Jan 17 21:18:32 ..... started STAR run
Jan 17 21:18:32 ... starting to generate Genome files
Jan 17 21:19:47 ... starting to sort Suffix Array. This may take a long time...
Jan 17 21:20:03 ... sorting Suffix Array chunks and saving them to disk...
Jan 17 21:32:57 ... loading chunks from disk, packing SA...
Jan 17 21:34:13 ... finished generating suffix array
Jan 17 21:34:13 ... generating Suffix Array index
Jan 17 21:37:29 ... completed Suffix Array index
Jan 17 21:37:29 ..... processing annotations GTF
Jan 17 21:37:35 ..... inserting junctions into the genome indices
Jan 17 21:39:49 ... writing Genome to disk ...
Jan 17 21:39:50 ... writing Suffix Array to disk ...
Jan 17 21:40:02 ... writing SAindex to disk
Jan 17 21:40:04 ..... finished successfully

推荐使用好理解的路径名，比如 /home/wangjl/data/ref/hg19/index/star

lab server上本来就有该STAR可用的hg19索引:
/data1/hou/RNA/refs/hg19_ERCC92
/data1/hou/RNA/refs/hg19_ERCC92/index/star


(2)生成小鼠的基因组STAR index（放在 nohup CMD & 中运行的）
STAR --runThreadN 50 --runMode genomeGenerate --genomeDir /home/wangjl/data/ref/mouse/STAR/  --genomeFastaFiles /home/wangjl/data/ref/mouse/mm9.fa --sjdbGTFfile /home/wangjl/data/ref/mouse/mm9_ucsc_genes-20190506.gtf --sjdbOverhang 100
[11:05 - 11:21]


## 193服务器上
$ STAR --runMode genomeGenerate \
	--runThreadN 50 \
	--genomeDir /home/wangjl/data/ref/mouse_M25/index/star/  \
	--genomeFastaFiles /home/wangjl/data/ref/mouse_M25/GRCm38.p6.genome.fa \
	--sjdbGTFfile /home/wangjl/data/ref/mouse_M25/gencode.vM25.annotation.gtf \
	--sjdbOverhang 100
#
[11:52 - 12:07]









4.Run mapping jobs. 

51页手册STARmanual.pdf： https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf
推荐直接用(3)，
多样本考虑(2)，
uniq map低看看(4)


(1) 基本比对，后面还有3个改进版。
test data
[wangjl@nih_jin test3]$ ls -lth /home/wangjl/data/test
total 2.5G
-rwxr-xr-x. 1 wangjl user 287M Jan 17 21:59 c16_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 1.5G Jan 17 21:59 c15_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 100M Jan 17 21:59 c14_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 365M Jan 17 21:59 c13_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 261M Jan 17 21:59 c12_A1.fa.gz

###基本语句to hg19
$ STAR --runThreadN 5  \
	--genomeDir /data1/hou/RNA/refs/hg19_ERCC92/index/star  \
	--readFilesIn /home/wangjl/data/test/c13_A1.fa.gz  \
	--readFilesCommand gunzip -c  \
	--outFileNamePrefix /home/wangjl/data/afterMapping/test3/c14_A1_

Jan 17 22:14:40 ..... started STAR run
Jan 17 22:14:40 ..... loading genome
Jan 17 22:15:10 ..... started mapping
Jan 17 22:17:03 ..... finished successfully
参数解释：
	--runThreadN 线程数
	--genomeDir 基因组index路径
	--readFilesIn 带路径名的fq测序文件
	--readFilesCommand gunzip -c  读取文件的解压命令和参数。也可以使用 --readFilesCommand zcat \
	--outFileNamePrefix 输出文件的前缀，默认是./
	
获得文件: c14_A1_Aligned.out.sam


(2)改进1： 对多个样本的比对，建议添加保留基因组选项（--genomeLoad LoadAndKeep），对于共用一套index的多个比对，能节省很多内存，提高比对的并发数量。
该测序read最长150bp。
$ STAR --runThreadN 10 \
	--genomeDir /data1/hou/RNA/refs/hg19_ERCC92/index/star \
	--readFilesIn /home/wangjl/data/test/c13_A1.fa.gz \
	--readFilesCommand gunzip -c \
	--outFileNamePrefix /home/wangjl/data/afterMapping/test3/c14_A1_ \
	--genomeLoad LoadAndKeep \
	--outSAMtype BAM SortedByCoordinate \
	--sjdbOverhang 149

##
--readFilesIn sample_r1.fq.gz sample_r2.fq.gz \#对于PE序列
--readFilesCommand zcat \#如果是gz文件，也可以这样解压缩
--outBAMsortingThreadN 10 \#输出bam的线程数


Note, if the spike-ins are used, the reference sequence should be augmented with the DNA sequence of the spike-in molecules prior to mapping.
注意：如果有spike-ins，参考序列也应该用spike-in分子DNA序列先扩容。

Note, when UMIs are used, their barcodes should be removed from the read sequence. A common practice is to add the barcode to the read name.
注意：使用UMI时，需要去除barcode。通常做法是把barcode加到read的名字上。



2) https://www.biostars.org/p/260069/
还有人建议，多样本比对时，
--genomeLoad LoadAndExit 载入基因组
	$ STAR --genomeLoad LoadAndExit --genomeDir starIndexDirectoryPath
然后for循环比对
	STAR --genomeLoad LoadAndKeep --genomeDir starIndexDirectoryPath --runThreadN nThreads -readFilesIn /pathToReadFile --outFileNamePrefix prefix
--genomeLoad Remove  移除基因组。
	STAR --genomeLoad Remove --genomeDir starIndexDirectoryPath


LoadAndRemove will automatically remove the index from memory once all STAR jobs using it finishes. 
LoadAndExit will leave the index in memory until you run STAR with --genomeLoad Remove.


## 测试实例: 首尾加载、卸载基因组，中间是若干比对过程。 
$ STAR --genomeLoad LoadAndExit --genomeDir /home/wangjl/data/ref/hg19/index/star/ --outFileNamePrefix map/tmp/_load_
$ STAR --runThreadN 10 \
		--outSAMtype BAM SortedByCoordinate  \
		--genomeDir /home/wangjl/data/ref/hg19/index/star/ \
		--readFilesIn clean/c16ROW01_trimmed.fq.gz \
		--readFilesCommand zcat  \
		--genomeLoad LoadAndKeep  \
		--limitBAMsortRAM 20000000000  \
		--outFileNamePrefix map/tmp/c16ROW01_
$ STAR --genomeLoad Remove --genomeDir /home/wangjl/data/ref/hg19/index/star/ --outFileNamePrefix map/tmp/_rm_






(3)改进2：推荐直接输出bam文件，适合py并发使用。
--outSAMtype BAM SortedByCoordinate 输出格式为BAM并排序过。

$ STAR --runThreadN 30  \
	--outSAMtype BAM SortedByCoordinate  \
	--genomeDir /home/wangjl/data/ref/hg19_mm10_transgenes/starIndex  \
	--readFilesIn /home/wangjl/data/apa/190515/trim_galore/c2_ROW01_R2_trimmed.fq.gz  \
	--readFilesCommand zcat  \
	--genomeLoad LoadAndKeep \  #单一会报错
	--limitBAMsortRAM 20000000000 \ #加入这一行就不报错了
	--outFileNamePrefix  /home/wangjl/data/apa/190515/hg19_mm10/c2_ROW01_
#


如果是双端 reads, 最后三个参数是为了提高比对的 uniq map，降低 too short 百分比:
$ STAR --genomeDir /home/niesy/data/reference/star_hg --runThreadN 10 --readFilesIn ATAC-1-3_L4_F0000570F0001151.R1_val_1.fq.gz ATAC-1-3_L4_F0000570F0001151.R2_val_2.fq.gz --readFilesCommand zcat --outFileNamePrefix test --outSAMtype BAM SortedByCoordinate --outBAMsortingThreadN 10 --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --outFilterMismatchNmax 2 








## 另一个例子: 双端测序
a) 用 ensembl的数据吧:
$ cd /home/wangjl/data/ref/mm10/ensembl/
$ wget ftp://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.chr.gtf.gz
$ wget ftp://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz


b) 生成索引
$ STAR --runMode genomeGenerate \
	--runThreadN 50 \
	--genomeDir /home/wangjl/data/ref/mm10/ensembl/STAR/  \
	--genomeFastaFiles /home/wangjl/data/ref/mm10/ensembl/Mus_musculus.GRCm38.dna.primary_assembly.fa \
	--sjdbGTFfile /home/wangjl/data/ref/mm10/ensembl/Mus_musculus.GRCm38.102.chr.gtf \
	--sjdbOverhang 100
# 19:41 - 19:57 = 16min;


c) 批量比对，双端。
$ ls *_1.fastq | tail -n 9 | while read id; do cid=$(basename $id "_1.fastq"); 
echo $cid; 
STAR --runThreadN 30  \
	--outSAMtype BAM SortedByCoordinate  \
	--genomeDir /home/wangjl/data/ref/mm10/ensembl/STAR/  \
	--readFilesIn /home/wangjl/data/apa/20200701Fig/zs/smart_seq2/${cid}_1.fastq /home/wangjl/data/apa/20200701Fig/zs/smart_seq2/${cid}_2.fastq \
	--genomeLoad LoadAndKeep \
	--limitBAMsortRAM 20000000000 \
	--outFileNamePrefix  /home/wangjl/data/apa/20200701Fig/zs/smart_seq2/map/${cid}_
done;







(4) 改进3：如果uniq比对率过低，出现 too short 过高，则可以调整比对参数 
### https://github.com/alexdobin/STAR/issues/169
You can increase the number of mapped reads by relaxing the requirements on the mapped length, e.g.: 
--outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3       
或者下面这个更好:
--outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --outFilterMatchNmin 0
the number of missmaches I controlled by --outFilterMismatchNmax 2 and got my preferred results.


## 参数解释
% of reads unmapped: too short |       21.10%
"Reads too short" doesn't really mean that, it just means "didn't map".

outFilterScoreMinOverLread      0.66
    float: same as outFilterScoreMin, but  normalized to read length (sum of mates' lengths for paired-end reads)


## 效果对比
Uniquely mapped reads % |	63.30%
Uniquely mapped reads % |	65.68%
# STAR 参数加上 --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --outFilterMatchNmin 0 \
Uniquely mapped reads % |	80.54%
Uniquely mapped reads % |	83.70%






(5)更多参数
除了上面常用的一些参数外，STAR的可选参数其实非常多.

输出BAM文件时，STAR还可以对BAM进行一些预处理，"--bamRemoveDuplicatesType"用于去重("UniqueIdentical","UniqueIdenticalNotMulti")

如果你希望输出信号文件(Wig格式),那么需要额外增加 --outWigType参数，如 --outWigType wiggle read2, 还可以用 --outWigStrand指定是否将两条链合并(Stranded, Unstranded), 默认 --outWigNorm RPM，也就是用RPM进行标准化，可以选择None.

如果你在建立索引或者比对的时候增加了注释信息，那么STAR还能帮你进行基因计数。参数为 --quantMode, 分为转录本水平(TranscriptomeSAM)和基因水平(GeneCounts)，在计数的时候还允许指定哪些哪些read不参与计数，"IndelSoftclipSingleend"和"Singleend"

对于非链特异性RNA-seq，同时为了保证能和Cufflinks兼容，需要添加 --outSAMstrandField intronMotif在SAM中增加XS属性，并且建议加上 --outFilterIntronMotifsRemoveNoncanonical。如果是链特异性数据，那么就不需要特别的参数，Cufflinks用 --library-type声明类型即可=

















5. 查看输出文件
log、sam、剪切点注释 三类文件，需要注意的是，sam里第五列 uniquely mapping reads的map质量值是255。


$ ls -lth
total 1.2G
-rw-r--r--. 1 wangjl user 1.9K Jan 17 22:17 c14_A1_Log.final.out
-rw-r--r--. 1 wangjl user  22K Jan 17 22:17 c14_A1_Log.out
-rw-r--r--. 1 wangjl user  364 Jan 17 22:17 c14_A1_Log.progress.out
-rw-r--r--. 1 wangjl user 351K Jan 17 22:17 c14_A1_SJ.out.tab
-rw-r--r--. 1 wangjl user 1.2G Jan 17 22:17 c14_A1_Aligned.out.sam

(1)3个log文件
1)
Log.out: 主要的log文件，对排错和debug很重要。
Log.progress.out: 报告该运行的统计结果，比如处理了多少reads，map上的占百分比。改文件每1min更新一次。
Log.final.out: mapping结束后的map统计结果，对质控很重要。对每个read（单个或双端）分别做统计，然后对全部reads汇总、求平均。
注意：STAR把一个paired-end read计为一个read，不像samtools agstat/idxstats是对每个mate分别计数。
大多信息是关于UNIQUE mappers的，不像samtools agstat/idxstats不区分unique or multi-mappers。
每个splicing都在splices数中计数，这和SJ.out.tab中的汇总一致。

mismatch/indel error rates是按照每个碱基统计的，比如 
total number of mismatches/indels in all unique mappers 除以total number of mapped bases.


2)
#目的：(python on Linux)从star结果文件获取Uniq比对reads数和百分比
import subprocess

#要点： 使用id拼接linux命令。建议都用绝对路径。
def doLinuxCMD(id):
    #cmd1
    cmd="grep 'Uniquely mapped reads number' /home/wangjl/data/apa/190517R/hg19/"+id+"_Log.final.out|cut -d '\t' -f2"
    (status, output)=subprocess.getstatusoutput(cmd)
    rs=str(status)+" "+output;
    
    #cmd2
    cmd2="grep 'Uniquely mapped reads %' /home/wangjl/data/apa/190517R/hg19/"+id+"_Log.final.out|cut -d '\t' -f2"
    (status2, output2)=subprocess.getstatusoutput(cmd2)
    rs2=str(status2)+" "+output2;

    return id+" "+rs+" "+rs2 #返回状态码status2=0表示命令正常执行，其他表示异常，需要查看output推测具体原因

#test 测试
doLinuxCMD('c12_ROW02')  #只需要传入细胞名字即可
##'c12_ROW02 0 2187377 0 72.56%'





(2)1个sam文件
Aligned.out.sam - alignments in standard SAM format.

 - 为了使sam结果和下游Cufflinks or StringTie兼容，要设置 --outSAMattrIHstart 0.(默认是1)
run Cufflinks with the library option --library-type options.

For example, 
$ cufflinks ... --library-type fr-firststrand 
should be used for the standard dUTP protocol, including
Illumina's stranded Tru-Seq. This option has to be used only for Cufflinks runs and not for STAR
runs.
In addition, it is recommended to remove the non-canonical junctions for Cufflinks runs using
 --outFilterIntronMotifs RemoveNoncanonical.

 
(3) "SJ.out.tab"存放的高可信的剪切位点，每一列的含义如下
第一列: 染色体
第二列: 内含子起始（以1为基）
第三列: 内含子结束（以1为基）
第四列：所在链，1(+)，2(-)
第五列: 内含子类型，0表示不是下面的任何一种功能，1表示GT/AG, 2表示:GT/AC,3表示GC/AG,4表示GT/GC,5表示AT/GC,6表示GT/AT
第六列: 是否是已知的注释
第七列: 有多少唯一联配支持
第八列: 有多少多重联配支持
第九列: maximum spliced alignment overhang, 这个比较难以翻译，指的是当短读比对到剪切位点时，中间会被分开，另一边能和基因组匹配的数目，例如ACGTACGT----------ACGT，就是4或者8，取决于方向。

控制过滤的参数为 --outSJfilter*系列，其中 --outSJfilterCountUniqueMin3111表示4类内含子唯一匹配的read支持数至少为3,1,1,1, 而 --outSJfilterCountTotalMin3111则表示4类内含子唯一匹配和多重匹配read的支持数和，至少为3,1,1,1。如果你设置的 --outSJfilterReadsUnique，那么上面两者是等价的，当然默认情况下是 All










refer:
STAR:
https://www.cnblogs.com/Dicor/p/4004819.html
http://www.mamicode.com/info-detail-1163133.html
http://www.bio-info-trainee.com/727.html
d:/ STARmanual.pdf



========================================
|-- Mapping QC: 用RSeQC、deepTools 对比对后的转录组数据进行质控(An RNA-seq Quality Control Package) 5'-3'测序覆盖度
----------------------------------------
RSeQC包是一个python软件


1.目的： 
There are many ways to measure the mapping quality, including: amount of reads mapping to rRNA/tRNAs, proportion of uniquely mapping reads, reads mapping across splice junctions, read depth along the transcripts. 
Reference: * RSeQC: quality control of RNA-seq experiments Bioinformatics (2012) 28 (16): 2184-2185. doi: 10.1093/bioinformatics/bts356




2.安装
$ wget -b https://sourceforge.net/projects/rseqc/files/RSeQC-2.6.4.tar.gz
安装报错，需要python2。算了，还是使用conda切换为python2.7环境，然后

$ pip install RSeQC






3.使用(输入排序后的bam格式文件，要先samtools index产生.bai文件)：http://dldcc-web.brc.bcm.edu/lilab/liguow/CGI/rseqc/_build/html/
中文教程： https://www.jianshu.com/p/f9da70fcaf8d
官方文档: http://rseqc.sourceforge.net/

python <RSeQCpath>/geneBody_coverage.py -i input.bam -r genome.bed -o output.txt
python <RSeQCpath>/bam_stat.py -i input.bam -r genome.bed -o output.txt
python <RSeQCpath>/split_bam.py -i input.bam -r rRNAmask.bed -o output.txt


(1) 对bam文件排序和索引
// 排序
$ ls ../R2bam -S | tail
ACTTTGTCAATTAGGA-1.bam
AAAGTCCAGCAGTCTT-1.bam
...

$ ls ../R2bam -S | head -n 5 | while read id; do 
cb=$(basename $id ".bam");
echo $cb;
samtools sort -@ 5 -o ${cb}.sorted.bam ../R2bam/$id;
done;

// 索引
$ ls *bam| while read id; do 
echo $id;
samtools index ${id};
done;




(2)geneBody_coverage.py
参考基因组bed文件下载 https://www.jianshu.com/p/e0676631bd35

为了保证染色体名字一致(主要是chr1和1的区别)，要下载参考基因组fa来源一致的bed文件。
一共三家UCSC，NCBI，ensemble。
比如从UCSC下载hg19的bed文件：
https://genome.ucsc.edu/cgi-bin/hgTables


警告: 
	- bam必须排序和索引过; 
	- 提供的bed文件和bam文件的染色体名字风格要一致: 都是chr1或者1。
#


运行：
$ geneBody_coverage.py -r hg19.refseq.bed  -i Pairend_nonStrandSpecific_36mer_Human_hg19.bam -o output
	-r 参考基因组bed格式[required]
	-i Alignment file in BAM or SAM format
	-o Prefix of output files(s). [required] 必须要设置前缀
	
#查看帮助文档
$ geneBody_coverage.py --help
-i INPUT_FILES, --input=INPUT_FILES
	Input file(s) in BAM format. 
	"-i" takes these input:
	1) a single BAM file. 
	2) "," separated BAM files. 
	3) directory containing one or more bam files. 
	4) plain text file containing the path of one or more bam file (Each row is a BAM file path). 
	All BAM files should be sorted and indexed using samtools.
#

1)单个bam文件
$ geneBody_coverage.py -r /data/jinwf/wangjl/ref/human/hg19_ucsc_genes-20190509.bed  -i /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bamR2Lh/c2_ROW02.sorted.bam  -o /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/mappingQC_human/c2_ROW02

2)多个bam文件时，文件之间用逗号隔开:
$ cd /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bamR2Lh/
$ geneBody_coverage.py -r /data/jinwf/wangjl/ref/human/hg19_ucsc_genes-20190506.bed  -i c2_ROW01.sorted.bam,c2_ROW02.sorted.bam  -o ../mappingQC_human/c2_ROW0102



# 如何获取逗号隔开的大量文件名？
$ ls -m *bam ##不好：逗号和空格隔开的
AATGCCATCTTCCCAG-1.sorted.bam, ACTTTCAAGCTTAGTC-1.sorted.bam, AGAAGTAAGTGAGCCA-1.sorted.bam
$ ls -m *bam | xargs echo  | sed 's/ //g' 
AATGCCATCTTCCCAG-1.sorted.bam,ACTTTCAAGCTTAGTC-1.sorted.bam,AGAAGTAAGTGAGCCA-1.sorted.bam,ATCCTATCAGAGCCCT-1.sorted.bam,ATGCATGGTGAAGCTG-1.sorted.bam,GGATGTTCACTAACGT-1.sorted.bam,GTACAACGTCTTCGAA-1.sorted.bam,GTGAGGACACACGTGC-1.sorted.bam,TCCTTCTTCCTGGGAC-1.sorted.bam,TGTTTGTCACCGAATT-1.sorted.bam
## 最好还是使用方法4)，把文件名放到一个文本中




3)输入-i也可以提供包含bam文件的文件夹：
$ geneBody_coverage.py -r /home/wangjl/data/ref/hg19/hg19_ucsc_genes-20190509.bed -i ./ -o ./test_


4)-i 后跟文本文件，文本内容是bam的路径，一行一个bam的路径。
可以多开几个窗口，并行运行。


提示: 运行需要的时间很长，一般要过夜。为防止以外中断，请在tmux内运行。
为了加快速度，可以每5个bam文件名放到一个文本文件中，使用 -i bamFile_2.txt 多开几个窗口并行运行。
最后用R重绘图。









4. 新版本 geneBody_coverage2.py，据说很快！(线条太粗糙，不好)
https://github.com/nf-core/rnaseq/issues/195
BAM -> bigWig -> geneBody_coverage2.py
geneBody_coverage2.py 输入 bigWig 文件。


在Ubuntu上安装:
$ pip3 install RSeQC
Successfully installed RSeQC-4.0.0


$ geneBody_coverage2.py
Usage: geneBody_coverage2.py [options]

Calculate the RNA-seq reads coverage over gene body.
This module uses bigwig file as input.

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -i INPUT_FILE, --input-file=INPUT_FILE
                        Coverage signal file in bigwig format
  -r REF_GENE_MODEL, --refgene=REF_GENE_MODEL
                        Reference gene model in bed format. [required]
  -o OUTPUT_PREFIX, --out-prefix=OUTPUT_PREFIX
                        Prefix of output files(s). [required]
  -t GRAPH_TYPE, --graph-type=GRAPH_TYPE
                        Graphic file type in "pdf", "jpeg", "bmp", "bmp",
                        "tiff" or "png".default=pdf [optional]
#

(1) 怎么 bam to bigWig?
bam -> bedGraph -> bigWig: 见 NGS/文件格式

(2) bigWig 做 gene body 覆盖度

$ geneBody_coverage2.py -i KO_1.bw -r /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene.bed -o geneBody2/KO_1_
# 17:35 -> 19:39  4min;
# 效果不好。毛刺太多，曲线像台阶一样，太粗糙。
# 还是用原版吧，虽然慢，但是线条细腻。


耗时对比
## 保证bed和bam中的染色体名字一致，都是带chr或不带。
$ cat /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene.bed | sed 's/^chr//' >/home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene_noChr.bed

$ geneBody_coverage.py -i /home/wangjl/data/ZhangMin/map/ADK_KO_1Aligned.sortedByCoord.out.bam -r /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene_noChr.bed -o geneBody/KO_1_
# 17:30 -> 19:01  1.5h.

还是用原函数，批量化，可能需要到明天出结果了。











5. 太慢了，有替代品吗？qualimap rnaseq 
https://github.com/nf-core/rnaseq/issues/195

结论：没有中间文件，不方便5'-3'覆盖度的二次绘图。
但给出比对的质控，uniq百分比，比对到不同位置的百分比:
    exonic =  32,876,847 (75.15%)
    intronic = 10,286,263 (23.51%)
    intergenic = 585,148 (1.34%)
    overlapping exon = 8,712,959 (19.92%)


(2)$ qualimap --help
rnaseq           Evaluate RNA-seq alignment data
$ qualimap rnaseq --help
usage: qualimap rnaseq [-a <arg>] -bam <arg> -gtf <arg> [-oc <arg>] [-outdir
       <arg>] [-outfile <arg>] [-outformat <arg>] [-p <arg>] [-pe] [-s]

## 运行
$ qualimap rnaseq -bam map/${id}Aligned.sortedByCoord.out.bam -gtf /data/wangjl/ref/mouse/ensembl/Mus_musculus.GRCm38.102.gtf -outdir QC_map/ -outfile ${id}.report.pdf -outformat PDF \
  --java-mem-size=10G

## try1:
WARNING: out of memory!
Qualimap allows to set RAM size using special argument: --java-mem-size
Check more details using --help command or read the manual. 
加上那个内存语句后就正常了。
18:33->18:38，确实快多了。
效果也不太好，图不太清晰，不方便二次绘制。









6. 尝试 deepTools computeMatrix (~20min)
$ deeptools --version
deeptools 3.5.1


(1) 全长RNA-seq样本，gene body 覆盖程度。(要注意物种！物种小鼠)
-rw-rw-r-- 1 wangjl wangjl 2.7G Jun  2 21:30 /home/wangjl/data/ZhangMin/map/ADK_KO_3Aligned.sortedByCoord.out.bam

$ id="ADK_KO_3"
$ bamCoverage -b map/${id}Aligned.sortedByCoord.out.bam -o QC_map/${id}.bw;  ##必须提前index 10:46->10:53(7min)
$ computeMatrix scale-regions  -p 10  \
    -b 2000 -a 2000 \
    -R /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene_noChr.bed \
    -S QC_map/${id}.bw  \
	--regionBodyLength 5000 \
    --skipZeros -o QC_map/matrix/${id}.body.gz #10:56->11:04(8min)
$ plotHeatmap -m QC_map/matrix/${id}.body.gz -out QC_map/matrix/${id}.body.Heatmap.pdf --plotFileFormat pdf

# try1: 第一次的结果很不好，5'-3'图看着乱七八糟的。检查发现 bam里染色体没有chr。
# try2: 重新生成noChr的bed ref文件，再做热图，还是很烂，发现该用小鼠的基因组gtf。
# try3: 正常。但是感觉基因间区有点短。
# try4: 加入 --regionBodyLength 5000 \
# 这个数据不是太好，说是全长，但是 gene body 区域不平坦。


(2) 对比 (巨慢)
$ geneBody_coverage.py --version
geneBody_coverage.py 4.0.0

$ id="ADK_KO_3"
$ geneBody_coverage.py -r /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene_noChr.bed  -i map/${id}Aligned.sortedByCoord.out.bam  -o QC_map/${id}_
10:55->? (3h)




ref:
https://www.jianshu.com/p/2b386dd437d3





========================================
|-- blast: 给定序列，从一个fasta库中查找最相近的序列
----------------------------------------
Wang Zhiwei: 芯片数据，如何根据序列注释出基因名字(gene symbols)？[2020.3.21]
比如: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL18109

hint: https://shengxin.ren/article/442

Me:

1. 找到文章的方法
https://gut.bmj.com/content/63/11/1700.long

Microarray processing and statistical analysis

LncRNA expression profiling was performed using the Agilent human lncRNA+mRNA array V.2.0 platform. After a filtering procedure, 8900 human lncRNAs (annotated by GENCODE(V13) database, lincRNAs from Cabili et al,21 and the University of California Santa Cruz database) were selected for the following analysis (see online supplementary methods). First, quantile normalisation of the microarray data (containing the 8900 lncRNAs and all mRNAs in the microarray) of all 119 paired tumour–normal samples was carried out. Then, the data was log 2-scale transformed. Missing values were imputed using the random Forest unsupervised classification algorithm (see online supplementary methods). The data of the 60 sample pairs in the independent cohort were processed independently in the same way.

(1)只有第2句是如何注释的(GENCODE, Cabili et al, 和UCSC)。注释后过滤，得到8900个lncRNA供后续分析。
After a filtering procedure, 
8900 human lncRNAs 
(annotated by GENCODE(V13) database, lincRNAs from Cabili et al,21 and the University of California Santa Cruz database) 
were selected for the following analysis (see online supplementary methods).



(2)文献21是
Cabili MN, Trapnell C, Goff L, et al. Integrative annotation of human large intergenic noncoding RNAs reveals global properties and specific subclasses. Genes Dev 2011;25:1915–27.


(3)补充材料1 
https://gut.bmj.com/content/gutjnl/suppl/2014/02/12/gutjnl-2013-305806.DC1/gutjnl-2013-305806supp1.pdf

1) Microarray fabrication
30 The Agilent human lncRNA+mRNA Array v2·0 was designed with four identical arrays per slide (4 x
31 180K format). Each array contained probes interrogating about 39,000 human lncRNAs and about
32 32,000 human mRNAs. Each RNA was detected by two probe repeats. The array also contained 4974
33 Agilent control probes


Then, we employed the blast program to map the
40 probes uniquely to the annotated lncRNA sequences, and 8900 lncRNAs with at least one unique 
1 probe were retrieved. For each of the 8900 lncRNAs, t

2)使用pubmed搜索 blast，第一个是网页版
https://blast.ncbi.nlm.nih.gov/Blast.cgi
其中有:
Standalone and API BLAST
Download BLAST
Get BLAST databases and executables

点击进入下载页面
https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=Download
看到标题: Download BLAST Software and Databases


3)软件下载: ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/
BLAST+ user manual, https://www.ncbi.nlm.nih.gov/books/NBK279690/
the BLAST Help manual, https://www.ncbi.nlm.nih.gov/books/NBK1762/

$ cd /home/wangjl/data/soft
## wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.10.0+-x64-linux.tar.gz
$ axel -n 30 ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.10.0+-x64-linux.tar.gz
## 并行下载更快


## 下载很慢，但是我发现我已经安装过
	$ blastn -version
	blastn: 2.6.0+
	Package: blast 2.6.0, build Jan 15 2017 17:12:27
	安装位置:
	$ whereis blastn
	blastn: /usr/bin/blastn
## 版本太老，好像不能下载官方库了。决定还是重新安装吧


解压后，进入bin目录: /data/wangjl/soft/ncbi-blast-2.10.0+/bin

加入到path中；
$ vim ~/.bashrc
末尾加一行并保存:
export PATH="/data/wangjl/soft/ncbi-blast-2.10.0+/bin":$PATH

更新path
$ source ~/.bashrc

查看版本号，已经是最新版了
$ blastp -version
blastp: 2.10.0+
 Package: blast 2.10.0, build Dec  3 2019 18:03:18



4)库
Download the databases you need,(see database section below), or create your own. Start searching.

ftp://ftp.ncbi.nlm.nih.gov/blast/
ftp://ftp.ncbi.nlm.nih.gov/blast/db/


查看有哪些库
$ perl update_blastdb.pl --showall
	Connected to NCBI
	16S_ribosomal_RNA
	18S_fungal_sequences
	28S_fungal_sequences
	Betacoronavirus
	ITS_RefSeq_Fungi
	ITS_eukaryote_sequences
	LSU_eukaryote_rRNA
	LSU_prokaryote_rRNA
	SSU_eukaryote_rRNA
	landmark
	nr 蛋白库
	nt 核酸库
	patnt
	pdbaa
	pdbnt
	ref_euk_rep_genomes
	ref_prok_rep_genomes
	ref_viroids_rep_genomes
	ref_viruses_rep_genomes
	refseq_protein
	refseq_rna
	swissprot
	taxdb
#
Contents of the /blast/db/ directory
The pre-formatted BLAST databases are archived in this directory. The names of these databases and their contents are listed below.
########################
File Name        #  Content Description   
16SMicrobial.tar.gz          #  Bacterial and Archaeal 16S rRNA sequences from BioProjects 33175 and 33117
FASTA/        #  Subdirectory for FASTA formatted sequences
README        #  README for this subdirectory (this file)
Representative_Genomes.*tar.gz        #  Representative bacterial/archaeal genomes database
cdd_delta.tar.gz          #  Conserved Domain Database sequences for use with stand alone deltablast
cloud/          #  Subdirectory of databases for BLAST AMI; see http://1.usa.gov/TJAnEt
env_nr.*tar.gz        #  Protein sequences for metagenomes
env_nt.*tar.gz        #  Nucleotide sequences for metagenomes
est.tar.gz        #  This file requires est_human.*.tar.gz, est_mouse.*.tar.gz, and est_others.*.tar.gz files to function. It contains the est.nal alias so that searches against est (-db est) will include est_human, est_mouse and est_others. 
est_human.*.tar.gz        #  Human subset of the est database from the est division of GenBank, EMBL and DDBJ.
est_mouse.*.tar.gz        #  Mouse subset of the est databasae
est_others.*.tar.gz           #  Non-human and non-mouse subset of the est database
gss.*tar.gz           #  Sequences from the GSS division of GenBank, EMBL, and DDBJ
htgs.*tar.gz          #  Sequences from the HTG division of GenBank, EMBL,and DDBJ
human_genomic.*tar.gz         #  Human RefSeq (NC_) chromosome records with gap adjusted concatenated NT_ contigs
nr.*tar.gz        #  Non-redundant protein sequences from GenPept, Swissprot, PIR, PDF, PDB, and NCBI RefSeq
nt.*tar.gz        #  Partially non-redundant nucleotide sequences from all traditional divisions of GenBank, EMBL, and DDBJ excluding GSS,STS, PAT, EST, HTG, and WGS.
other_genomic.*tar.gz         #  RefSeq chromosome records (NC_) for non-human organisms
pataa.*tar.gz         #  Patent protein sequences
patnt.*tar.gz         #  Patent nucleotide sequences. Both patent databases are directly from the USPTO, or from the EPO/JPO via EMBL/DDBJ
pdbaa.*tar.gz         #  Sequences for the protein structure from the Protein Data Bank
pdbnt.*tar.gz         #  Sequences for the nucleotide structure from the Protein Data Bank. They are NOT the protein coding sequences for the corresponding pdbaa entries.
refseq_genomic.*tar.gz        #  NCBI genomic reference sequences
refseq_protein.*tar.gz        #  NCBI protein reference sequences
refseq_rna.*tar.gz        #  NCBI Transcript reference sequences
sts.*tar.gz           #  Sequences from the STS division of GenBank, EMBL,and DDBJ
swissprot.tar.gz          #  Swiss-Prot sequence database (last major update)
taxdb.tar.gz          #  Additional taxonomy information for the databases listed here providing common and scientific names
tsa_nt.*tar.gz        #  Sequences from the TSA division of GenBank, EMBL,and DDBJ
vector.tar.gz         #  Vector sequences from 2010, see Note 2 in section 4.
wgs.*tar.gz           #  Sequences from Whole Genome Shotgun assemblies





如何下载某个库?
NCBI提供了一个非常智能化的脚本update_blastdb.pl来自动下载所有blast数据库。
$ perl update_blastdb.pl nt

更优化的命令
$ nohup perl update_blastdb.pl --decompress nt >out.log 2>&1 &
自动在后台下载，然后自动解压。（下载到一半断网了，再运行会接着下载，而不会覆盖已经下载好的文件）





(4) blast搜索过程

1) blast构建索引 (makeblastdb)
https://www.ncbi.nlm.nih.gov/books/NBK279688/

如果我们下载的是已经建好索引的数据库，可以省去makeblastdb的过程。

$ makeblastdb -in mature.fa -input_type fasta -dbtype nucl -title miRBase -parse_seqids -out miRBase -logfile File_Name

-in 后接输入文件，你要格式化的fasta序列
-dbtype 后接序列类型，nucl为核酸，prot为蛋白
-title 给数据库起个名，好看~~(不能用在后面搜索时-db的参数)
-parse_seqids 推荐加上，现在有啥原因还没搞清楚
-out 后接数据库名，自己起一个有意义的名字，以后blast+搜索时要用到的-db的参数
-logfile 日志文件，如果没有默认输出到屏幕




2) 资源消耗 
常见的命令参数有下面几个：
	-query <File_In> 要查询的核酸序列
	-db <String> 数据库名字
	-out <File_Out> 输出文件
	-evalue <Real> evalue阈值
	-outfmt <String> 输出的格式

$ blastx -query test.merged.transcript.fasta -db nr -out test.blastx.out

其中fasta文件只有19938行。
可是运行起来耗费了很多资源：
	平均内存消耗：51.45G；峰值：115.37G
	cpu：1个
	运行时间：06:00:24（你敢信？这才是一个小小的test）
所以我强烈推荐用diamond替代blast来做数据库搜索。(??)


3) blast结果解读
每一个合格的序列比对都会给出一个这样的结果（一个query sequence比对到多个就有多个结果）：

	>AAB70410.1 Similar to Schizosaccharomyces CCAAT-binding factor (gb|U88525).
	EST gb|T04310 comes from this gene [Arabidopsis thaliana]
	Length=208
	 
	 Score = 238 bits (607),  Expect = 7e-76, Method: Compositional matrix adjust.
	 Identities = 116/145 (80%), Positives = 127/145 (88%), Gaps = 2/145 (1%)
	 Frame = +1
	 
	Query  253  FWASQYQEIEQTSDFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR  432
				FW +Q++EIE+T+DFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR
	Sbjct  39   FWENQFKEIEKTTDFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR  98
	 
	Query  433  SWNHTEENKRRTLQKNDIAAAITRNEIFDFLVDIVPREDLKDEVLASIPRGTLPMGAPTE  612
				SWNHTEENKRRTLQKNDIAAA+TR +IFDFLVDIVPREDL+DEVL SIPRGT+P  A
	Sbjct  99   SWNHTEENKRRTLQKNDIAAAVTRTDIFDFLVDIVPREDLRDEVLGSIPRGTVPEAA-AA  157
	 
	Query  613  GLPYYYMQPQHAPQVGAPGMFMGKP  687
				G PY Y+    AP +G PGM MG P
	Sbjct  158  GYPYGYLPAGTAP-IGNPGMVMGNP  181　　

结果解读网上很多，这里不啰嗦了。

以下是我在同样条件下测试的diamond：
	平均内存消耗：11.01G；峰值：12.44G
	cpu：1个（571.17%）也就是会自动占用5-6个cpu
	运行时间：00:26:15
	而且diamond注明了，它的优势是处理>1M 的query，量越大速度越快。

diamond的简单用法：
diamond makedb --in nr.fa -d nr
diamond blastx -d nr -q test.merged.transcript.fasta -o test.matches.m8
但是diamond使用有限制，只能用于比对蛋白数据库。





############
# 实例测试
############
(5) 获得序列文件
1)准备库文件所需的fasta
$ grep -v '^[!^#]' GPL18109_family.soft >GPL18109.txt  #去掉注释行
$ awk '$6!="" {print $0}' GPL18109.txt > GPL18109_seq.txt #只保留有序列的行
$ cat GPL18109_seq.txt | awk 'NR>1{print ">"$1"|"$2"|"$3"|"$4"\n"$6}' > GPL18109.fa #一行转为2行，fasta格式
$ cat GPL18109_seq.txt | awk 'NR>1{print ">"$1"\n"$6}' > GPL18109.fa #一行转为2行，fasta格式
行数 143032

2)建库(时间: 2s)
$ makeblastdb -in GPL18109.fa -input_type fasta -dbtype nucl -title probeDB -parse_seqids -out probeDB -logfile probeDB.log
输出log为
$ cat probeDB.log
	Building a new DB, current time: 03/21/2020 21:51:53
	New DB name:   /data/wangjl/blast_data/probeDB
	New DB title:  probeDB
	Sequence type: Nucleotide
	Keep MBits: T
	Maximum file size: 1000000000B
	Adding sequences from FASTA; added 71516 sequences in 0.626761 seconds.

3)比对
##   tail GPL18109.fa >test.fasta #造数据，并为每一行>后添加前缀AA_
$ blastn -query test.fasta -db probeDB -out test.blastn.out


4)查看比对结果
$ cat test.blastn.out | grep -P "^(Query=|>)"
Query= AA_4064|521|33|Y4-New #这相当于未知序列和编号
>4064   #这相当于库中找到的条目id

Query= AA_4970|518|261|Y5-New
>4970
Query= AA_4677|519|167|Y6-New
>4677
Query= AA_5412|517|57|Y7-New
>5412
Query= AA_4497|519|188|Y8-New
>4497


(6) 准备lincRNA库文件所需的fasta格式，统一用hg38;
1) gencode, Release 33 (GRCh38.p13)
https://www.gencodegenes.org/human/
最相近的似乎是
Long non-coding RNA transcript sequences: Nucleotide sequences of long non-coding RNA transcripts on the reference chromosomes

$ cd /data/wangjl/lincRNADB
$ wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/gencode.v33.lncRNA_transcripts.fa.gz

## 2020-03-21 22:16:33 (822 KB/s) - ‘gencode.v33.lncRNA_transcripts.fa.gz’ saved [14647143]

#行数
$ zcat gencode.v33.lncRNA_transcripts.fa.gz |wc
1137813 1137813 70303664


$ gunzip gencode.v33.lncRNA_transcripts.fa.gz ##/home/wangjl/data/lincRNADB ## 1137813行
$ makeblastdb -in gencode.v33.lncRNA_transcripts.fa -input_type fasta -dbtype nucl -title genCodeDB -parse_seqids -out genCodeDB -logfile genCodeDB.log
## 报错 BLAST Database creation error: Near line 1, the local id is too long.  Its length is 110 but the maximum allowed local id length is 50.  Please find and correct all local ids that are too long. 
## 就是第一行太长了，最多50个字符，而这个已经100个了
$ head gencode.v33.lncRNA_transcripts.fa
>ENST00000473358.1|ENSG00000243485.5|OTTHUMG00000000959.2|OTTHUMT00000002840.1|MIR1302-2HG-202|MIR1302-2HG|712|
GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCCTACCCGTGCTTTCT

大于号开头的共 48438 行;
使用python替换掉这些序列: row后面编号
替换后字典文件:
$ head gencode.v33.lncRNA_transcripts_codeNameMap.txt
Row1    ENST00000473358.1       ENSG00000243485.5       OTTHUMG00000000959.2    OTTHUMT00000002840.1    MIR1302-2HG-202 MIR1302-2HG     712
Row2    ENST00000469289.1       ENSG00000243485.5       OTTHUMG00000000959.2    OTTHUMT00000002841.2    MIR1302-2HG-201 MIR1302-2HG     535

fa文件:
$ head gencode.v33.lncRNA_transcripts_short.fa
>Row1
GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCCTACCCGTGCTTTCT

建库: 2s
$ makeblastdb -in gencode.v33.lncRNA_transcripts_short.fa -input_type fasta -dbtype nucl -title genCodeDB -parse_seqids -out genCodeDB -logfile genCodeDB.log

未知信息的序列:(来自于基因芯片) 143032行
$ head ../blast_data/GPL18109.fa
>12414
TTTACCTCGGTGTCCTACCAGCAAGGGGTCCTGTCTGCCACCATCCTCTATGAGATCCTG
>13811
TCTACAGTCTTGAAGAACGGGTTGAAAACAACAGTGTGCCAAGTCGCTTCTCACCTGAAT


比对: (20s) 
$ blastn -query ../blast_data/GPL18109.fa -db genCodeDB -out GPL18109.blastn.out

提取结果:
$ cat GPL18109.blastn.out | grep -P "^(Query=|ROW)" >GPL18109.blastn.out2 #144384行
$ head GPL18109.blastn.out2
Query= 12414 #这一行是未知序列，下面没有>表示没有在库中查到，***** No hits found *****
Query= 13811
Query= 12228
Query= 33906
Query= 653 #这一行是未知序列
ROW37443       111        5e-25 #这行是查到的结果
Query= 108
ROW29246       111        5e-25 #如果查到多个序列，默认取第一个
ROW29245       111        5e-25
...
Query= 79840  
ROW35254       111        5e-25
ROW40969       84.2       1e-16 #第一个e值最小
ROW31327       84.2       1e-16
ROW16715       84.2       1e-16


原比对结果
	Query= 108
	Length=60
												   Score        E
	Sequences producing significant alignments:   (Bits)     Value
												  
	ROW29246                                       111        5e-25
	ROW29245                                       111        5e-25


	>ROW29246
	Length=298

	 Score = 111 bits (60),  Expect = 5e-25
	 Identities = 60/60 (100%), Gaps = 0/60 (0%)
	 Strand=Plus/Plus

	Query  1    ATGAGCAGTGACTTTCTACTCTCCATCCAGCAGCCAACAATAGACTTCGTCATTCAGATG  60
				||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
	Sbjct  196  ATGAGCAGTGACTTTCTACTCTCCATCCAGCAGCCAACAATAGACTTCGTCATTCAGATG  255


使用python把未知序列和已知序列做一一对应，在库中查到多个的，取第一个，查不到的放弃;
13089 行输出
$ head GPL18109_gencode.csv #第一列是芯片的第一列，后面的是gencode fasta第一行的信息
653,Row37443,ENST00000563906.1,ENSG00000261538.1,OTTHUMG00000175441.1,OTTHUMT00000429982.1,AC096996.2-201,AC096996.2,426
108,Row29246,ENST00000638100.1,ENSG00000283138.1,OTTHUMG00000191673.1,OTTHUMT00000489239.1,AC006207.1-202,AC006207.1,298







2) UCSC 
http://genome.ucsc.edu/cgi-bin/hgTables

TUCP(Transcripts with Unknown Coding Potential)
?


ref:
[1]构建NCBI本地BLAST数据库 (NR NT等) | blastx/diamond使用方法 | blast构建索引 | makeblastdb
https://www.cnblogs.com/leezx/p/6425620.html
[2]五分钟搞定一个芯片的重注释，让那些没有genesymbol的数据再次好用
https://shengxin.ren/article/442



========================================
RNA表达量的定量
----------------------------------------

========================================
|-- 定量比对结果 HTseq-count
----------------------------------------
1. 概述
https://www.cnblogs.com/timeisbiggestboss/p/7171535.html

HTSeq:一个用于处理高通量数据（High-throughout sequencing)的python包。
HTSeq包有很多功能类，熟悉python脚本的可以自行编写数据处理脚本。
另外，HTSeq也提供了两个脚本文件能够直接处理数据:htseq-qa(检测数据质量)和htseq-count（reads计数）。
文档：http://htseq.readthedocs.io/

htseq-count用于reads计数的轻便软件。貌似所有能转换为sam格式文件的输出都可以用htseq-count计数。

htseq-count的输入文件
输入为sam/bam格式的文件，如果是paired-end数据必须按照reads名称排序（sort by name）。官方推荐了msort，不过我用起来感觉不是很方便（也可能是使用方法不当），于是我采用了samtools先对bam文件（tophat2的输出结果为bam）排序，再转换为sam。
命令：samtools sort -n file.bam #sort bam by name
      samtools view -h bamfile.bam>samfile.sam
其实 htseq-count 加上 -f bam 参数就可以使用bam格式的输入。




2.下载和安装
$ pip -V
pip 9.0.1 from /home/wangjl/software/anoconda3/lib/python3.6/site-packages (python 3.6)

$ pip insatll htseq
几秒钟后安装好了。

版本号
$ htseq-count -h
...
version 0.11.2.


(2) 下载gtf数据
https://htseq.readthedocs.io/en/release_0.11.1/count.html
Q: I have used a GTF file generated by the Table Browser function of the UCSC Genome Browser, and most reads are counted as ambiguous. Why?
A: In these files, the gene_id attribute incorrectly contains the same value as the transcript_id attribute and hence a different value for each transcript of the same gene. Hence, if a read maps to an exon shared by several transcripts of the same gene, this will appear to htseq-count as and overlap with several genes. Therefore, these GTF files cannot be used as is. Either correct the incorrect gene_id attributes with a suitable script, or use a GTF file from a different source.

从UCSC Table Browser下载的gtf文件不能直接用到htseq-count中，因为gene_id和transcript_id一样，导致同一个gene有不同的value。所以，如果一个read map到多个转录本共享的exon上时，htseq-count就认为和几个基因重叠了。所以，这些gtf文件不能直接使用，要么使用脚本纠正错误的gene_id，或者从不同的途径下载GTF文件。

chr1    hg19_knownGene  exon    13221   14409   0.000000        +       .       gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; 
chr1    hg19_knownGene  exon    11874   12227   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  exon    12646   12697   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  exon    13221   14409   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  start_codon     12190   12192   0.000000        +       .       gene_id "uc010nxq.1"; transcript_id "uc010nxq.1"; 

修改为：
chr1	stdin	transcript	11874	14409	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; 
chr1	stdin	exon	11874	12227	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "1"; exon_id "uc001aaa.3.1";
chr1	stdin	exon	12613	12721	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "2"; exon_id "uc001aaa.3.2";
chr1	stdin	exon	13221	14409	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "3"; exon_id "uc001aaa.3.3";
chr1	stdin	transcript	11874	14409	.	+	.	gene_id "DDX11L1"; transcript_id "uc010nxr.1"; 
chr1	stdin	exon	11874	12227	.	+	.	gene_id "DDX11L1"; transcript_id "uc010nxr.1"; exon_number "1"; exon_id "uc010nxr.1.1";


(2.1)怎么修改？







(2.2)
https://www.gencodegenes.org/human/release_30lift37.html
下载gtf文件和Gene symbol吧。

如何修改gtf文件？
https://www.gencodegenes.org/pages/data_format.html
python版： https://github.com/openvax/gtfparse







3. 用法：
(1)常用
$ htseq-count -h
usage: htseq-count [options] alignment_file gff_file
...
<alignment_file> :contains the aligned reads in the SAM format.
	Make sure to use a splicing-aware aligner such as TopHat.
	To read from standard input, use - as <alignment_file>.

$ cd /home/wangjl/data/afterMapping/quantify
$ htseq-count ../c12_A1_Aligned.out.sam /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid.gtf > htseq_c12_A1.sam.count 2>htseq_c12_A1.sam.count.log
$ ls -lth
total 276K
-rw-r--r--. 1 wangjl user 270K Jan 19 16:58 htseq_c12_A1.sam.count
-rw-r--r--. 1 wangjl user 1.8K Jan 19 16:58 htseq_c12_A1.sam.count.log


警告！如果是bam文件，一定要加-f bam ，否则获得的表达数据全是0！


(2)更多参数
usage: htseq-count [options] alignment_file gff_file

-f {sam,bam}  (default: sam)
-r {pos,name}  (default: name) 比对文件的排序方式。PE数据必须按照pos或者name排序，且必须明确指定。SE忽略该设置。
-s {yes,no,reverse}  (default: yes) reads是否匹配到同一条链上。 #此处关于选项-s为我自己的认识，不一定对
    #数据是否来源于链特异性测序，链特异性是指在建库测序时，只测mRNA反转录出的cDNA序列，而不测该cDNA序列反向互补的另一条DNA序列；换句话说就是，链特异性能更准确反映出mRNA的序列信息
    #我们知道在gff/gtf中第7列是+-信息，+表示来源于参考基因组序列正链，-表示参考基因组序列的反向互补链
    #sam/bam文件的第2列是flag信息，也可以看出比对到正链还是负链
    #stranded=no，无链特异性，一条reads通过flag列知道比对到+还是-链后，不管是不是和gff/gtf相匹配，都算是这个feature中的
    #stranded=yes, 且se测序，要和gff/gtf相匹配，才算是这个feature中的
    #stranded=yes, 且pe测序，要和gff/gtf相匹配，才算是这个feature中的
    #stranded=reverse，是yes的相反，这时不是和gff/gtf相匹配了，而是恰好相反，可能源于另一种链特异性，只测cDNA序列反向互补的另一条DNA序列
-a MINAQUAL (default: 10)
    #忽略比对质量低于此值的比对结果
-t feature type 外显子为最小的定义单位，对基因计数，只需要将包含的外显子计数相加即可。 
    #feature type (3rd column in GFF file) to be used, all features of other type are ignored (default, suitable for Ensembl GTF files: exon)
    #没想到这个还能自己设置
-i IDATTR 最终的计数单位，一般为基因。 默认为：gene_id   也可以设置转录本，但由于模型问题，计数效果不佳。
    #GFF attribute to be used as feature ID (default, suitable for Ensembl GTF files: gene_id)
-m {union,intersection-strict,intersection-nonempty} (default: union) 计数模型，统计reads的时候对一些比较特殊的reads定义是否计入。具体说明如官网图所示。


note:
加上 -i gene_name 是不是就可以输出gene symbol了？测试一下，确实可以。
然后使用 gencode.v30lift37.metadata.HGNC 第二列的HGNC基因3.7万个取交集： awk '{print $2}' gencode.v30lift37.metadata.HGNC|sort|uniq -c|wc
37409   74818  586179





4.输出结果
$ ls *count
SRR3286802.count  SRR3286803.count  SRR3286804.count  SRR3286805.count  SRR3286806.count  SRR3286807.count

#基于相同gff/gtf得到的计数文件，行数相同，第一列（基因名）相同
$ wc -l *count
  37889 SRR3286802.count
  37889 SRR3286803.count
  37889 SRR3286804.count
  37889 SRR3286805.count
  37889 SRR3286806.count
  37889 SRR3286807.count

#且最后5列统计了整个计数过程没有使用到的reads
$ tail -n 5 SRR3286802.count
__no_feature    237560
__ambiguous 1846779
__too_low_aQual 0
__not_aligned   1323985
__alignment_not_unique  2015872

####
their alignment did not overlap any gene (no_feature).
their alignment overlapped with more than one gene (ambiguous);
their alignment quality was lower than the user-specified threshold (too_low_aQual);
they did not align at all (not_aligned);
based on the NH tag in the BAM file, they aligned to more than one place in the reference genome (alignment_not_unique);





http://www.bio-info-trainee.com/244.html





========================================
|-- 定量比对结果 featureCounts: a ultrafast and accurate read summarization program
----------------------------------------
1. featurecounts具有count速度快，兼容性好的特点。
http://blog.sciencenet.cn/home.php?mod=space&uid=2609994&do=blog&id=985692

发现Subread是个功能很全面的软件，而且还有相对应的R包Rsubread。二进制包直接可用。
http://www.360doc.com/content/18/0112/01/50153987_721213961.shtml


软件的作者认为其软件的优点在于（我就复制黏贴了）：

It carries out precise and accurate read assignments by taking care of indels, junctions and structural variants in the reads
It takes only half a minute to summarize 20 million reads（真是快。。。）
It supports GTF and SAF format annotation
It supports strand-specific read counting
It can count reads at feature (eg. exon) or meta-feature (eg. gene) level
Highly flexible in counting multi-mapping and multi-overlapping reads. Such reads can be excluded, fully counted or fractionally counted（这点跟HTSeq-count不一样了，其对于多重比对的reads并不是只采用全部丢弃的策略，按照其说法是更加灵活的对待）
It gives users full control on the summarization of paired-end reads, including allowing them to check if both ends are mapped and/or if the fragment length falls within the specified range（可让使用者更加个性化的使用）
Reduce ambuiguity in assigning read pairs by searching features that overlap with both reads from the pair
It allows users to specify whether chimeric fragments should be counted（考虑的有点周到）
Automatically detect input format (SAM or BAM)
Automatically sort paired-end reads. Users can provide either location-sorted or namesorted bams files to featureCounts. Read sorting is implemented on the fly and it only incurs minimal time cost


2. 可用性和实现：featureCounts作为Subread（http://www.sourceforge.net/projects/subread）或Rsubread（http://www.bioconductor.org/packages/release/bioc/html/Rsubread.html）软件包的一部分


(1) github 
https://github.com/ShiLab-Bioinformatics/subread

$ wget https://github.com/ShiLab-Bioinformatics/subread/releases/download/2.0.2/subread-2.0.2-Linux-x86_64.tar.gz

发现了个之前安装的:
$ featureCounts -v
featureCounts v2.0.1





(2) 之前的
https://sourceforge.net/projects/subread/
$ wget -b https://sourceforge.net/projects/subread/files/subread-1.6.0/subread-1.6.0-Linux-x86_64.tar.gz/download
$ mv download subread-1.6.0-Linux-x86_64.tar.gz
$ tar zxvf subread-1.6.0-Linux-x86_64.tar.gz
$ vim ~/.bashrc 
在该文件结尾增加一行
export PATH=/home/wangjl/software/subread-1.6.0-Linux-x86_64/bin:$PATH
保存后加载改文件，使其生效。
$ source ~/.bashrc

$ featureCounts -v
featureCounts v1.6.0





3. 使用：
$ featureCounts -h 参数有点多。

官方简单教程如下：
$featureCounts -T 6 -p -t exon -g gene_id -a ~/annotation/mm10/gencode.vM13.annotation.gtf -o SRR3589959_featureCounts222.txt SRR3589959.bam

主要的参数：
-a 输入GTF/GFF基因组注释文件
-p 这个参数是针对paired-end数据
-F 指定-a注释文件的格式，默认是GTF
-g 从注释文件中提取Meta-features信息用于read count，默认是gene_id
-t 跟-g一样的意思，其是默认将exon作为一个feature
-o 输出文件
-T 多线程数

其他参数介绍只能看文档了，不常用的话也是记不住的，要用时再去翻就行
运行中和运行后有两张图可以看看，主要讲了其运行中的一些信息，如下：




例子：
# include multimapping
<featureCounts_path>/featureCounts -O -M -Q 30 -p -a genome.gtf -o outputfile input.bam
# exclude multimapping
<featureCounts_path>/featureCounts -Q 30 -p -a genome.gtf -o outputfile input.bam




测试：
$ featureCounts -O -M -Q 30 -p -a genome.gtf -o outputfile input.bam
-O Assign reads to all their overlapping meta-features (or features if -f is specified).
-M Multi-mapping reads will also be counted. For a multi-mapping read, all its reported alignments will be counted. The 'NH' tag in BAM/SAM input is used to detect multi-mapping reads.
-Q <int>    The minimum mapping quality score a read must satisfy in order to be counted. For paired-end reads, at least one end should satisfy this criteria. 0 by default.
-p        If specified, fragments (or templates) will be counted instead of reads. This option is only applicable for paired-end reads.
-a <string>   Name of an annotation file. GTF/GFF format by default.
        See -F option for more format information. Inbuilt annotations (SAF format) is available in 'annotation' directory of the package.


$ featureCounts -T 5 -O -M -a /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid.gtf -o /home/wangjl/data/afterMapping/quantify2/fC_c12_A1.sam.count ../c12_A1_Aligned.out.sam 2>fC_c12_A1.sam.count.log

输出文件的解释：
[wangjl@nih_jin quantify2]$ ls -lth
total 19M
-rw-r--r--. 1 wangjl user  19M Jan 19 21:54 fC_c12_A1.sam.count
-rw-r--r--. 1 wangjl user 3.6K Jan 19 21:54 fC_c12_A1.sam.count.log
-rw-r--r--. 1 wangjl user  330 Jan 19 21:54 fC_c12_A1.sam.count.summary

(1).从log也就是屏幕输出可见，输出为
[wangjl@nih_jin quantify2]$ cat *log

==========     _____ _    _ ____  _____  ______          _____
=====         / ____| |  | |  _ \|  __ \|  ____|   /\   |  __ \
  =====      | (___ | |  | | |_) | |__) | |__     /  \  | |  | |
    ====      \___ \| |  | |  _ <|  _  /|  __|   / /\ \ | |  | |
      ====    ____) | |__| | |_) | | \ \| |____ / ____ \| |__| |
==========   |_____/ \____/|____/|_|  \_\______/_/    \_\_____/
  v1.6.0

//  featureCounts setting  \\
||                                                                            ||
||             Input files : 1 SAM file                                       ||
||                           S ../c12_A1_Aligned.out.sam                      ||
||                                                                            ||
||             Output file : /home/wangjl/data/afterMapping/quantify2/fC_ ... ||
||                 Summary : /home/wangjl/data/afterMapping/quantify2/fC_ ... ||
||              Annotation : /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_ge ... ||
||      Dir for temp files : /home/wangjl/data/afterMapping/quantify2         ||
||                                                                            ||
||                 Threads : 5                                                ||
||                   Level : meta-feature level                               ||
||              Paired-end : no                                               ||
||         Strand specific : no                                               ||
||      Multimapping reads : counted                                          ||
|| Multi-overlapping reads : counted                                          ||
||   Min overlapping bases : 1                                                ||
||                                                                            ||
\\  http://subread.sourceforge.net/  //

//  Running  \\
||                                                                            ||
|| Load annotation file /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid. ... ||
||    Features : 742585                                                       ||
||    Meta-features : 28610                                                   ||
||    Chromosomes/contigs : 152                                               ||
||                                                                            ||
|| Process SAM file ../c12_A1_Aligned.out.sam...                              ||
||    Single-end reads are included.                                          ||
||    Assign reads to features...                                             ||
||    Total reads : 3353091                                                   ||
||    Successfully assigned reads : 2732203 (81.5%)                           ||
||    Running time : 0.04 minutes                                             ||
||                                                                            ||
||                         Read assignment finished.                          ||
||                                                                            ||
|| Summary of counting results can be found in file "/home/wangjl/data/after  ||
|| Mapping/quantify2/fC_c12_A1.sam.count.summary"                             ||
||                                                                            ||
\\  http://subread.sourceforge.net/  //

Successfully assigned reads : 2732203 (81.5%)  说明有81.5%定位到基因上了。
其余的为什么没有定位上？请看summary文件。

(2) $ cat *summary
Status  ../c12_A1_Aligned.out.sam
Assigned        2732203
Unassigned_Unmapped     0
Unassigned_MappingQuality       0
Unassigned_Chimera      0
Unassigned_FragmentLength       0
Unassigned_Duplicate    0
Unassigned_MultiMapping 0
Unassigned_Secondary    0
Unassigned_Nonjunction  0
Unassigned_NoFeatures   620888
Unassigned_Overlapping_Length   0
Unassigned_Ambiguity    0

(3)运行速度？没的说，仅仅Running time : 0.04 minutes！比HTseq快了一个数量级。
fC_c12_A1.sam.count 文件包含了很多杂乱的信息，如果想了解每个基因上的count数，则只需要提取出第1列和第7列的信息
$ cut -f 1,7 fC_c12_A1.sam.count |grep -v '^#' >fC_c12_A1.sam.count.lite





Citation
We have published papers on our Subread/Subjunc read aligners and featureCounts read quantifiers.

The Subread aligner: fast, accurate and scalable read mapping by seed-and-vote, Y Liao, GK Smyth, W Shi, Nucleic acids research, 2013 PMID:23558742

featureCounts: an efficient general purpose program for assigning sequence reads to genomic features, Y Liao, GK Smyth, W Shi, Bioinformatics, 2014 PMID:24227677

The R package Rsubread is easier, faster, cheaper and better for alignment and quantification of RNA sequencing reads, Y Liao, GK Smyth, W Shi, Nucleic acids research, 2019 PMID:30783653






========================================
|-- 定量比对结果 RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome //todo1
----------------------------------------
1. 
paper: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-323
https://deweylab.github.io/RSEM/

安装
wget https://github.com/deweylab/RSEM/archive/v1.3.1.tar.gz
mv v1.3.1.tar.gz RSEM-v1.3.1.tar.gz
tar zxvf RSEM-v1.3.1.tar.gz
cd RSEM-1.3.1/
#To compile RSEM, simply run
make
#To install RSEM, simply put the RSEM directory in your environment's PATH variable. Alternatively, run
make install
or 
#make install DESTDIR=/home/my_name prefix=/software #will install RSEM executables to /home/my_name/software/bin.
make install DESTDIR=/home/ prefix=/wangjl #$ ls ~/bin 已经安装了
rsem-calculate-expression #- Estimate gene and isoform expression from RNA-Seq data.


docs: https://github.com/deweylab/RSEM




2. 使用笔记
RSEM，老牌的工具依旧是笔者的第一选择
原创： 老牛哥哥  生信草堂  8月17日


Alignment-based的转录本定量-RSEM
http://www.bioinfo-scrounger.com/archives/482











========================================
|-- 使用Salmon对RNAseq进行直接定量 //todo How?
----------------------------------------
无需mapping，直接对RNA结果fq文件进行定量。
手册：https://combine-lab.github.io/salmon/getting_started/



使用 salmon 直接对 fq 进行定量。
注意：salmon 产生估计的read计数和估计的转录本每百万( transcripts per million (tpm))，
按照我们的经验，后者过于纠正scRNASeq中的长基因的表达，所以推荐使用read计数。

$ cd /home/wangjl/data/afterMapping/quantify2



1. Salmon is a tool for wicked-fast transcript quantification from RNA-seq data. 
官网：https://combine-lab.github.io/salmon/
文档：http://salmon.readthedocs.io/en/latest/salmon.html

$ cd /home/wangjl/software
$ wget -b https://github.com/COMBINE-lab/salmon/releases/download/v0.9.1/Salmon-0.9.1_linux_x86_64.tar.gz
$ tar xzvf Salmon-0.9.1_linux_x86_64.tar.gz
$ vim ~/.bashrc
结尾增加一行：
export PATH=/home/wangjl/software/Salmon-latest_linux_x86_64/bin:$PATH
保存后执行该文件：
$ source ~/.bashrc 

$ salmon -h
Salmon v0.9.1

Usage:  salmon -h|--help or
        salmon -v|--version or
        salmon -c|--cite or
        salmon [--no-version-check] <COMMAND> [-h | options]

Commands:
     index Create a salmon index
     quant Quantify a sample
     swim  Perform super-secret operation
     quantmerge Merge multiple quantifications into a single file

例子1：
$ salmon quant -i salmon_transcript_index -1 reads1.fq.gz -2 reads2.fq.gz -p #threads -l A -g genome.gtf --seqBias --gcBias --posBias


例子2：
#!/bin/bash
for fn in data/DRR0161{25..40};
do
samp=`basename ${fn}`
echo "Processing sample ${samp}"
salmon quant -i athal_index -l A \
         -1 ${fn}/${samp}_1.fastq.gz \
         -2 ${fn}/${samp}_2.fastq.gz \
         -p 8 -o quants/${samp}_quant
done 
其中
-i 是index位置
-l A 是自动判断文库类型（链特异与否）
-p 指定线程
-o 输出文件位置
输入read文件：-r, -1, -2


(1)生成索引
$ salmon index -t athal.fa.gz -i athal_index

找不到人的transcriptome，没法生成索引。
https://github.com/COMBINE-lab/salmon/issues/186
作者留言说怎么下载。

1).表达组 human transcriptome 下载 
https://www.gencodegenes.org/releases/current.html
我提的github issue: https://github.com/COMBINE-lab/salmon/issues/186

$ wget -c ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_27/gencode.v27.transcripts.fa.gz
不要用axel多线程下载，这个网站太敏感，会禁止访问，欧洲人就是没有美国人大气。







========================================
|-- 比对质量质控 deepTools: tools for exploring deep sequencing data
----------------------------------------
1. 基本使用
(1) 安装
https://deeptools.readthedocs.io/en/develop/

deepTools is a suite of python tools particularly developed for the efficient analysis of high-throughput sequencing data, such as ChIP-seq, RNA-seq or MNase-seq.
deeptools是基于Python开发的一套工具，用于处理诸如RNA-seq, ChIP-seq, MNase-seq, ATAC-seq等高通量数据。


工具分为四个模块，当然也可以简单分为两个部分：数据处理和可视化。
	BAM和bigWig文件处理
	质量控制
	热图和其他描述性作图
	其他
#

$ pip3 install deeptools
$ deeptools --version
deeptools 3.5.1



2. 用法 
https://deeptools.readthedocs.io/en/develop/content/list_of_tools.html

(1) bam to bigWig

deeptools提供bamCoverage和bamCompare进行格式转换，为了能够比较不同的样本，需要对先将基因组分成等宽分箱(bin)，统计每个分箱的read数，最后得到描述性统计值。
对于两个样本，描述性统计值可以是两个样本的比率，或是比率的log2值，或者是差值。如果是单个样本，可以用SES方法进行标准化。

ap2_chip_rep1_2_sorted.bam是前期比对得到的BAM文件，得到的bw文件就可以送去IGV/Jbrowse进行可视化。 
bamCoverage的基本用法：
$ bamCoverage -e 170 -bs 10 -b ap2_chip_rep1_2_sorted.bam -o ap2_chip_rep1_2.bw 
参数解释:
--bam BAM file, -b BAM file [必须参数：输入bam文件名] BAM file to process (default: None)
--outFileName FILENAME, -o FILENAME  [必须参数: 输出bw文件名] Output file name. (default: None)

-e/--extendReads和-bs/--binSize即拓展了原来的read长度，且设置分箱的大小。

--filterRNAstrand {forward, reverse}: 仅统计指定正链或负链
--region/-r CHR:START:END: 选取某个区域统计
--smoothLength: 通过使用分箱附近的read对分箱进行平滑化


2) 如果为了其他结果进行比较，还需要进行标准化，deeptools提供了如下参数：
--scaleFactor: 缩放系数
—normalizeUsingRPKMReads: Per Kilobase per Million mapped reads (RPKM)标准化
--normalizeTo1x: 按照1x测序深度(reads per genome coverage, RPGC)进行标准化
--ignoreForNormalization： 指定那些染色体不需要经过标准化

3)如果需要以100为分箱，并且标准化到1x，且仅统计某一条染色体区域的正链（这个正怎么体现的??），输出格式为bedgraph,那么命令行可以这样写
$ bamCoverage -e 170 -bs 100 -of bedgraph -r Chr4:12985884:12997458 --normalizeTo1x 100000000 -b 02-read-alignment/ap2_chip_rep1_1_sorted.bam -o chip.bedgraph
参数解释:
--region CHR:START:END, -r CHR:START:END
	Region of the genome to limit the operation to - this is useful when testing parameters to reduce the computing time. 
	The format is chr:start:end, for example --region chr10 or --region chr10:456700:891000. (default: None)


bamCompare和bamCoverage类似，只不过需要提供两个样本，并且采用SES方法进行标准化，于是多了--ratio参数。



多线程
--numberOfProcessors INT, -p INT
	Number of processors to use. 
	Type "max/2" to use half the maximum number of processors or "max" to use all available processors. (Default: 1)
#





ref:
http://www.360doc.com/content/18/0205/00/19913717_727772433.shtml



========================================
RNAseq分析: 使用 Limma, DEseq2, edgeR, 筛选差异表达基因DEG
----------------------------------------

详情参考本博客专题 R / 分析差异表达基因DEG。






========================================
|-- 使用DEXSeq分析NGS数据中的exon表达差异 //todo
----------------------------------------
1. 官网: http://bioconductor.org/packages/release/bioc/html/DEXSeq.html

Inference of differential exon usage in RNA-Seq 区分RNAseq中差异表达的外显子

The package is focused on finding differential exon usage using RNA-seq exon counts between samples with different experimental designs. It provides functions that allows the user to make the necessary statistical tests based on a model that uses the negative binomial distribution to estimate the variance between biological replicates and generalized linear models for testing. The package also provides functions for the visualization and exploration of the results.
该包聚焦于使用RNAseq的外显子count数，在不同实验设计的样品间，发现差异外显子，
提供一个基于负二项分布的模型，和广义线性模型的检验，估算生物学重复之间的变异，进行必要的统计学检验。
该包也提供对结果的探索和可视化函数。

安装： 
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("DEXSeq")


文档： 
browseVignettes("DEXSeq") #弹出网页包含pdf文档和示例代码。由于端口会变，所以使用服务器Rstudio时看不到这些文件。





2.sam文件转换成counts数据
#第一步：计数

(1)把gtf文件转为gff文件
# 找到DEXSeq提供的python角本的路径。
system.file('python_scripts', package='DEXSeq')
# [1] "/home/wangjl/R/x86_64-pc-linux-gnu-library/3.6/DEXSeq/python_scripts"
#$ python ~/projects/RLib.3.01/DEXSeq/python_scripts/dexseq_prepare_annotation.py Homo_sapiens.GRCh37.75.fixed.gtf DEXSeq.hg19.gene.gff
## # 注意的是，gtf文件必须与mapping的基因组一致，尤其是染色体的名字要一致，注意mapping的bam文件和gtf中是chr1还是1.

> pythonScriptsDir = system.file( "python_scripts", package="DEXSeq" )
> list.files(pythonScriptsDir)
## [1] "dexseq_count.py"              "dexseq_prepare_annotation.py"

官方推荐句子：python /path/to/library/DEXSeq/python_scripts/dexseq_prepare_annotation.py Drosophila_melanogaster.BDGP5.72.gtf Dmel.BDGP5.25.62.DEXSeq.chr.gff


因为一直报错，有人说是版本问题，就改用py2试试。需要重新安装py2的HTSeq包。
$ pip2 install --user HTSeq
$ cd /home/wangjl/data/ref/hg19
$ python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_prepare_annotation.py gencode.v30lift37.basic.annotation.gtf gencode.v30lift37.basic.annotation.gff


这里需要注意的是，gtf文件必须与mapping的基因组一致，尤其是染色体的名字要一致，比如说如果mapping时有chr，gtf文件中的染色体一定也需要有chr。这里运行python是在terminal中，而不是R中。


(2).而后使用dexseq_count.py来计数每个exon上的reads数。 Counting reads
#$ python ~/projectGREEN/RLib.3.01/DEXSeq/python_scripts/dexseq_count.py \
#    -p no -s yes -a 10 -f bam ~/DEXSeq/DEXSeq.hg19.gene.gff bam.file out.counts
#参数-p指出mapping文件是否是pair end文件，默认no。
#参数-s表示是否是stranded，默认为yes。
#-f指输入文件的格式，默认为sam。bam需要安装 pysam包。
# -a to specify the minimum alignment quality，sam文件第五列。跳过低于该值的。

# 在运行计数结束之后，需要检查一下最后四行，看看empty的多不多，如果超过20%，可能需要检查一下mapping的结果
#，当然也可能是计数文件准备错误，比如mapping结果没有index等等。如果以上都不是，那可能是polyA太多了。




$ cd /home/wangjl/data/apa/190705PAS/test/
$ samtools view -h c01_ROW07.bam >c01_ROW07.sam
$ python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py -p no -s yes -a 10 \
/home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  c01_ROW07.sam c01_ROW07.counts





(3)批量化计数
/home/wangjl/data/apa/190705PAS/MAPQ255/c01_ROW07.bam 

同步化的syncHeLa:
c01_ROW24
c01_ROW35
c01_ROW31

非同步化的nonSyncHeLa:
c12_ROW10
c12_ROW16
c12_ROW17

$ cat sync.id | while read id; do echo ${id}; samtools view -h /home/wangjl/data/apa/190705PAS/MAPQ255/${id}.bam > ${id}.sam; \
python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  ${id}.sam ${id}.counts; done;

$ cat non.id | while read id; do echo ${id}; samtools view -h /home/wangjl/data/apa/190705PAS/MAPQ255/${id}.bam > ${id}.sam; \
python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  ${id}.sam ${id}.counts; done;










3.读入R环境中
# 在R中读入计数数据，需要准备好计数文件，实验设计，以及前面用到的gff文件。
#在这里，我们使用Bioconductor中已有的pasilla数据来示例。
library("DEXSeq")
#1.count file names
inDir <- system.file("extdata", package="pasilla")
countFiles <- list.files(inDir, pattern="fb.txt$", full.names=TRUE)
basename(countFiles)
## [1] "treated1fb.txt"   "treated2fb.txt"   "treated3fb.txt"   "untreated1fb.txt" "untreated2fb.txt" 
## [6] "untreated3fb.txt" "untreated4fb.txt"


#2.gff
gffFile <- list.files(inDir, pattern="gff$", full.names=TRUE) #Dmel.BDGP5.25.62.DEXSeq.chr.gff
##注意，如果是自己的数据的话，比如之前示例使用的是DEXSeq.hg19.gene.gff，这里就用DEXSeq.hg19.gene.gff

##实验设计
sampleTable <- data.frame(row.names=c(paste("treated", 1:3, sep=""), paste("untreated", 1:4, sep="")),
                          condition=rep(c("knockdown", "control"), c(3, 4)))
sampleTable
##            condition
## treated1   knockdown
## treated2   knockdown
## treated3   knockdown
## untreated1   control
## untreated2   control
## untreated3   control
## untreated4   control


dxd <- DEXSeqDataSetFromHTSeq(
  countFiles,
  sampleData=sampleTable,
  design= ~sample + exon + condition:exon,
  flattenedfile=gffFile)
## converting counts to integer mode
dim(dxd) #[1] 70463    14
head(dxd)

#查看矩阵
dim(featureCounts(dxd)) #[1] 70463     7
head( featureCounts(dxd), 5 )


# 第三步：获得差异表达数据
#只需要一步
dxr <- DEXSeq(dxd) #耗时
head(dxr)

## 对于结果dxr，可以直接视为data.frame来操作。也可以使用as.data.frame来转换它。
## 结合使用plotDEXSeq就可以查看自己感兴趣的目标基因中的exon的表达情况。
head(unique(dxr$groupID))
plotDEXSeq(dxr, geneID='FBgn0000014')
plotDEXSeq(dxr, geneID='FBgn0010909')
plotDEXSeq( dxr, "FBgn0010909", displayTranscripts=TRUE, legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )



#
########################begin 可选的三步法
## 同DESeq一样，它分为三个步骤：normalization, Dispersion estimation 以及 testing for differential exon usage。
dxd2 <- estimateSizeFactors(dxd) #第一步
dxd3 <- estimateDispersions(dxd2) #第二步，耗时。此时可以使用plotDispEsts(dxd)来观察离散情况
plotDispEsts(dxd3)
## Figure 1: Fit Diagnostics
## The initial per-exon dispersion estimates (shown by black points), the fitted 
## mean-dispersion values function (red line), and the shrinked values in blue
dxd4 <- testForDEU(dxd3) #第三步: differential exon usage (DEU)
#dxd5 <- estimateExonFoldChanges(dxd4, fitExptoVar="condition")
##  Error in estimateExonFoldChanges(dxd4, fitExptoVar = "condition") : 
## unused argument (fitExptoVar = "condition")
dxd5 <- estimateExonFoldChanges(dxd4)
dxr2 <- DEXSeqResults(dxd5) #可以使用plotMA(dxr1)来查看结果
head(dxr2)
#
dxr[1:4,1:4]
dxr2[1:4,1:4] #一模一样
########################end




plotMA(dxr2)
mcols(dxr2)$description #描述意义

#  how many exonic regions are significant with a false discovery rate of 10%:
table ( dxr2$padj < 0.1 )
## FALSE  TRUE 
## 42985   233

##may also ask how many genes are affected
table ( tapply( dxr2$padj < 0.1, dxr2$groupID, any ) )
## FALSE  TRUE 
## 5220   166

plotMA( dxr2, cex=0.8 ) ## Figure 2: MA plot
## Mean expression versus log2 fold change plot. Significant hits at an FDR=0.1 are coloured in red.

#查看样本注释信息
sampleAnnotation(dxd)

## 其他技术和实验因素 6


#具体基因外显子的可视化
plotDEXSeq( dxr2, "FBgn0010909", legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 ) ## Figure 3: Fitted expression
## The plot represents the expression estimates from a call to testForDEU(). 
##  Shown in red is the exon that showed significant differential exon usage.

# 每个样品 with normalized count values of each exon in each of the samples.
plotDEXSeq( dxr2, "FBgn0010909", expression=FALSE, norCounts=TRUE, 
            legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )

# after subtraction of overall changes in gene expression.
plotDEXSeq( dxr2, "FBgn0010909", expression=FALSE, splicing=TRUE,
            legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )
#
#把所有的显著性差异的基因的相关图片全部画一遍
DEXSeqHTML( dxr, FDR=0.1, color=c("#FF000080", "#0000FF80") )
getwd()








# 不知道干啥用的
wh = (dxr2$groupID=="FBgn0010909")
stopifnot( sum(dxr2$padj[wh] < formals(plotDEXSeq)$FDR)==1 )


##
# 查看gene
head( geneIDs(dxd) )
## [1] "FBgn0000003" "FBgn0000008" "FBgn0000008" "FBgn0000008" "FBgn0000008" "FBgn0000008"

# 查看外显子
head( exonIDs(dxd) )
## [1] "E001" "E001" "E002" "E003" "E004" "E005"


## Overlap operations
interestingRegion = GRanges( "chr2L", IRanges(start=3872658, end=3875302) )
subsetByOverlaps( x=dxr, ranges=interestingRegion )
#
#This functions could be useful for further downstream analysis.
findOverlaps( query=dxr2, subject=interestingRegion )
## queryLength: 70463 / subjectLength: 1




#
library(ggplot2)
ggplot(as.data.frame(dxr2), aes( log10(exonBaseMean), log2fold_knockdown_control) )+ geom_point()

library(ggplot2)
ggplot(as.data.frame(dxr2), aes( log10(exonBaseMean), dispersion) )+ geom_point()







refer:
1.https://www.plob.org/article/6960.html
2.http://www.bio-info-trainee.com/bioconductor_China/software/DEXSeq.html
3.http://www.biotrainee.com/thread-1220-1-1.html

4. #软件工具#使用DEXSeq分析NGS数据中的exon表达差异 泊客  云生信学生物信息学  2016-10-26




========================================
|-- 加权基因共表达网络分析 (WGCNA, Weighted correlation network analysis)
----------------------------------------
https://mp.weixin.qq.com/s?__biz=MzI5MTcwNjA4NQ==&mid=2247485220&idx=1&sn=007188964e7c43d75dcd0b11b880bbfa&scene=21#wechat_redirect





========================================
bedtools : a powerful toolset for genome arithmetic 强有力的基因组算法瑞士军刀
----------------------------------------
1. 简介 
(1)bedtools: flexible tools for genome arithmetic and DNA sequence analysis.
usage:    bedtools <subcommand> [options]


软件相关论文：
Quinlan, A.R. & Hall, I.M. BEDTools: a flexible suite of utilities for comparing genomic features. Bioinformatics 26, 841-842 (2010).

bedtools说明书：
https://bedtools.readthedocs.io/en/latest/index.html
http://quinlanlab.org/tutorials/bedtools/bedtools.html


最新版的官网：
http://bedtools.readthedocs.io/en/latest/index.html

旧版本的pdf：
https://insidedna.me/tool_page_assets/pdf_manual/bedtools.pdf

Collectively, the bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF. While each individual tool is designed to do a relatively simple task (e.g., intersect two interval files), quite sophisticated analyses can be conducted by combining multiple bedtools operations on the UNIX command line.

bedtools is developed in the Quinlan laboratory at the University of Utah and benefits from fantastic contributions made by scientists worldwide.



(2)为什么不得不用bedtools？

速度，当数据到达百万级以上，R和C的速度差别就非常明显了
专业，但凡涉及到region、peak的处理，bedtools都可以胜任

https://www.cnblogs.com/leezx/p/14475092.html






2. 安装方法：
#apt-get install bedtools #Debian/Ubuntu. 
#yum install BEDTools #Fedora/Centos

或者：
$ wget https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz
$ tar -zxvf bedtools-2.25.0.tar.gz
$ cd bedtools2
$ make
$ make prefix=~/ install #安装到家目录下的bin文件夹中。



### 如果缺少 include <zlib.h>
http://zlib.net/ download zlib
$ wget http://zlib.net/zlib-1.2.11.tar.gz
$ tar zxvf zlib-1.2.11.tar.gz
$ cd zlib-1.2.11/
$ ./configure
$ make 
$ sudo make install

直到两年后我才理解 linux下 的软件安装需要指定路径，而且是自己有权限的路径
./configure --prefix=/home/wangjl/bin/
-- 再在bwa目录使用make，正常了。（bwa的安装也依赖zlib）



查看版本号：
$ bedtools -version 
## bedtools v2.25.0












3. 使用细节

查看帮助：
$ bedtools -help 



$ bedtools
bedtools: flexible tools for genome arithmetic and DNA sequence analysis.
usage:    bedtools <subcommand> [options]

The bedtools sub-commands include:

[ Genome arithmetic ]
    intersect     Find overlapping intervals in various ways.
    window        Find overlapping intervals within a window around an interval.
    closest       Find the closest, potentially non-overlapping interval.
    coverage      Compute the coverage over defined intervals.
    map           Apply a function to a column for each overlapping interval.
    genomecov     Compute the coverage over an entire genome. 基因组覆盖度
    merge         Combine overlapping/nearby intervals into a single interval.
    cluster       Cluster (but don't merge) overlapping/nearby intervals.
    complement    Extract intervals _not_ represented by an interval file.
    subtract      Remove intervals based on overlaps b/w two files.
    slop          Adjust the size of intervals.
    flank         Create new intervals from the flanks of existing intervals.
    sort          Order the intervals in a file.
    random        Generate random intervals in a genome.
    shuffle       Randomly redistrubute intervals in a genome.
    sample        Sample random records from file using reservoir sampling.
    spacing       Report the gap lengths between intervals in a file.
    annotate      Annotate coverage of features from multiple files.

[ Multi-way file comparisons ] 多文件比较
    multiinter    Identifies common intervals among multiple interval files.
    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.

[ Paired-end manipulation ] 双端操作
    pairtobed     Find pairs that overlap intervals in various ways.
    pairtopair    Find pairs that overlap other pairs in various ways.

[ Format conversion ] 格式转变
    bamtobed      Convert BAM alignments to BED (& other) formats.
    bedtobam      Convert intervals to BAM records.
    bamtofastq    Convert BAM records to FASTQ records.
    bedpetobam    Convert BEDPE intervals to BAM records.
    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.

[ Fasta manipulation ] fasta 文件操作
    getfasta      Use intervals to extract sequences from a FASTA file.
    maskfasta     Use intervals to mask sequences from a FASTA file.
    nuc           Profile the nucleotide content of intervals in a FASTA file.

[ BAM focused tools ] bam 操作
    multicov      Counts coverage from multiple BAMs at specific intervals.
    tag           Tag BAM alignments based on overlaps with interval files.

[ Statistical relationships ] 统计关系
    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.
    reldist       Calculate the distribution of relative distances b/w two files.
    fisher        Calculate Fisher statistic b/w two feature files.

[ Miscellaneous tools ] 其余工具
    overlap       Computes the amount of overlap from two intervals.
    igv           Create an IGV snapshot batch script.
    links         Create a HTML page of links to UCSC locations.
    makewindows   Make interval "windows" across a genome.
    groupby       Group by common cols. & summarize oth. cols. (~ SQL "groupBy")
    expand        Replicate lines based on lists of values in columns.
    split         Split a file into multiple files with equal records or base pairs.

[ General help ]
    --help        Print this help menu.
    --version     What version of bedtools are you using?.
    --contact     Feature requests, bugs, mailing lists, etc.
##










使用方法：
# bedtools sorted
$ bedtools intersect \
           -a ccds.exons.bed -b aln.bam.bed \
           -c \
           -sorted

# bedtools unsorted
$ bedtools intersect \
           -a ccds.exons.bed -b aln.bam.bed \
           -c

# bedmap (without error checking)
$ bedmap --echo --count --bp-ovr 1 \
         ccds.exons.bed aln.bam.bed

# bedmap (no error checking)
$ bedmap --ec --echo --count --bp-ovr 1 \
         ccds.exons.bed aln.bam.bed

# bam to bed(bed文件比bam文件坐标小1)
$ bedtools bamtobed -i reads.bam | head -3
-i 后面跟着输入的bam文件





4. 使用实例

(1) 第一个功能 genomecov
我们先看第一个功能，把alignment的结果文件转为bedgraph格式文件。 不过这个功能用处不是很大。

参考：http://bedtools.readthedocs.io/en/latest/content/tools/genomecov.html

bedtools genomecov [OPTIONS] [-i|-ibam] -g (iff. -i)
genomeCoverageBed  [OPTIONS] [-i|-ibam] -g (iff. -i)

这个命令本身并不是设计来做格式转换的，bam2bedgraph也只是其中的一个小功能而已，需要加上-bg参数，就可以Report depth in BedGraph format. For details, see: http://genome.ucsc.edu/goldenPath/help/bedgraph.html

大家观摩我下面给出的测试例子，就明白该功能如何使用了

bedtools genomecov  -bg -i E001-H3K4me1.tagAlign -g mygenome.txt >E001-H3K4me1.bedGraph
bedtools genomecov  -bg -i E001-Input.tagAlign -g mygenome.txt >E001-Input.bedGraph
nohup bedtools genomecov  -bg -ibam BAF180_CT10.unique.sorted.bam >BAF180_CT10.bedGraph &
nohup bedtools genomecov  -bg -ibam BAF180_CT22AM.unique.sorted.bam >BAF180_CT22AM.bedGraph &
nohup bedtools genomecov  -bg -ibam BAF180_CT22.unique.sorted.bam >BAF180_CT22.bedGraph &
nohup bedtools genomecov  -bg -ibam inputCT10sonication.unique.sorted.bam >inputCT10sonication.bedGraph &

首先alignment的文件必须是sort的，
然后如果是bed格式的比对文件，用-i 参数来指定输入文件，需要加入参考基因组的染色体大小记录文件(mygenome.txt )，
如果是bam格式的比对文件，用-ibam指定输入文件，而且不需要参考基因组的染色体大小记录文件。




(2) 
















更多用法：
http://bedtools.readthedocs.io/en/latest/content/example-usage.html
高级用法：
http://bedtools.readthedocs.io/en/latest/content/advanced-usage.html

Report the number of genes that each alignment overlaps.
$ bedtools intersect -a reads.bed -b genes.bed -c

https://www.jianshu.com/p/f8bbd51b5199






========================================
|-- bedtools multicov: 统计 bed 每行区间内bam中的reads数
----------------------------------------

对RNA-seq的比对文件中的比对到各个基因的reads进行计数。
或者对于合并call peak后得到的bed文件，统计每个样本在这些峰中的reads数，获得count 矩阵。


参考: http://www.bio-info-trainee.com/745.html
http://bedtools.readthedocs.io/en/latest/content/tools/multicov.html


1. 示例
(1) 需要对bam做index。
$ samtools index xx.bam

(2) bed格式可以是4列或6列的，可以从 MACS2 的输出文件中使用 awk 生成。
$ awk '{print $1"\t"$2"\t"$3"\t"$4"\t"$5"\t"$6}' macs2_result/total_peaks.narrowPeak > macs2_result/total.bed
$ head macs2_result/total.bed
chr1    633888  634322  total_peak_1    628     .
chr1    1724255 1724472 total_peak_2    61      .
chr1    2133178 2133401 total_peak_3    32      .


(3) 获取矩阵
$ bedtools multicov -bams aln1.bam aln2.bam aln3.bam -bed ivls-of-interest.bed
# ivls-of-interest.bed这个文件是必须的，可能需要自己制作，其实用gtf文件也可以的，如下：
chr1 0   10000   ivl1
chr1 10000   20000   ivl2
chr1 20000   30000   ivl3
chr1 30000   40000   ivl4

输出结果前三列是坐标，第四列是基因名，跟我们的bed文件一样，只是最后三列是三个样本的计数，是添加上来的！
chr1 0       10000   ivl1    100 2234    0
chr1 10000   20000   ivl2    123 3245    1000
chr1 20000   30000   ivl3    213 2332    2034
chr1 30000   40000   ivl4    335 7654    0
可以看到，它实现的需求，跟htseq这个软件差不多。


2) 改进版，如果需要更多sample，可以使用shell子命令
$ bedtools multicov -bams `ls map_clean/P1*.final.bam` -bed macs2_result/total.bed >macs2_result/matrix0.txt




(4) 矩阵优化：添加行名、列名

1) 获取样本名字
$ ls map_clean/*.final.bam |  sed -e 's/map_clean\///' -e 's/\.final\.bam//' >matrix/colname.txt


2) R script
dt=read.table("macs2_result/matrix0.txt", header = F)
dim(dt)
dt[1:4,1:8]
rownames(dt)=dt$V4

dt2=dt[, -c(1:6)]
colnames(dt2)=readLines("matrix/colname.txt")
dt2[1:4,1:8]
dim(dt2) #3560   96

write.table(dt2, "P1_raw_counts.txt")


然后就可以用 Seurat 分析了。









========================================
|--  getfasta: 根据坐标区域来从基因组里面提取fasta序列
----------------------------------------
http://bedtools.readthedocs.io/en/latest/content/tools/getfasta.html

1. 简介
bedtools getfasta extracts sequences from a FASTA file for each of the intervals defined in a BED/GFF/VCF file.

(1)注意：
1). The headers in the input FASTA file must exactly match the chromosome column in the BED file.
fa和bed的chr名字要一致。

2). You can use the UNIX fold command to set the line width of the FASTA output. For example, fold -w 60 will make each line of the FASTA file have at most 60 nucleotides for easy viewing.
可以使用 -w 60 指定每60个碱基换行。方便预览。

3). BED files containing a single region require a newline character at the end of the line, otherwise a blank output file is produced.
bed文件只有一行时，结尾必须有换行符，否则输出空白。


(2) 命令格式
$ bedtools getfasta [OPTIONS] -fi <input FASTA> -bed <BED/GFF/VCF>
(or):
$ getFastaFromBed [OPTIONS] -fi <input FASTA> -bed <BED/GFF/VCF>

参考基因组用-fi参数指定具体位置，输出的fasta序列文件用-fo参数指定





2. 示例

(1) 从fa文件获取bed指定坐标的碱基
$ cat test.fa
>chr1
AAAAAAAACCCCCCCCCCCCCGCTACTGGGGGGGGGGGGGGGGGG

$ cat test.bed
chr1 5 10

$ bedtools getfasta -fi test.fa -bed test.bed
>chr1:5-10
AAACC

# optionally write to an output file
$ bedtools getfasta -fi test.fa -bed test.bed -fo test.fa.out

$ cat test.fa.out
>chr1:5-10
AAACC


(2) 在bed文件第4列添加名字，作为新生成的fasta的序列名字
$ cat test.fa
>chr1
AAAAAAAACCCCCCCCCCCCCGCTACTGGGGGGGGGGGGGGGGGG

$ cat test.bed
chr1 5 10 myseq

$ bedtools getfasta -fi test.fa -bed test.bed -name
>myseq
AAACC



(3) 
$ bedtools getfasta -fi ~/biosoft/bowtie/hg19_index/hg19.fa  -bed ../macs14_results/highQuality_summits.bed  -fo highQuality.fa
$ bedtools getfasta -fi ~/biosoft/bowtie/hg19_index/hg19.fa  -bed ../macs14_results/highQuality_peaks.bed  -fo highQuality.fa

我的例子脚本里面用的是bed格式来记录坐标区域。





========================================
|-- bedtools intersect / samtools: 取 大bam文件 的子集
----------------------------------------
http://bedtools.readthedocs.io/en/latest/content/tools/intersect.html


原始序列
$ samtools view apa/190705PAS/hg19/CutA_c01_ROW31_Aligned.sortedByCoord.out.bam | wc
2158830 32382450 581771742



1. 使用 bedtools intersect 取子集
$ samtools view CutA_c01_ROW31_Aligned.sortedByCoord.out.bam |grep chr8| grep 14601 | wc
   2362   35430  690623

(1).建立一个bed文件，限定区域
$ vim RPL8.bed
chr8	146015076	146015318


(2).从bam中取出该范围的行
$ bedtools intersect -a CutA_c01_ROW31_Aligned.sortedByCoord.out.bam -b RPL8.bed >c01_ROW31_RPL8.bam

检查:
$ samtools view c01_ROW31_RPL8.bam |wc
   2363   35445  690971
#



2. 使用samtools view  获取
(1) 获取某染色体序列
$ samtools view apa/190705PAS/hg19/CutA_c01_ROW31_Aligned.sortedByCoord.out.bam | grep chr8 | wc
  67760 1016400 17762256

直接在bam后加染色体名字
$ samtools view apa/190705PAS/hg19/CutA_c01_ROW31_Aligned.sortedByCoord.out.bam chr8 | wc
  67759 1016385 17762079

(2) 获取某个染色体某范围的reads
$ samtools view apa/190705PAS/hg19/CutA_c01_ROW31_Aligned.sortedByCoord.out.bam chr8:146015076-146015318 | wc
	2363   35445  690971

(3) 输出成bam格式 -b，保存头部信息-h
$ samtools view -b -h apa/190705PAS/hg19/CutA_c01_ROW31_Aligned.sortedByCoord.out.bam chr8:146015076-146015318 >test.bam

$ samtools view test.bam |wc
   2363   35445  690971



ref:
使用samtools/bedtools提取bam/sam文件指定区域reads 
http://www.dengfeilong.com/note/290.html




========================================
|-- 区域注释 intersect，及实例若干
----------------------------------------
http://bedtools.readthedocs.io/en/latest/content/tools/intersect.html

1. 注释CNV区域时哪个基因？

(1) 从gencode数据库里面下载，然后解析成bed格式
$ head ~/reference/gtf/gencode/protein_coding.hg19.position
chr1    69091   70008   OR4F5
chr1    367640  368634  OR4F29
chr1    621096  622034  OR4F16
chr1    859308  879961  SAMD11
chr1    879584  894689  NOC2L
chr1    895967  901095  KLHL17
chr1    901877  911245  PLEKHN1
chr1    910584  917473  PERM1
chr1    934342  935552  HES4
chr1    936518  949921  ISG15


(2) 下载的CNV文本文件，转为bed格式的，就是把列的顺序调换一下：

$ head Features.bed  
chr1    3218610 95674710    TCGA-3C-AAAU-10A-01D-A41E-01    53225   0.0055
chr1    95676511    95676518    TCGA-3C-AAAU-10A-01D-A41E-01    2   -1.6636
chr1    95680124    167057183   TCGA-3C-AAAU-10A-01D-A41E-01    24886   0.0053
chr1    167057495   167059336   TCGA-3C-AAAU-10A-01D-A41E-01    3   -1.0999
chr1    167059760   181602002   TCGA-3C-AAAU-10A-01D-A41E-01    9213    -8e-04
chr1    181603120   181609567   TCGA-3C-AAAU-10A-01D-A41E-01    6   -1.2009
chr1    181610685   201473647   TCGA-3C-AAAU-10A-01D-A41E-01    12002   0.0055
chr1    201474400   201474544   TCGA-3C-AAAU-10A-01D-A41E-01    2   -1.4235
chr1    201475220   247813706   TCGA-3C-AAAU-10A-01D-A41E-01    29781   -4e-04

(3) 命令很简单，如下：
$ bedtools intersect -a Features.bed  -b  ~/reference/gtf/gencode/protein_coding.hg19.position \
-wa -wb   | bedtools groupby -i - -g 1-4 -c 10 -o collapse

参数解释：
-g -grp		Specify the columns (1-based) for the grouping.
			The columns must be comma separated.
			- Default: 1,2,3
-c -opCols	Specify the column (1-based) that should be summarized.
			- Required.
-o -ops		Specify the operation that should be applied to opCol.
			Valid operations:
			    sum, count, count_distinct, min, max,
			    mean, median, mode, antimode,
			    stdev, sstdev (sample standard dev.),
			    collapse (i.e., print a comma separated list (duplicates allowed)), 
			    distinct (i.e., print a comma separated list (NO duplicates allowed)), 
			    distinct_sort_num (as distinct, but sorted numerically, ascending), 
			    distinct_sort_num_desc (as distinct, but sorted numerically, descending), 
			    concat   (i.e., merge values into a single, non-delimited string), 
			    freqdesc (i.e., print desc. list of values:freq)
			    freqasc (i.e., print asc. list of values:freq)
			    first (i.e., print first value)
			    last (i.e., print last value)
			- Default: sum

注释结果如下：可以看到，每个CNV片段都注释到了对应的基因，有些特别大的片段，会被注释到非常多的基因。

chr8    42584924    42783715    TCGA-5T-A9QA-01A-11D-A41E-01    CHRNB3,CHRNA6,THAP1,RNF170,HOOK3
chr8    42789728    42793594    TCGA-5T-A9QA-01A-11D-A41E-01    HOOK3
chr8    42797957    42933372    TCGA-5T-A9QA-01A-11D-A41E-01    RP11-598P20.5,FNTA,HOOK3
chr8    70952673    70964372    TCGA-5T-A9QA-01A-11D-A41E-01    PRDM14
chr10    42947970    43833200    TCGA-5T-A9QA-01A-11D-A41E-01    BMS1,RET,RASGEF1A,ZNF33B,CSGALNACT2
chr10    106384615   106473355   TCGA-5T-A9QA-01A-11D-A41E-01    SORCS3
chr10    106478366   107298256   TCGA-5T-A9QA-01A-11D-A41E-01    SORCS3
chr10    117457285   117457859   TCGA-5T-A9QA-01A-11D-A41E-01    ATRNL1
chr11    68990173    69277078    TCGA-5T-A9QA-01A-11D-A41E-01    MYEOV
chr11    76378708    76926535    TCGA-5T-A9QA-01A-11D-A41E-01    LRRC32,B3GNT6,OMP,TSKU,MYO7A,ACER3,CAPN5






2. 更多实例

加-wa参数可以报告出原始的在A文件中的feature，
加-wb参数可以报告出原始的在B文件中的feature, 
加-c参数可以报告出两个文件中的overlap的feature的数量，
参数-s可以得到忽略strand的overlap

注意：使用tab分割！

(1) 案例一：包含着染色体位置的两个文件，分别记为A文件和B文件。
分别来自于不同文件的染色体位置的交集是什么？
$ cat A.bed
chr1	10	20
chr1	30	40

$ cat B.bed
chr1	15	25

$ bedtools intersect -a A.bed -b B.bed 
chr1	15	20


(2) 案例二：包含着染色体位置的两个文件，分别记为A文件和B文件。
求A文件中哪些染色体位置是与文件B中的染色体位置有overlap.
-wa 报告a中的行。

$ bedtools intersect -a A.bed -b B.bed -wa
chr1	10	20

(3) 案例三：包含着染色体位置的两个文件，分别记为A文件和B文件。
求A文件中染色体位置与文件B中染色体位置的交集，以及对应的文件B中的染色体位置.
-wb 报告交集，及B的行

$ bedtools intersect -a A.bed -b B.bed -wb
chr1	15	20	chr1	15	25




(4) 案例四（常用）： 包含着染色体位置的两个文件，分别记为A文件和B文件。
求对于A文件的染色体位置是否与文件B中的染色体位置有交集。
	如果有交集，分别输入A文件的染色体位置和B文件的染色体位置；
	如果没有交集，输入A文件的染色体位置并以'. -1 -1'补齐文件。

-loj	Perform a "left outer join". That is, for each feature in A report each overlap with B.  
		If no overlaps are found, report a NULL feature for B.
左外连接，保持左边a的完整性。有交集就输出b的行，没有交集则输出 . -1 -1 占位。

$ bedtools intersect -a A.bed -b B.bed -loj
chr1 10 20 chr1 15 25
chr1 30 40 . -1 -1


(5) 案例五： 包含着染色体位置的两个文件，分别记为A文件和B文件。
对于A文件中染色体位置，如果和B文件中染色体位置有overlap,则输出在A文件中染色体位置和在B文件中染色体位置，以及overlap的长度.

-wo	Write the original A and B entries plus the number of base pairs of overlap between the two features.
		- Overlaps restricted by -f and -r.
		  Only A features with overlap are reported.
报告原始的A和B的行，及重叠的碱基数。
只报告A中有重合的行。

$ cat A.bed
chr1	10	20
chr1	30	40

$ cat B2.bed
chr1	15	20
chr1	18	25

$ bedtools intersect -a A.bed -b B2.bed -wo
chr1	10	20	chr1	15	20	5
chr1	10	20	chr1	18	25	2



(6)  包含着染色体位置的两个文件，分别记为A文件和B文件。
对于A文件中染色体位置，如果和B文件中染色体位置有overlap,则输出在A文件中染色体位置和在B文件中染色体位置，以及overlap的长度；如果和B文件中染色体位置都没有overlap,则用'. -1-1'补齐文件

加了all参数，则报告全部A的行。

$ bedtools intersect -a A.bed -b B2.bed -wao
chr1	10	20	chr1	15	20	5
chr1	10	20	chr1	18	25	2
chr1	30	40	.	-1	-1	0



(7) 案例七： 包含着染色体位置的两个文件，分别记为A文件和B文件。
对于A文件中染色体位置，输出在A文件中染色体位置和有多少B文件染色体位置与之有overlap.

-c	For each entry in A, report the number of overlaps with B.
	- Reports 0 for A entries that have no overlap with B.
	- Overlaps restricted by -f, -F, -r, and -s.
报告每行A与几个B行重合。

$ bedtools intersect -a A.bed -b B2.bed -c
chr1	10	20	2
chr1	30	40	0




(8) 案例八(常用)： 包含着染色体位置的两个文件，分别记为A文件和B文件。
对于A文件中染色体位置，输出在A文件中染色体位置和与B文件染色体位置至少有X%的overlap的记录。


$ cat A3.bed
chr1	100	200

$ cat B3.bed
chr1	130	201
chr1	180	220

$ bedtools intersect -a A3.bed -b B3.bed -f 0.50 -wa -wb
chr1	100	200	chr1	130	201




========================================
|-- 文件转换 bam to fastq
----------------------------------------
1. 命令使用
bedtools bamtofastq [OPTIONS] -i <BAM> -fq <FASTQ>

可选项
Option	Description
-fq2	FASTQ for second end. Used if BAM contains paired-end data. BAM should be sorted by query name (samtools sort -n aln.bam aln.qsort) if creating paired FASTQ with this option.
-tags	Create FASTQ based on the mate info in the BAM R2 and Q2 tags.




(1)实例: 输入10x 单端一个cell barcode的bam文件
$ bedtools bamtofastq -i ../mapRaw/AGCCTAATCCTGCCAT_raw_R2_Aligned.sortedByCoord.out.bam -fq aa.fq

$ head aa.fq 
@A00129:409:HKVNVDSXX:2:2173:24551:24752
ACCGCCATGGACAACGCCAGCAAGAACGCTTCTGATATGATTGACAAATTGACCTTGACTTTCAACCGCACCCGCCAGGCTGTCATCACAAAGGAGTTGATTGAAATCATCTCTGGGGCTGCTGCTCTGGATTAATGAAAATCAAGTTGC
+
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFF



(2) -fq2 Creating two FASTQ files for paired-end sequences.
注意Note:
When using this option, it is required that the BAM file is sorted/grouped by the read name. 
This keeps the resulting records in the two output FASTQ files in the same order. 
One can sort the BAM file by query name with samtools sort -n aln.bam aln.qsort.


$ samtools sort -n aln.bam aln.qsort

$ bedtools bamtofastq -i aln.qsort.bam \
                      -fq aln.end1.fq \
                      -fq2 aln.end2.fq

$ head -8 aln.end1.fq

ref: https://bedtools.readthedocs.io/en/latest/content/tools/bamtofastq.html



========================================
|-- bedtools 其他小功能
----------------------------------------
1. bedtools merge

用于合并位于同一个bed/gff/vcf 文件中的重叠区域。

$ bedtools merge [OPTION] –i

-s 必须相同(正负)链的区域才合并（软件默认不考虑正负链特征）
-n 报告合并的区域数量，没有被合并则1
-d 两个独立区域间距小于（等于）该值时将被合并为一个区域
-nms 报告被合并区域的名称
-scores 报告几个被合并特征区域的scores




其它小功能
1）pairToPair
比较BEDPE文件搜索overlaps, 类似于pairToBed。

2）bamToBed
将BAM文件转换为BED文件或者BEDPE文件。
bamToBed -i reads.bam

3）windowBed类似于intersectBed, 但是可以指定一个数字，让A中的genome feature增加上下游去和B中的genome features进行overlap。默认情况这个值为1000，可以使用-w加定义，可以用-l指定是上游，用-r指定下游
windowBed -a A.bed -b B.bed -w 5000
windowBed -a A.bed -b B.bed -l 200 -r 20000

4）subtractBed在A中去除掉B中有的genome features
5）coverageBed可以计算深度和覆盖度。如计算基因组任意1Kb的测序read的覆盖度
6）genomeCoverageBed。可以计算给定bam文件在基因组上的覆盖度及每个碱基的深度。











========================================
samtools用法 – Utilities for the Sequence Alignment/Map (SAM) format
----------------------------------------
代码： https://github.com/samtools/samtools
文档： http://www.htslib.org/doc/samtools.html


SAM (Sequence Alignment/Map) format is a generic format for storing large nucleotide sequence alignments. SAM aims to be a format that:
http://samtools.sourceforge.net/



1.安装

(1) 方法1：不一定work...
第一步，从github下载所需版本samtools软件包
https://sourceforge.net/projects/samtools/files/samtools/1.5/

第二步，解压，此处以目前最新版samtools-1.5为例：
tar -jxf samtools-1.5.tar.bz2
cd samtools-1.5

第三步，编译，安装：
make
make prefix=/opt/biosoft/samtools-1.5 install

第四步，加入环境变量
echo 'export PATH=$PATH:/opt/biosoft/samtools-1.5/bin' >> /etc/profile

现在你已经不需要刚刚下载和解压的软件包了，愉快的删除吧。
cd ../ && rm -rf samtools-1.5 samtools-1.5.tar.bz2


bcftools安装几乎完全一样。需要注意的是，此处我用了root账户。对于普通用户，可以把软件安装在自己有读写权限的目录下，也就是说，要更改prefix=xxx和/etc/profile至你自己的文件目录和文件。
https://samtools.github.io/bcftools/howtos/install.html




(2) 方法2：CentOS 亲测可用
从github获取最新版本的包
git clone git://github.com/samtools/samtools.git
git clone https://github.com/samtools/htslib.git #并列放到同一个文件夹下

进入samtools文件夹，执行命令：
autoheader            # Build config.h.in (this may generate a warning about
                      # AC_CONFIG_SUBDIRS - please ignore it).
autoconf -Wno-syntax  # Generate the configure script
./configure           # Needed for choosing optional functionality #如果samtool并列没有HTSlib会报错。
make
##make install #error install: cannot create regular file ‘/usr/local/bin/samtools’: Permission denied
make prefix=~/ install #或在~/bin建软链接

检查版本号：
$ samtools --version
samtools 1.9-69-gb217a91




(3) Ubuntu上安装 https://blog.csdn.net/cuicanlianyue/article/details/79458594

git clone git://github.com/samtools/samtools.git
git clone https://github.com/samtools/htslib.git

需要先安装两个库
sudo apt-get install libbz2-dev #进行编译时出现error：建立HTSlib需要libbzip2文件，需要安装
sudo apt-get install liblzma-dev #进行编译时出现error：需要liblzma文件

然后安装hslib
autoheader
autoconf
./configure 
make
make prefix=~/ install
然后同样命令安装samtools









========================================
|-- samtools常用功能：sam-bam格式转化、排序索引
----------------------------------------
Version: 1.9-69-gb217a91 (using htslib 1.9-149-gf5b75ff)

常用语句：
samtools view -bS input.sam >aln.bam 	#1.转换为bam
samtools sort -o aln.sorted.bam aln.bam #2.排序
samtools index aln.sorted.bam 		#3.建索引


适用于1.3及之后的版本。一步sam转bam并排序。老版本需要2步：1先转bam，2再排序。
$ nohup samtools sort -@ 8 -o ERR188044_chrX.bam ERR188044_chrX.sam 2>ERR188044_sam.err &


常用参数: 
$ samtools sort --help
Usage: samtools sort [options...] [in.bam]
-@, --threads INT     线程数 Number of additional threads to use [0]
-o FILE    Write final output to FILE rather than standard output 输出文件



服务器自带的古老的 0.1.19 可能只支持这么写
Usage:   samtools sort [options] <in.bam> <out.prefix>
最后一个是参数是输出文件的前缀，如：
$ samtools sort c12_ROW03.Mm.bam c12_ROW03.Mm.s
吐槽：CentOS7自带的软件版本都太古老！可靠===过时，不适用于追新的科研。还是用Ubuntu省劲，源都比较新。





$ samtools view -@ 10 -bS -F 4 input.sam > aln.bam
## -F INT   only include reads with none of the FLAGS in INT present [0]

$ samtools depth aln.sorted.bam >depth_reads.txt
$ wc -l depth_reads.txt > Coverage-aln_reads.txt


更多命令：http://www.htslib.org/doc/samtools.html






2.sam 和 bam 格式转换
https://www.cnblogs.com/emanlee/p/4316581.html

(1)SAM转换为BAM
samtools view -bS input.sam >out.bam
samtools view -b -S NA12878.sam > NA12878_2.bam

$ samtools view -bS ../c12_A1_Aligned.out.sam -o c12_A1.bam -@ 10

-b 输出BAM format
-S 输入格式自动检查(sam,bam,cram)。 如果@SQ 缺省， 要写-t (?)
-t FILE:  FILE listing reference names and lengths (see long help) [null]
-o File 输出文件名
-@ 额外使用的线程数

所以如果没有@SQ
samtools faidx ref.fa
samtools view -bt ref.fa.fai out.sam > out.bam


(2)BAM转换为SAM
samtools view -h -o out.sam input.bam
samtools view -h NA12878.bam >NA12878_2.sam
# 参数
 -h       include header in SAM output
 -H       print SAM header only (no alignments)

 -@, --threads INT:  Number of additional threads to use [0]
 -o FILE  output file name [stdout] 输出文件名字[默认是输出到屏幕]
 -O, --output-fmt FORMAT[,OPT[=VAL]]...
        Specify output format (SAM, BAM, CRAM) 指定输出文件的格式




3.排序
$ samtools sort c12_A1.bam -o c12_A1.sorted.bam -@ 10
$ samtools sort -T /tmp/aln.sorted -o aln.sorted.bam aln.bam

# -T 是指定临时文件
-o File 输出到文件而不是标准输出
-@ 线程数


4.建索引
$ samtools index aln.sorted.bam

或者批量化
$ head getIndex.sh
for id in `cat id.txt`
do
  samtools index ${id}.sorted.bam;
done





5. 找snp
对于sort和index过的bam文件
$ samtools pileup -vcf  ref.fa  aln.bam | tee raw.txt | samtools.pl varFilter -D100 > flt.txt
以上命令是寻找最大深度为100的SNP，raw.txt是原始SNP的文件

The -D option of varFilter controls the maximum read depth
$ awk '($3=="*"&&$6>=50)||($3!="*"&&$6>=20)' flt.txt > final.txt
以上是根据sam文件的第三列和第六例进行质量控制。这个根据自己设定的阈值，进行筛选。





========================================
|-- samtools其他功能: 可视化、统计、去重
----------------------------------------
4. tview 直观显示reads比对到基因组的情况，和基因组浏览器有点类似。
-d display		输出类型
-p chr:pos 		直接定位到该位置
-s STR		只显示该sample或group的reads

利用sort进行排序，再利用index建立索引后 samtools tview xx.sort.bam 



(1) 截取bam子集：按染色体、位置
view  从bam/sam文件中提取/打印部分比对结果。默认为所有的区域，也可以染色体区域（1-based，须sort并index）。
例如：
samtools view -bt ref_list.txt -o aln.bam aln.sam.gz
samtools view aln.sorted.bam chr2:20,100,000-20,200,000
samtools view aln.sorted.bam chr2:20,100,000-20,200,000 > sample.sam #到igv中查看

例：
samtools view -b -h B.addgroup.bam chr2 chr3 chr5 >B_others.bam
以空格分隔要截取的染色体数据，这样，得到比对到chr2，chr3，和chr5的部分bam文件，保存在B_others.bam文件中。
-h: 输出的sam文件带header，默认不带

samtools view -b -h 225.sort.bam chr14 >sample.bam
samtools view -b -h 225.sort.bam chr7 >sample.bam
samtools index sample.bam 
## 然后可以用IGV载入了。
## 太慢了，可以考虑多线程 -@ 20



(2) 截取 bam 子集，按照flag(就是sam的第2列)
https://www.bioinfo-scrounger.com/archives/245/

1 0x1 这序列是PE双端测序
2 0x2 这序列和参考序列完全匹配，没有错配和缺失
4 0x4 这序列没有mapping到参考序列上
8 0x8 这序列的mate序列没有mapping到参考序列上
16 0x10 这序列比对到参考序列的负链上
32 0x20 这序列的mate序列比对到参考序列的负链上
64 0x40 这序列是read1
128 0x80 这序列是read2
256 0x100 这序列不是【主要的比对】，因为序列可能比对到参考序列的多个位置上
512 0x200 这序列没有通过QC
1024 0x400 这序列是PCR重复序列
2048 0x800 这序列是补充比对

$ samtools view [options] in.sam|in.bam|in.cram [region...]
-f INT   only include reads with all  of the FLAGs in INT present [0]
-F INT   only include reads with none of the FLAGS in INT present [0]
-G INT   only EXCLUDE reads with all  of the FLAGs in INT present [0]

-f 提取 ## -f 4 提取出没有mapping上的reads
-F 过滤 ## -F 4 过滤掉没有mapping上的reads，也就是说提取出mapping上的reads
-u 输出格式为未压缩的bam格式
-q 过滤掉MAPQ值低某个阈值 ## -q 1 过滤掉MAPQ值低于1的情况
-h 设定输出的SAM文件带有header
-b 输出格式设定为BAM
-S 输入格式为SAM

按flag提取的示例
提取比对到参考序列的结果: samtools view -bF 4 tmp.bam > tmp_F.bam
提取双端序列都比对到参考序列（4+8）的结果：samtools view -bF 12 tmp.bam > tmp_F.bam
提取比对到chr1的结果 samtools view -b tmp.bam chr1 > tmp_chr1.bam

默认
$ samtools view map/c13ROW40_Aligned.sortedByCoord.out.bam|awk '{print $2}' | sort | uniq -c
   7777 0
   6124 16
   1632 256
   1258 272

只要比对到负链上的
$ samtools view -f 16 map/c13ROW40_Aligned.sortedByCoord.out.bam|awk '{print $2}' | sort | uniq -c
   6124 16
   1258 272

只要主要比对，次要比对去掉
$ samtools view -F 256 map/c13ROW40_Aligned.sortedByCoord.out.bam|awk '{print $2}' | sort | uniq -c
   7777 0
   6124 16

一下两条对比可见，MAPQ=255的全部是最佳比对(过滤前后counts一样)。而主要比对并不全是MAPQ=255的，为什么不是全部？
$ samtools view -F 256 map/c13ROW40_Aligned.sortedByCoord.out.bam|awk '{print $5}' | sort | uniq -c
    140 0
    438 1
  12234 255
   1089 3
$ samtools view map/c13ROW40_Aligned.sortedByCoord.out.bam|awk '{print $5}' | sort | uniq -c
    935 0
   1444 1
  12234 255
   2178 3








(3) 
samtools tview -p B05:53425172 accepted_hits.bam Bju.genome.fa
“.” 比对到正链;
“，” 表示比对到负链;
“<”或“>” 表示reference skip   RNA-seq当中内含子剪切;
"ATCGN"  表示正向mismatch;
"atcgn"  表示反向mismatch;
‘+[0-9]+[ACGTNacgtn]+’ insertion;
‘-[0-9]+[ACGTNacgtn]+’ 表示deletion;
“^”标记reads起始;
“$”标记reads segment结尾;

实例









5.flagstat 对reads的比对情况统计

samtools flagstat xx.sort.bam 





6.depth 每个碱基位点的测序深度
samtools depth [options] [in1.sam|in1.bam|in1.cram [in2.sam|in2.bam|in2.cram] [...]]

-a 输出所有的碱基深度，包括0
-b/-r 控制深度的范围(后面跟染色体)
-f bam文件名字
-l 设置read长度阈值
-d/-m 最大覆盖深度
-q 碱基质量阈值
-Q 比对质量阈值

samtools depth -a -r chr3 x.sort.bam 
#显示chr3染色体上所有碱基的测序深度，
第一列chr名字，第二列碱基位置，第三列测序深度。





7.mpileup 对参考基因组每个位点做碱基堆积，用于call SNP和INDEL。主要是生成BCF、VCF文件或者pileup一个或多个bam文件。比对记录以在@RG中的样本名作为区分标识符。如果样本标识符缺失，那么每一个输入文件则视为一个样本。

用法：
生成一个简单的vcf文件
samtools mpileup -vu test.sort.bam

如果有参考基因组的话
samtools mpileup -vuf genome.fasta  test.sort.bam



7.dict 建立参考基因组字典
samtools dict  test.sort.bam sequences.fa 




8.fastq bam文件转换为fastq
samtools fastq test.bam



9.idxstats 检索和打印与输入文件相对应的index file里的统计信息
Usage: samtools idxstats <in.bam>

用法：
samtools idxstats test.sort.bam
结果返回一个表格，4列。
第一列：染色体名
第二列：序列长度
第三列：比对上的reads数
第四列：未比对数目



10. stats 对bam文件做详细统计,其统计结果可用mics/plot-bamstats作图
samtools stats test.bam

输出的信息比较多，部分如下：
Summary Numbers，raw total sequences，filtered sequences, reads mapped, reads mapped and paired,reads properly paired等信息
Fragment Qualitites：根据cycle统计每个位点上的碱基质量分布
Coverage distribution：深度为1，2，3，，，的碱基数目
ACGT content per cycle：ACGT在每个cycle中的比例
Insert sizes：插入长度的统计
Read lengths：read的长度分布






11.rmdup 将由PCR duplicates 获得的reads去掉，并保留高比对质量的reads
-s    rmdup for SE reads
-S    treat PE reads as SE in rmdup (force -s)
用法：
samtools rmdup -sS test.bam  output.bam

单端测序结果去除PCR重复
$ samtools rmdup -s tmp.sorted.bam tmp.rmdup.bam

仔细探究samtools的rmdup是如何行使去除PCR重复reads功能的
http://www.bio-info-trainee.com/2003.html

只需要开始-s的标签， 就可以对单端测序进行去除PCR重复。其实对单端测序去除PCR重复很简单的~，因为比对flag情况只有0,4,16，只需要它们比对到染色体的起始终止坐标一致即可，flag很容易一致。

但是对于双端测序就有点复杂了~

很明显可以看出，去除PCR重复不仅仅需要它们比对到染色体的起始终止坐标一致，尤其是flag，在双端测序里面一大堆的flag情况，所以我们的94741坐标的5个reads，一个都没有去除！

这样的话，双端测序数据，用samtools rmdup效果就很差，所以很多人建议用picard工具的MarkDuplicates 功能~~~
The optimal solution depends on many factors - the consensus seems to be the the picard markduplicates could be the best current solution.
最优的方案依赖很多因素，最一致的似乎就是picard markduplicates 可以达到目前最好的结果。

The appropriateness of duplicate removal depends on coverage - one would want to only remove artificial duplicates and keep the natural duplicates.
去除重复的适当性取决于覆盖度——人们只希望去除人工重复并保留自然重复。

MarkDuplicates is "more correct" in the strict sense. Rmdup is more efficient simply because it does handle those tough cases. Rmdup works for single-end, too, but it cannot do paired-end and single-end at the same time. It does not work properly for mate-pair reads if read lengths are different.
MarkDuplicates功能在严格模式下更“正确”，rmdup更高效仅仅是因为它确实处理这些事情。rmdup对于SE数据有效，但是对于同时有PE和SE的无效，对于长度不等的 mate-pair数据也无效。






========================================
|-- 合并几个bam文件: samtools merge output.bam 1.bam 2.bam ...
----------------------------------------
例: 把c1-c4合并成total.bam文件 

$ samtools merge total.bam c1.bam c2.bam c3.bam c4.bam


例2: 把某个文件夹下的一类 bam 全部合并
$ samtools merge merged/total.bam `ls map_clean/*final.bam |xargs`



一般合并后要 index 一下：
$ samtools merge -@ $threads condition1.merged.bam sample1.bam sample2.bam sample3.bam
$ samtools index -@ $threads condition1.merged.bam






========================================
|-- How to calculate coverage using samtools
----------------------------------------

1. Command to calculate coverage from bam file:

$ samtools sort accepted_hits.bam -o sorted_accepted_hits.bam
$ samtools index -b sorted_accepted_hits.bam
$ samtools view -b sorted_accepted_hits.bam > accepted_reads.bam

$ samtools depth accepted_reads.bam > read_coverage.txt

ref: https://github.com/arefeen/TAPAS






========================================
|-- Python包pysam: htslib(samtools) interface for python
----------------------------------------
https://pysam.readthedocs.io/en/latest/index.html
Pysam is a python module for reading, manipulating and writing genomic data sets.

1.
>>> line2
<pysam.libcalignedsegment.AlignedSegment object at 0x7f68cb20f888>
>>> samfile
<pysam.libcalignmentfile.AlignmentFile object at 0x7f68ca3800d0>

2. line2的方法名：
['__class__', '__copy__', '__deepcopy__', '__delattr__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'aend', 'alen', 'aligned_pairs', 'bin', 'blocks', 'cigar', 'cigarstring', 'cigartuples', 'compare', 'flag', 'get_aligned_pairs', 'get_blocks', 'get_cigar_stats', 'get_overlap', 'get_reference_positions', 'get_reference_sequence', 'get_tag', 'get_tags', 'has_tag', 'infer_query_length', 'infer_read_length', 'inferred_length', 'is_duplicate', 'is_paired', 'is_proper_pair', 'is_qcfail', 'is_read1', 'is_read2', 'is_reverse', 'is_secondary', 'is_supplementary', 'is_unmapped', 'isize', 'mapping_quality', 'mapq', 'mate_is_reverse', 'mate_is_unmapped', 'mpos', 'mrnm', 'next_reference_id', 'next_reference_name', 'next_reference_start', 'opt', 'overlap', 'pnext', 'pos', 'positions', 'qend', 'qlen', 'qname', 'qqual', 'qstart', 'qual', 'query', 'query_alignment_end', 'query_alignment_length', 'query_alignment_qualities', 'query_alignment_sequence', 'query_alignment_start', 'query_length', 'query_name', 'query_qualities', 'query_sequence', 'reference_end', 'reference_id', 'reference_length', 'reference_name', 'reference_start', 'rlen', 'rname', 'rnext', 'seq', 'setTag', 'set_tag', 'set_tags', 'tags', 'template_length', 'tid', 'tlen', 'tostring']

3. 常用方法
line2.get_tags()[2]
a=line2

#获取所有标签，并给出第n个。缺点：这些数据并不是标准化的，标签位置不完全一样。
a.get_tags()[9] #('CB', 'ACACCCTCATCGGACC-1')
a.get_tags()[12] #('UB', 'ATACATGGTA')

line.get_tag("NH")!=1 #标签NH是否为1
line.has_tag("CB") #是否有CB标签

line.cigarstring # 匹配情况 1S74M
line.flag #16 表示负链， 0表示正链



4.实例代码：从bam文件中逐行读取，NH不等于1的不要，不包含键CB或UB的不要，CB不在预定列表内的不要，
通过三种过滤的条目，保存到一个bam文件。

$ cat filterBAMby3rules_B116.py
################
#第3个脚本，按照NH、CB_UB、cell barcode list过滤bam文件
################
import re
import time
import os


######### 需要修改的文件名
projectName="B116"
path_in = '../hg19_B116/outs/'
fcb=open("B116"+"_CellBarCode_list.txt",'r') #cell barcode部分
########

#读取cb
cbset=set();#cb保存的地方
for lineR in fcb.readlines():
    arr=re.split(" ",lineR.strip())
    cbset.add( arr[0] )
print(len(cbset)) #5973 cell barcode


#读取bam文件
import pysam
samfile = pysam.AlignmentFile(path_in+"possorted_genome_bam.bam", "rb")
#写入bam文件
samOut=pysam.AlignmentFile(projectName+"_NH_CB_list_filtered.bam", "wb",template=samfile)

print("begin for") #
i=0
for line in samfile:
    i=i+1
    ######################
    #进度条
    ######################
    if i%1000000==0:
        tstr=time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        print( tstr+" Processing line:",i)
    #if i>10000:
    #    pass
        #break

    #########
    #三次过滤
    #1
    if line.get_tag("NH")!=1:
        continue;
    #2
    if (not line.has_tag("CB")) or (not line.has_tag("UB")):
        continue;
    #3
    cb=line.get_tag("CB")
    if cb not in cbset:
        continue;

    #########
    #写入文件
    samOut.write(line)


#关闭文件
fcb.close()
samfile.close()
samOut.close()

print("===the End===")











========================================
|-- samtools faidx 命令: 生成fa索引，或者提取提取fa中的序列
----------------------------------------
1.对fasta 序列建立一个后缀为.fai 的文件

$ samtools faidx xx.fa
该命令对输入的fasta序列有一定要求：对于每条序列，除了最后一行外， 其他行的长度必须相同

$ xx.fa
>one
ATGCATGCATGCATGCATGCATGCATGCAT
GCATGCATGCATGCATGCATGCATGCATGC
ATGCAT
>two another chromosome
ATGCATGCATGCAT
GCATGCATGCATGC

最后生成的.fai文件如下， 共5列，\t分隔；
one	66	5	30	31
two	28	98	14	15

第一列 NAME   :   序列的名称，只保留“>”后，第一个空白之前的内容；
第二列 LENGTH :   序列的长度， 单位为bp；
第三列 OFFSET :   第一个碱基的偏移量， 从0开始计数，换行符也统计进行；
第四列 LINEBASES : 除了最后一行外， 其他代表序列的行的碱基数， 单位为bp；
第五列 LINEWIDTH : 行宽， 除了最后一行外， 其他代表序列的行的长度， 包括换行符。
	在linux系统中换行符为\n, 要在序列长度的基础上加1；
	在windows系统中换行符为\r\n, 要在序列长度的基础上加2；



(2) 提取序列：
$ samtools faidx input.fa chr1 > chr1.fa
$ samtools faidx input.fa chr1:100-200 > chr1.fa

测试
$ samtools faidx xx.fa one 
>one
ATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGC
ATGCAT


对于UCSC的chr肯定是可以用的：
>chr1
>chr2
...

对于ensemble呢？可行
>1 dna:chromosome chromosome:GRCh38:1:1:248956422:1 REF
>2 dna:chromosome chromosome:GRCh38:2:1:242193529:1 REF


# samtools faidx input.fa 1 > chr1.fa
# head -n 2 chr1.fa

# samtools faidx input.fa 1 2 3 > chr1+2+3.fa
# samtools faidx input.fa chr1 chr2 chr3 > chr1_2_3.fa 


提取all：
$ samtools faidx GRCh37.p13.genome.fa chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY > GRCh37.chr.fa







========================================
提取参考基因组某位置的碱基(4种方法)： 根据基因组坐标获得碱基序列(注意bed文件是0-based)
----------------------------------------

1. 利用samtools faidx
samtools faidx 常常用来对参考基因组建立索引，但它还有个鲜为人知的功能，就是序列提取，如下：
-i, --reverse-complement Reverse complement sequences. 反向互补


(1)实例1：(+链上)获得bam文件后面的几个碱基
$ samtools view rmdup_c16_ROW17.bam |head -n 25008|tail -n 1
E00300:165:H3CMMALXX:5:1118:27082:64896	0	chr14	56078824	255	115M	*	0	0	GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAG	AAFFFKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKFFKKKKKKKFKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK	NH:i:1	HI:i:1	AS:i:113	nM:i:0


$ bedtools bamtobed -i rmdup_c16_ROW17.bam |head -n 25008|tail -n 1
chr14	56078823	56078938	E00300:165:H3CMMALXX:5:1118:27082:64896	255	+

结论： bed的坐标是0-based，比原始bam文件(和genome坐标)小1。 



#samtools根据sam坐标获取序列：起点bed比bam小1， 终点56078824+115-1=56078938 同bed。
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr14:56078824-56078938
>chr14:56078824-56078938
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCA
AAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag
##去掉换行符和第一行，多行变一行。和bam文件中的目标序列一致: 
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr14:56078824-56078938 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}' 
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag

查看下游10nt在genome上是什么碱基？(start=oldEnd 算一个， end=start+10-1)
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr14:56078938-56078947 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}' 
gaaaagaaaa #不能包含这个上一个的结束，此后的序列，则应该末尾坐标+1。

#start=oldEnd+1; end=start+10-1
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr14:56078939-56078948 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}'
aaaagaaaaa  #该位置后面在基因组上有4个A。



2)搜索原始RNAseq raw data，发现该序列后面是polyA。不过polyA前面多事GA或TA，而refer后面也有A，不过没有RNA那么多，也就是无法判断从哪里断开的。
$ grep -i GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag c16_ROW17.10A.fq 
CCTTCAATAGTTATTACAGTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGAAAAAAAAAAAAAAAAA
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGAAGG
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGTAAAAAAAAAAAAAAAATAAAAATAAAAAAAATTT
AGTTATTACAGTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGTAAAAAAAAAAAAAAAAAAAAAAAA
CAGTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGTAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA





(2)实例2：(-链上)获得bam文件后面的几个碱基
$ samtools view rmdup_c16_ROW17.bam |head -n 10000|tail -n 1
E00300:165:H3CMMALXX:5:1110:4208:28294	16	chr11	65651393	255	49M	*	0	0	CCAAACTCAGAGCAACTTTATTGTCAGCATGGGCGGAGCGTTGGGAGGC	KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKFFFAA	NH:i:1	HI:i:1	AS:i:46	nM:i:1
[wangjl@bio_svr1 MAPQ255]$

$ bedtools bamtobed -i rmdup_c16_ROW17.bam |head -n 10000|tail -n 1
chr11	65651392	65651441	E00300:165:H3CMMALXX:5:1110:4208:28294	255	-

#samtools根据sam坐标获取序列：起点bed比bam小1， 终点 65651393+49-1=65651441 同bed。
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr11:65651393-65651441
>chr11:65651393-65651441
CCAAACTCAGAGCAACTTTATTGTCAGCGTGGGCGGAGCGTTGGGAGGC

CCAAACTCAGAGCAACTTTATTGTCAGCA (from RNA seq). RNA结果其实和refer相差一个碱基


查看下游10nt在genome上(因为是负链，其实是genome上游)是什么碱基？(end=oldStart-1, start=start-10+1)
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr11:65651389-65651398 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}'
GTCTCCAAAC #测试发现向右多延伸5个碱基，出现CCAAAC结尾，3'端多6个碱基。

$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr11:65651383-65651392 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}'
GGACCAGTCT
#右端没有出现genomic polyT。


2)bam是按照refer的顺序写的RNA序列，序列保留了RNA的突变信息。+基因和RNA相同，-链基因则和RNA序列反向互补。
搜原始RNA raw data时，需要求bam序列的反向互补序列。
$ grep GCCTCCCAACGCTCCGCCCATGCTGACAATAAAGTTGCTCTGAGTTTGG c16_ROW17.10A.fq 
GCCTCCCAACGCTCCGCCCATGCTGACAATAAAGTTGCTCTGAGTTTGGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAATCGGCAGATGCAGATGGGAAGAGCGTCGGGTGGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATTA







(3)根据 序列ID 查找序列(fq/sam/bed)
$ head -n 1272166 c16_ROW17.fq|tail -n 1
GTAGATGCTATAATAAAAATAGCTGTTTGGTAACCATAGTTTCACTTGTTCAAAGCTGTGTAATCGTGGGGGTACCATCTCAACTGCTTTTGTATTCATTGTATTAAAAGAATCTGTTTAAACAACAAAAAAAAAAAAAAAAAAAAAAAA
$ head -n 1272165 c16_ROW17.fq|tail -n 1
@E00300:165:H3CMMALXX:4:2209:10774:12666 2:N:0:TAGGCATG

$ grep E00300:165:H3CMMALXX:4:2209:10774:12666 2:N:0:TAGGCATG  c16_ROW17.sam
grep: 2:N:0:TAGGCATG: No such file or directory #说明到空格name就断开了
c16_ROW17.sam:E00300:165:H3CMMALXX:4:2209:10774:12666	0	chr5	10265001	255	126M	*	0	0	GTAGATGCTATAATAAAAATAGCTGTTTGGTAACCATAGTTTCACTTGTTCAAAGCTGTGTAATCGTGGGGGTACCATCTCAACTGCTTTTGTATTCATTGTATTAAAAGAATCTGTTTAAACAAC	AAFFFKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKFKKKKKKKKKKKKKKKKKKKKKKKKKKFKKKKKKKKKKKKKKKKKAFKKKK	NH:i:1	HI:i:1	AS:i:124	nM:i:0

$ grep E00300:165:H3CMMALXX:4:2209:10774:12666 h_c16_ROW17.bed 
chr5	10265000	10265126	E00300:165:H3CMMALXX:4:2209:10774:12666	255	+











2. 利用bedtools getfasta (只能输出到文件，但可以批量处理)
bedtools说明文档中对getfasta的描述是“Extract DNA sequences into a fasta file based on feature coordinates.”显而易见，bedtools getfasta的功能就是根据坐标信息提取序列信息。操作如下：


需要准备bed文件，至少三列chr start end。
$ cat pos.bed 
chr14	56078823	56078938	E00300:165:H3CMMALXX:5:1118:27082:64896	255	+

$ bedtools getfasta -fi /home/wangjl/data/ref/hg19/hg19.fa -bed pos.bed -s -fo result.txt
bedtools getfasta有三个必选参数：
-fi即参考基因组fasta文件；
-bed即需要提取的位置坐标信息，格式：chr\tstart\tend；
-fo：输出文件。
-s	Force strandedness. If the feature occupies the antisense, strand, the sequence will be reverse complemented. 根据第六列信息，如果是-则给出反向互补序列。
	- By default, strand information is ignored. 
	不加-s则默认忽略掉链方向，也就是都按照+处理。
有一点需要说明，bedtools接收的是bed文件，而bed文件是0-based。
要获取chr1:n位点的序列，就需要减去1，前闭后开区间 chr1:(n-1)-n

$ cat result.txt 
>chr14:56078823-56078938(+)
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag


(2)如果位置文件有很多行bed格式数据，则能批量获取reads。









3. 利用Python3的pysam模块
import pysam 
ref=pysam.FastaFile("/home/wangjl/data/ref/hg19/hg19.fa")
chr,start,end='chr14','56078823','56078938' #如果是bed文件坐标，则直接用；如果是sam文件坐标，start要减1。
print(ref.fetch(chr)[int(start)-0: int(end)])

## 
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag








4.使用UCSC的web接口
http://genome.ucsc.edu/cgi-bin/das/hg19/dna?segment=chr14:56078824,56078938
说明： 使用的起始位置是bam文件中的，比bed文件大1。end=start+115-1，和bed一致。

去除空格后，和上文序列一致。
gtaattttcctcttcttctggcttttcatgaaagaaacattatatgatgaagttcttgcaaaacagaaaagagaacaaaagcttattcctaccaaaacagataaaaagaaagcag








5. 性能比较
三种方法耗时如下：
 	  samtools	bedtools	python
time	0.002s	0.034s	2.077s
可以看出，samtools和bedtools的性能很好，python的性能就比较尴尬了。

其实，个人比较推荐bedtools，比较容易进行批量处理，把想处理的位置信息写到输出文件，然后就可以轻松的进行序列提取。


refer: https://blog.csdn.net/whenfree/article/details/85305616






========================================
判断测序文库的链特异性: IGV法, RSeQC统计法
----------------------------------------

#################
#一、IGV法
#################

1.官网
http://www.igv.org/
http://software.broadinstitute.org/software/igv/

官方培训文档： http://www.igv.org/workshops/




#IGV color: A(009600)green; T(FF0000)red; G(D17105)orange; C(0000FF)blue



2. 安装 
(1)
略


3.首先，需要辨明正反链，正义/反义链，编码链，模板链的概念。

DNA 的正链和负链，就是那两条反向互补的链。参考基因组给出的那个链就是所谓的正链（forword），另一条链是反链（reverse）。但是这正反一定不能和正义链（sense strand）反义链（antisense strand）混淆，两条互补的DNA链其中一条携带编码蛋白质信息的链称为正义链，另一条与之互补的称为反义链。但是携带编码信息的正义链不是模板，只是因为它的序列和RNA相同，正义链也是编码链。而反义链虽然和RNA反向互补，但它可是真正给RNA当模板的链，因此反义链也是模板链。

总结两点
- 正义链（sense strand）= 编码链（coding strand）= 非模板链
- forword strand 上可以同时有sense strand 和 antisense strand。因为这完全是两个不同的概念。


(2)最后谈谈正链和正义链。
正链一般是固定的，DNA双链中上面这条链就是正链（+）。
正义链是相对的，它是依据基因的转录方向看的，向右转录（👉）的基因，正链即是它的正义链或有义链或非模板链，向左转录（👈）的基因，负链即是它的正义链。





4.操作步骤
(1).使用star比对获得bam文件
然后对bam文件 sort和index。
如果是star获得的sort过的bam，只需要再用samtools index即可。


(2).使用IGV看比对结果
1) 向IGV中导入参考基因组
Genomes -> Creat a .genome file…
在弹出窗口中加入基因组序列fasta文件与基因组结构注释gff/gtf文件

2)导入比对文件
File -> Load from File…
比对的bam/sam文件需要具有索引
左侧右键，勾上“show coverage track”，即可看到比对上的reads数目和方向。



可能还需要选择菜单 Tools - run igvtools, 把bam转变成tdf文件。
关掉igv，再重新load bam文件，才能看到reads的峰值。




3)检验转录组文库是否具有链特异性

(右键track, 选color alignments by | read strand )
在IGV的Read strand模式中，显示的reads分为红蓝两色，其中红色代表read方向与DNA正链方向相同(5’ -> 3’)，蓝色代表read方向与DNA正链方向相反


如果是双端测序：在First-of-pair strand模式中，红色代表成对的reads中，第一链的方向与正链相同(5’ -> 3’)，蓝色代表成对的reads中，第一链的方向与正链相反(5’ -> 3’)。这对于展示链特异性的文库特别有帮助。
也就是说双末端测序，第一次测得是反义链，第二次测得是正义链。这应该就是链特异性建库。



### For a given transcript, non-directional libraries will show a mix of red and blue reads aligning to the locus.
Directional libraries will show reads of one color in the direction matching the transcript orientation.
### 对于非链特异性的文库，匹配到同一个基因的reads会表现出红蓝混合的情况；
### 对于链特异性的文库，匹配到同一个基因的reads则会表现出与转录本方向相匹配的颜色。




refer:
https://sr-c.github.io/2018/11/06/STAR/
http://www.omicsclass.com/article/300


什么是链特异性建库？https://www.jianshu.com/p/a63595a41bed





#################
#二、使用RSeQC统计
#################
http://rseqc.sourceforge.net/#infer-experiment-py


$ infer_experiment.py -r /home/wangjl/data/ref/hg19/hg19_ucsc_genes-20190509.bed -i c16_ROW17.bam
Reading reference gene model /home/wangjl/data/ref/hg19/hg19_ucsc_genes-20190509.bed ... Done
Loading SAM/BAM file ...  Total 200000 usable reads were sampled


This is SingleEnd Data
Fraction of reads failed to determine: 0.0717
Fraction of reads explained by "++,--": 0.9136
Fraction of reads explained by "+-,-+": 0.0147






========================================
MACS(Model-based Analysis of ChIP-Seq) 的安装
----------------------------------------
1. MACS 1.4.2
http://liulab.dfci.harvard.edu/MACS/Download.html

最新版：
https://github.com/macs3-project/MACS/tags

MACS全称Model-based Analysis of ChIP-Seq，最初的设计是用来鉴定转录因子的结合位点，但是它也可以用于其他类型的富集方式测序。


子命令: 
- bdgcmp使用 *_treat_pileup.bdg和 *_control_lambda.bdg计算得分轨(score track)
- bdgpeakcall使用 *_treat_pvalue.bdg 或bdgcmp得到的结果或begGraph文件进行peak calling.bdgbroadcall差不多也是这样子。
- bdgdiff能用来分析4个bedgraph文件,得到treatment1 vs control1, treatment2 vs control2, treatment1 vs control2, treament2 vs control1的得分。
- filterdup：过滤重复，结果是BED文件
- predictd：从比对文件中估计文库大小或d
- randsample： 随机抽样
- pileup：以给延伸大小去堆积(pileup)比对得到的reads。这一步不会有去重和测序深度标准化，你需要预先做这些工作。





2. MACS2
(1)下载和安装
https://pypi.org/project/MACS2/
https://github.com/taoliu/MACS/releases

好像可以用pip安装
$ pip install MACS2 #报错，必须python2.7
## 再试
$ python2 -V ## Python 2.7.5
$ pip2 install --user MACS2 ##Successfully installed MACS2-2.1.2

查看版本号
$ macs2 --version
macs2 2.1.2

2) on Ubuntu, CentOS: (2021.2.22)
$ pip3 install --user MACS2

$ macs2 --version
macs2 2.2.7.1



(2)都是输入bam文件，能输入BED文件吗？可以
The BED format can be found at [UCSC genome browser website](http://genome.ucsc.edu/FAQ/FAQformat#format1).
chr7    127471196  127472363  Pos1  0  +
chr7    127472363  127473530  Pos2  0  -

The essential columns in BED format input are the 1st column "chromosome name", the 2nd "start position", the 3rd "end position", and the 6th, "strand".

1) For BED format, the 6th column of strand information is required by MACS. And please pay attention that the coordinates in BED format is zero-based and half-open (http://genome.ucsc.edu/FAQ/FAQtracks#tracks1).
bed的坐标是从0开始的，前闭后开区间。



(3) 运行命令
$ macs2 callpeak -t ChIP.bam -c Control.bam -f BAM -g hs -n test -B -q 0.05

参数解释：
-t/--treatment FILENAME
这是MACS唯一必须的参数，文件可以是 --format 选项指定的任何格式。如果有多个比对文件，可以将它们指定为 -t A B C 。MACS 会将所有这些文件合并在一起。

-c/--control
	control 或 mock(非特异性抗体，如IgG)组
	control: input DNA，没有经过免疫共沉淀处理；
	mock: 1）未使用抗体富集与蛋白结合的DNA片段 2）非特异性抗体，如IgG
-n/--name
	MACS 输出文件名前缀。
-f/--format FORMAT
	声明输入文件的格式，目前 MACS 能够识别的格式有 ELAND、BED、ELANDMULTI、ELANDEXPORT、ELANDMULTIPET（双端测序）、SAM、BAM、BOWTIE、BAMPE、BEDPE。除了BAMPE和BEDPE需要额外声明，其他格式都可以用 AUTO 自动检测。
-g/--gsize
	有效基因组大小(可比对基因组大小)；基因组中有大量重复序列测序测不到，实际上可比对的基因组大小只有原基因组90% 或 70%；人类默认值是– 2.7e9（UCSC human hg18 assembly）
-s/--tsize
	测序读长；如果不设定，MACS 利用输入的 treatment 文件前10个序列自动检测；设定会覆盖自动检测的标签大小。
-q/--qvalue
	q 值（最小的 FDR）的阈值，默认是 0.05 。可以根据结果进行修正，q 值是 p 值经 Benjamini-Hochberg-Yekutieli 修正后的值。
-p/--pvalue
	p 值，如果 -p 设定，MACS2会使用 p 值代替 q 值。
--verbose
	隐藏MACS运行过程信息，设置0；想了解各条染色体peak信息，设置为3或>3的数。
#
https://github.com/taoliu/MACS/











3. 更多参数

(0) MACS 参数解释： 
$ macs2 callpeak
usage: macs2 callpeak [-h] -t TFILE [TFILE ...] [-c [CFILE [CFILE ...]]]
                      [-f {AUTO,BAM,SAM,BED,ELAND,ELANDMULTI,ELANDEXPORT,BOWTIE,BAMPE,BEDPE}]
                      [-g GSIZE] [-s TSIZE] [--keep-dup KEEPDUPLICATES]
                      [--outdir OUTDIR] [-n NAME] [-B] [--verbose VERBOSE]
                      [--trackline] [--SPMR] [--nomodel] [--shift SHIFT]
                      [--extsize EXTSIZE] [--bw BW] [--d-min D_MIN]
                      [-m MFOLD MFOLD] [--fix-bimodal] [-q QVALUE | -p PVALUE]
                      [--scale-to {large,small}] [--down-sample] [--seed SEED]
                      [--tempdir TEMPDIR] [--nolambda] [--slocal SMALLLOCAL]
                      [--llocal LARGELOCAL] [--max-gap MAXGAP]
                      [--min-length MINLEN] [--broad]
                      [--broad-cutoff BROADCUTOFF] [--cutoff-analysis]
                      [--call-summits] [--fe-cutoff FECUTOFF]
                      [--buffer-size BUFFER_SIZE] [--to-large] [--ratio RATIO]
macs2 callpeak: error: the following arguments are required: -t/--treatment

更详细的参数解释
$ macs2 callpeak --help




i)输入文件参数：
-f BAM 输入文件格式，支持bam,sam,bed等
	{AUTO,BAM,SAM,BED,ELAND,ELANDMULTI,ELANDEXPORT,BOWTIE,BAMPE,BEDPE}
	除'BAMPE', 'BEDPE'需要特别声明外，其他格式都可以用 AUTO自动检测。

-g hs 人类基因组
	hs: 2.7e9
	mm: 1.87e9
	ce: 9e7
	dm: 1.2e8
	对于其他物种，则需要自己指定有效基因组的大小，单位为bp。
-t:实验组，IP的数据文件
-c: 对照组

--keep-dup all #要不要去重看你之前的处理，如果前面已经去过可以不用去重。

--nolambda：不考虑peak 候选区域的偏差，使用背景λ作为 localλ。


ii)输出文件参数：
--outdir macs2_result/01/ -n total 文件夹、文件名前缀

-B/--bdg:输出bedgraph格式的文件，输出文件以NAME+'_treat_pileup.bdg' for treatment data, NAME+'_control_lambda.bdg' for local lambda values from control显示。
	以bedGraph格式存放fragment pileup, control lambda, -log10pvalue 和log10qvale.
-B, --bdg   Whether or not to save extended fragment pileup, and local lambda tracks (two files) at every bp into a bedGraph file. DEFAULT: False
	是否输出2个bedGraph文件: 每个bp位置的扩展片段堆叠文件，本地lambda轨道。


--SPMR    If True, MACS will SAVE signal per million reads for fragment pileup profiles. 
	该参数如果为 True，MACS 会为片段重叠谱保存数据 signal per million reads。

	It won't interfere with computing pvalue/qvalue during peak calling, since internally MACS2 keeps using the raw pileup and scaling factors between larger and smaller dataset to calculate statistics measurements. 
	这个不干扰peak calling 过程中的p值/q值的计算，因为内部MACS2使用 raw pileup 和大小数据集之间的缩放因子做统计测试。

	If you plan to use the signal output in bedGraph to call peaks using bdgcmp and bdgpeakcall, you shouldn't use this option because you will end up with different results.
	如果你计划输出这个信号，然后使用 bdgcmp and bdgpeakcall call peak，不要加这个选项，因为你会得到不同的结果。

	However, this option is recommended for displaying normalized pileup tracks across many datasets. 
	然而，在很多数据集中展示标准化后的堆积轨道图时，推荐加该参数。

	Require -B to be set. Default: False
	需要设置-B参数。默认 False。




iii)peak calling 参数
-q/--qvalue 和 -p/--pvalue: q value默认值是0.05，与pvalue不能同时使用。

--nolambda: 不要考虑在峰值候选区域的局部偏差/λ
--nolambda      If True, MACS will use fixed background lambda as local lambda for every peak region. 
	Normally, MACS calculates a dynamic local lambda to reflect the local bias due to the potential chromatin accessibility.
	如果True，对于每个peak区域，macs使用固定的背景 lambda 作为局部lambda。
	通常，macs计算每个动态局部 lambda 来反应可能的染色质可接近性引起的局部偏差。


一般常规是够用的，但是如果你需要看那些更加宽的peak，可以按照官方的建议使用如下参数
--broad  peak有narrow peak和broad peak, 设置时可以call broad peak 的结果文件。
	broad region最大长度是 4d。其中d表示MACS的双峰模型两个peak的距离。结果会得到BED12格式文件，存放着附近高度附近的区域。由于要足够的宽，所以需要专门的参数进行统计学过滤。

--broad-cutoff 和pvalue、以及qvalue相似
	用于过滤 broad得到的peak，默认是q值，如果设置 -p就用p值。

--max-gap MAXGAP  Maximum gap between significant sites to cluster them together. The DEFAULT value is the detected read length/tag size.
	峰最大间隔。默认值时检测到的reads的长度。



iv)Shift 模型参数：
Shifting model arguments:
--nomodel    Whether or not to build the shifting model. 
If True, MACS will not build model. by default it means shifting size = 100, try to set extsize to change it. 
It's highly recommended that while you have many datasets to process and you plan to compare different conditions, aka differential calling, use both 'nomodel' and 'extsize' to make signal files from different datasets comparable. DEFAULT: False



--shift SHIFT   (NOT the legacy --shiftsize option!) The arbitrary shift in bp. 
	Use discretion while setting it other than default value. 
	When NOMODEL is set, MACS will use this value to move cutting ends (5') towards 5'->3' direction then apply EXTSIZE to extend them to fragments. 
	When this value is negative, ends will be moved toward 3'->5' direction. 
	Recommended to keep it as default 0 for ChIP-Seq datasets, or -1 * half of EXTSIZE together with EXTSIZE option for detecting enriched cutting loci such as certain DNAseI-Seq datasets. 
	Note, you can't set values other than 0 if format is BAMPE or BEDPE for paired-end data. DEFAULT: 0.
	shift推荐设置：
		ChIP-Seq数据使用 0 
		或者 -0.5*extsize: detecting enriched cutting loci such as certain DNAseI-Seq datasets
	对于 BAMPE or BEDPE 数据， shift 只能是0.



--extsize EXTSIZE     The arbitrary extension size in bp. 
	When nomodel is true, MACS will use this value as fragment size to extend each read towards 3' end, then pile them up. 
	It's exactly twice the number of obsolete SHIFTSIZE. 
	In previous language, each read is moved 5'->3' direction to middle of fragment by 1/2 d, then extended to both direction with 1/2 d. 
	This is equivalent to say each read is extended towards 5'->3' into a d size fragment. DEFAULT: 200. 
	EXTSIZE and SHIFT can be combined when necessary. Check SHIFT option.

						
--nomodel  这个参数和extsize、shift是配套使用的，有这个参数才可以设置extsize和shift。
--shift  当设置了--nomodel，MACS用这个参数从5' 端移动剪切，然后用--extsize延伸，如果--shift是负值表示从3'端方向移动。
	建议ChIP-seq数据集这个值保持默认值为0，对于检测富集剪切位点如DNAsel数据集设置为EXTSIZE的一半。
--extsize  当设置了nomodel时，MACS会用--extsize这个参数从5'->3'方向扩展reads修复fragments。比如说你的转录因子结合范围200bp，就设置这个参数是200。


-m MFOLD MFOLD, --mfold MFOLD MFOLD
	Select the regions within MFOLD range of high-confidence enrichment ratio against background to build model. 
	选择这些高可信 与背景富集比 区域构建model。

	Fold-enrichment in regions must be lower than upper limit, and higher than the lower limit. Use as "-m 10 30". 
	富集倍数必须比上限低，比下限高。

	This setting is only used while building the shifting model. 
	该设置仅用于构建 shifting model.

	Tweaking it is not recommended. DEFAULT:5 50



v) Post-processing options:
--call-summits  If set, MACS will use a more sophisticated signal processing approach to find subpeak summits in each enriched peak region. DEFAULT: False
	设置这个参数时，MACS 会使用更复杂的 信号处理方法，找子峰。默认不找。


















4. 实战、输出解读
https://www.jianshu.com/p/edfe4ac6b085
macs2 callpeak -t pas.bed -f BED --outdir out_dir2 -n pas_ -g hs -q 0.05

$ macs2 callpeak -t N5_NH_CB_list_filtered_c1.bam --outdir macs2 -n c1_ -g hs --keep-dup all -q 0.05 --nolambda --nomodel --shift 0
$ macs2 callpeak -t N5_NH_CB_list_filtered_c5.bam --outdir macs2 -n c5_ -g hs --keep-dup all -q 0.05 --nolambda --nomodel --shift 0

$ samtools view N5_NH_CB_list_filtered_c5.bam | awk '{print $2}'| sort | uniq -c
3777423 0
4048925 1024
2546823 1040
3038720 16

$ samtools view N5_NH_CB_list_filtered_c5.bam | head

1)后缀为xls的文件是peak的输出结果
# 开头的是注释信息，显示了软件调用的具体命令和参数设置，便于核查；
其他的行记录了peak的区间信息，这里的起始位置采用的是从1开始计数的方式。

$ grep -v '^#' c1__peaks.xls |head
chr     start   end     length  abs_summit  pileup  -log10(pvalue)  fold_enrichment -log10(qvalue)  name
1       14519   14802   284     14671       44      36.5958          11.5829         34.2358        c1__peak_1
1       185318  185528  211     185429      26      16.8188          6.94973         14.6765        c1__peak_2
1       629032  629507  476     629231      12857   41338.7          3309.62         41334          c1__peak_3
1       629870  630087  218     629940      22      13.0267          5.92014         10.9504        c1__peak_4

2) 后缀为narrowpeak的文件是BED6+4格式，内容示意如下。
$ head c1__peaks.narrowPeak 
##我加一行数字
#1      2       3       4               5       6       7        8       9      10
1       14518   14802   c1__peak_1      342     .       11.5829 36.5958 34.2358 152
1       185317  185528  c1__peak_2      146     .       6.94973 16.8188 14.6765 111
1       629031  629507  c1__peak_3      413340  .       3309.62 41338.7 41334   199
1       629869  630087  c1__peak_4      109     .       5.92014 13.0267 10.9504 70

前四列代表peak区间和名称(chr, start, end, peakName)，注意bed格式中起始位置从0开始计数，

第五列代表score,在macs2的输出结果中为 int(-10*log10qvalue) ==> 大概是 10*第9列

第六列strand 用+/- 表示链或者方向。如果是“.”则代表没有指定方向。macs2的输出结果中为.
第七列为 signalvalue， 通常使用fold_enrichment的值，
第八列为-log10(pvalue),
第九列为-log10(qvalue),
第十列为peak的中心，relative summit position to peak start
	即summit距离peak起始位置的距离，对应abs_summit - start。


3)后缀为bed的文件为peak中心，即summit对应的bed文件，内容示意如下
$ head c1__summits.bed 
1       14670   14671   c1__peak_1      34.2358
1       185428  185429  c1__peak_2      14.6765
1       629230  629231  c1__peak_3      41334
1       629939  629940  c1__peak_4      10.9504

最后一列为-log10(qvalue)。






========================================
|-- MACS2 输出文件解读
----------------------------------------
ATAC-seq关心的是在哪切断，断点才是peak的中心，所以使用shift模型，--shift -75或-100
对人细胞系ATAC-seq 数据call peak的参数设置如下：
$ macs2 callpeak -t H1hesc.final.bam -n sample --shift -100 --extsize 200 --nomodel -B --SPMR -g hs --outdir Macs2_out 2> sample.macs2.log


MACS2输出文件解读
$ ls -lh macs2
total 280K
-rw-rw-r-- 1 wangjl wangjl  17K Jun 19 10:36 treat_vs_control-shift100-m_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  14K Jun 19 10:36 treat_vs_control-shift100-m_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  11K Jun 19 10:36 treat_vs_control-shift100-m_summits.bed
-rw-rw-r-- 1 wangjl wangjl 3.7K Jun 19 10:36 shift100-m.macs2.log
-rw-rw-r-- 1 wangjl wangjl  82K Jun 19 10:36 treat_vs_control-shift100-m_model.r
-rw-rw-r-- 1 wangjl wangjl  21K Jun 19 10:39 treat_vs_control-shift100-m_model.pdf




1.NAME_peaks.xls
包含peak信息的tab分割的文件，前几行会显示callpeak时的命令。

$ grep "^c" macs2/treat_vs_control-shift100-m_peaks.xls|head
自己加的行2        3        4         5       6         7             8                9               10
chr   start      end      length  abs_summit  pileup  -log10(pvalue)  fold_enrichment  -log10(qvalue)  name
chr1  1013306   1013391   86      1013366     3        11.0199        3.98448          4.04898 macs2/treat_vs_control-shift100-m_peak_1
chr1  33037014  33037099  86      33037034    2       8.0082          2.98836          2.16415 macs2/treat_vs_control-shift100-m_peak_2
chr1  37692348  37692433  86      37692351    2       7.03643         2.97555          2.16415 macs2/treat_vs_control-shift100-m_peak_3
chr1  51236156  51236241  86      51236156    2       8.0082          2.98836          2.16415 macs2/treat_vs_control-shift100-m_peak_4
chr1  75786189  75786274  86      75786225    3       11.0199         3.98448          4.04898 macs2/treat_vs_control-shift100-m_peak_5

输出矩阵的列：
1 染色体号
2 peak起始位点
3 peak结束位点
4 peak区域长度
5 peak的峰值位点（summit position）
6 peak 峰值的堆积高度（pileup height at peak summit, -log10(pvalue) for the peak summit）
7 -log10(pvalue)
8 peak的富集倍数（相对于random Poisson distribution with local lambda）
	fold enrichment for this peak summit against random Poisson distribution with local lambda
9 -log10(qvalue)
10 峰编号

Coordinates in XLS is 1-based which is different with BED(0-based) format
XLS里的坐标和bed格式的坐标还不一样，起始坐标需要减1才与narrowPeak的起始坐标一样。



2.NAME_peaks.narrowPeak
*narrowPeak文件是BED6+4格式，可以上传到UCSC genome browser 浏览。输出文件每列信息分别包含：

$ cat macs2/treat_vs_control-shift100-m_peaks.narrowPeak|head
#自己加的一行2          3           4                                             5       6       7       8       9       10
chr1    1013305      1013391      macs2/treat_vs_control-shift100-m_peak_1        40      .       3.98448 11.0199 4.04898 60
chr1    33037013     33037099     macs2/treat_vs_control-shift100-m_peak_2        21      .       2.98836 8.0082  2.16415 20
chr1    37692347     37692433     macs2/treat_vs_control-shift100-m_peak_3        21      .       2.97555 7.03643 2.16415 3
chr1    51236155     51236241     macs2/treat_vs_control-shift100-m_peak_4        21      .       2.98836 8.0082  2.16415 0
chr1    75786188     75786274     macs2/treat_vs_control-shift100-m_peak_5        40      .       3.98448 11.0199 4.04898 36

1；染色体号
2：peak起始位点
3：结束位点
4：peak name
5：int(-10*log10qvalue)  ==> 大概是10*第9列。
6 ：正负链，一般全是点号。
7：fold change
8：-log10pvalue
9：-log10qvalue
10：峰位与peak起点的距离：relative summit position to peak start 
	==> 可以根据 第2列 + 第10列 算出来峰尖(summit)坐标。
	就是上文xml中的 abs_summit - start







3. NAME_summits.bed (找motif用这个文件)

BED格式的文件，包含peak的summits位置，第5列是-log10qvalue。如果想找motif，推荐使用此文件。
Remove the beginning track line if you want to analyze it by other tools.???

$ cat macs2/treat_vs_control-shift100-m_summits.bed|head
#我加的  2               3              4                                               5
chr1    1013365         1013366         macs2/treat_vs_control-shift100-m_peak_1        4.04898
chr1    33037033        33037034        macs2/treat_vs_control-shift100-m_peak_2        2.16415
chr1    37692350        37692351        macs2/treat_vs_control-shift100-m_peak_3        2.16415
chr1    51236155        51236156        macs2/treat_vs_control-shift100-m_peak_4        2.16415

1 chr 
2 summits的1位
3 summits的2位
4 name 
5 -log10qvalue  <==





4. .bdg
bedGraph格式，可以导入UCSC或者转换为bigwig格式。

两种bfg文件：
- treat_pileup: 实验组bedGraph 文件
- and control_lambda: 对照组bedGraph 文件




5. NAME_peaks.broadPeak
BED6+3格式与narrowPeak类似，只是没有第10列。


6. NAME_peaks.gappedPeak
BED12+3格式，存放broad region 和 narrow peaks，可以使用UCSC genome browser查看。


7. NAME_model.r
R程序，运行后在当前文件夹下，生成基于输入数据产生的模型图片
$ Rscript NAME_model.r






========================================
|-- macs2 bdgdiff: 不同条件下的差异peak分析
----------------------------------------
通过bdgdiff子命令来进行差异peak分析， 该命令不需要基于已有的peak calling结果，只需要输入每个样本对应的bedGraph格式的文件。

需要注意的是，该命令只针对两个样本间的差异peak进行设计，适用于没有生物学重复的情况。

https://github.com/macs3-project/MACS/wiki/Call-differential-binding-events


1. 第一步 Generate pileup tracks using callpeak module

$ macs2 callpeak -B -t cond1_ChIP.bam -c cond1_Control.bam -n cond1 --nomodel --extsize 120
$ macs2 callpeak -B -t cond2_ChIP.bam -c cond2_Control.bam -n cond2 --nomodel --extsize 120

在运行这一步的时候，会输出每个样本过滤之后的reads数目，示意如下

$ egrep "tags after filtering in treatment|tags after filtering in control" cond1_peaks.xls
 # tags after filtering in treatment: 19291269
 # tags after filtering in control: 12914669

$ egrep "tags after filtering in treatment|tags after filtering in control" cond2_peaks.xls
 # tags after filtering in treatment: 19962431
 # tags after filtering in control: 14444786




2. 第二步

$ macs2 bdgdiff \
	--t1 cond1_treat_pileup.bdg --c1 cond1_control_lambda.bdg \
	--t2 cond2_treat_pileup.bdg --c2 cond2_control_lambda.bdg \
	--d1 12914669 --d2 14444786 -g 60 -l 120 --o-prefix diff_c1_vs_c2

其中-d1和-d2的值就是第二步运行时输出的reads数目，-o参数指定输出文件的前缀。运行成功后，会产生3个文件
diff_c1_vs_c2_c3.0_cond1.bed
diff_c1_vs_c2_c3.0_cond2.bed
diff_c1_vs_c2_c3.0_common.bed






========================================
----------------------------------------





========================================
----------------------------------------








========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------








========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------


