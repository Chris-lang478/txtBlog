本地生信软件
	- 本地运行的生信软件
	- 包括可以下载安装的tool/VM/docker等

生物信息学常见1000个软件的安装代码
http://www.360doc.com/content/17/1014/11/42030643_694826019.shtml



========================================
Galaxy生信分析平台-搭建（本地化）
----------------------------------------
1.简介
https://usegalaxy.org/

Galaxy is an open source, web-based platform for data intensive biomedical research. If you are new to Galaxy start here or consult our help resources. You can install your own Galaxy by following the tutorial and choose from thousands of tools from the Tool Shed.




2.在本地搭建
http://www.bioinfo-scrounger.com/archives/683








========================================
测序数据质控 fastqc 及报告解读
----------------------------------------
1.download and install(for centOS7 no root)
http://www.bioinformatics.babraham.ac.uk/projects/fastqc/
http://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqc

(1)
$ wget http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.7.zip
$ unzip fastqc_v0.11.7.zip
$ cd FastQC
#添加运行权限
$ sudo chmod 755 fastqc
#添加软连接
$ ln -s /home/wangjl/software/FastQC/fastqc /home/wangjl/bin/fastqc

$ fastqc -v
FastQC v0.11.7
OK now.

安装：http://www.bioinformatics.babraham.ac.uk/projects/fastqc/INSTALL.txt
README：http://www.bioinformatics.babraham.ac.uk/projects/fastqc/README.txt



(2) 更多选项
$ fastqc --help  帮助
FastQC - A high throughput sequence QC analysis tool

fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN

-o用来指定输出文件的所在目录，注意是不能自动新建目录的。
	输出的结果是.zip文件，默认自动解压缩，命令里加上--noextract则不解压缩。
	--outdir=/some/other/dir/ 指定输出文件位置
-f用来强制指定输入文件格式，默认会自动检测。
-c用来指定一个contaminant文件，fastqc会把overrepresented sequences往这个
contaminant文件里搜索。contaminant文件的格式是"Name\tSequences"，#开头的行是注释。
加上 -q 会进入沉默模式，即不出现下面的提示：
Started analysis of target.fq
Approx 5% complete for target.fq
Approx 10% complete for target.fq


如果输入的fastq文件名是target.fq，fastqc的输出的压缩文件将是target.fq_fastqc.zip。解压后，查看html格式的结果报告


(3) 常用命令
$ nohup fastqc -o . -t 5 -f fastq SRR3101251.fastq &
-o . 表示输出文件位置，输出到fastq文件所在目录时可以忽略该参数
-t 5：表示开5个线程运行。每个thread分配250MB内存。32位系统最多6threads。
-f fastq 表示使用的输入文件格式，支持Valid formats are bam,sam,bam_mapped,sam_mapped and fastq

简单用法：
$ fastqc untreated.fq -o fastqc_out_dir/

当fastq文件比较多时，也可以批量执行：
$ fastqc /path_to_fq/*.fq -o fastqc_out_dir/

#多线程
$ fastqc -t 20 /home/wangjl/data/apa/fq_files/bc/c19_ROW13_R2.fastq -o /home/wangjl/data/apa/190610APA/QC_before_trim/

或者后台执行
$ nohup fastqc -t 10 ../fastq/19C001854_WES001_CapNGS_R1.fq -o fastqc_out/ >R1.log 2>&1 &




2)输入bam文件呢？
$ fastqc  -t 5 -o hg19_mm10_rmdup_QC/ -f bam hg19_mm10_rmdup/c01_ROW01.rmdup.bam








2.fastqc结果报告怎么看？
fastqc结果该怎么看及切割接头：
http://www.huangshujia.me/2017/08/25/2017-08-25-Begining-WGS-Data-Analysis-Fastq-Data-Quality-Control.html
https://www.plob.org/article/5987.html



(4) Per Base Sequence Content
对所有reads的每一个位置，统计ATCG四种碱基（正常情况）的分布：

横轴为位置，纵轴为百分比。 正常情况下四种碱基的出现频率应该是接近的，而且没有位置差异。因此好的样本中四条线应该平行且接近。当部分位置碱基的比例出现bias时，即四条线在某些位置纷乱交织，往往提示我们有overrepresented sequence的污染。当所有位置的碱基比例一致的表现出bias时，即四条线平行但分开，往往代表文库有bias (建库过程或本身特点)，或者是测序中的系统误差。
当任一位置的A/T比例与G/C比例相差超过10%，报"WARN"；当任一位置的A/T比例与G/C比例相差超过20%，报"FAIL"。


(9) Duplicate Sequences
统计序列完全一样的reads的频率。测序深度越高，越容易产生一定程度的duplication，这是正常的现象，但如果duplication的程度很高，就提示我们可能有bias的存在（如建库过程中的PCR duplication）。


(10) Overrepresented Sequences
如果有某个序列大量出现，就叫做over-represented。fastqc的标准是占全部reads的0.1%以上。和上面的duplicate analysis一样，为了计算方便，只取了fq数据的前200,000条reads进行统计，所以有可能over-represented reads不在里面。而且大于75bp的reads也是只取50bp。如果命令行中加入了-c contaminant file，出现的over-represented sequence会从contaminant_file里面找匹配的hit（至少20bp且最多一个mismatch），可以给我们一些线索。
当发现超过总reads数0.1%的reads时报”WARN“，当发现超过总reads数1%的reads时报"FAIL"。

建库接头序列 https://github.com/csf-ngs/fastqc/blob/master/Contaminants/contaminant_list.txt








3.Ubuntu1804 报错处理
(1)$ fastqc *.fastq &
Analysis complete for 19C001854_WES001_CapNGS_R1.fq
Exception in thread "Thread-1" java.awt.AWTError: Assistive Technology not found: org.GNOME.Accessibility.AtkWrapper
        at java.awt.Toolkit.loadAssistiveTechnologies(Toolkit.java:807)
        at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:886)
        at sun.swing.SwingUtilities2.getSystemMnemonicKeyMask(SwingUtilities2.java:2020)
        at javax.swing.plaf.basic.BasicLookAndFeel.initComponentDefaults(BasicLookAndFeel.java:1158)

解决方法:
$ sed -i 's/^assistive_technologies=/#&/' /etc/java-8-openjdk/accessibility.properties
Or just comment out below line in  /etc/java-8-openjdk/accessibility.properties
assistive_technologies=org.GNOME.Accessibility.AtkWrapper











========================================
|-- MultiQC: 批量显示QC结果的利器
----------------------------------------

1.安装
pip3 install multiqc
multiqc --help

$ fastqc --version
FastQC v0.11.7

$ multiqc --version
multiqc, version 1.7






2.使用 
(1) fastqc获得每个文件的QC报告
#单个文件：
$ fastqc untreated.fq -o fastqc_out_dir/

#批量化 先获取QC结果 *gz
ls ../R2_left/*.fastq | while read id; do fastqc -t 4 $id -o .; done


(2)multiqc 批量汇总QC报告
# multiqc
multiqc *fastqc.zip --pdf #需要提前安装某些包
# or
multiqc *fastqc.zip -o /path/to/store/




例子:
$ ls ../R2_left/c2_ROW01_R2.fastq | while read id; do fastqc -t 40 $id -o ./; done
$ multiqc *fastqc.zip -o /data/jinwf/wangjl/c1_2019APA/sc/combine/sc_fq/



refer:
https://blog.csdn.net/ada0915/article/details/77201871




========================================
测序文件预处理:切除接头adapter/primer等
----------------------------------------
1.概述
In addition, poly(A) (or poly(T) on the reverse strands) could also be removed by cutadapt or FASTXToolkit. 

Tools to remove adapter sequences from next-generation sequencing data
http://bioscholar.com/genomics/tools-remove-adapter-sequences-next-generation-sequencing-data/
https://www.biostars.org/p/98707/

1) Remove adapter sequences using fastX toolkit
2) Run fastQC to identify read quality and trim accordingly
3) Run Deconseq to remove contaminants
4) Remove short reads (<10nt)
5) Assemble using bowtie2 against reference (nb our strain is not exactly the reference but should similar enough)


去接头的软件包括 Trimmomatic、cutadapt、fastx_toolkit、fastp 等。


Q&A:
1.没必要去除polyA，因为不比对到任何地方?
https://www.biostars.org/p/148743/
Have you tried mapping the RNA-seq data yet? It may not be necessary to remove polyA stretches because they won't map to a unique location and you may already have enough reads to not worry about it. 




========================================
|-- 高通量测序数据质控神器—Trimmomatic （也是一个java程序）
----------------------------------------
高通量测序数据质控神器——Trimmomatic。这个于 2014 年发表在 Bioinformatics 上的软件，至今为止在 Web of Science 上可以检索到 2,098 次引用，而在谷歌学术上更是达到了惊人的 3,391 次：

这个软件为什么深受大家的喜爱呢？今天小编就给大家分析一下它在质控方面的强大之处。

1. 无脑安装、使用"简单"、运行速度可观
这个软件是用 Java 写的，运行效率比较高.


2. 强大的去接头能力
一般的质控软件在处理含有接头序列的 reads 时，通常采用 "在允许错配的情况下，如果分析的 read 匹配一定数量的接头序列即去除这条 read 或从匹配开始的位置截断 read，仅保留匹配位置之前的部分序列" 的方式。

如果采取 "去除含有接头序列的 reads" 的方式，会造成测序数据的浪费 (如果片段选择没有控制好，整个 lane 会有很大一部分数据含有接头序列，怎么办？);

如果采取 "从匹配开始的位置截断 read，仅保留匹配位置之前的部分序列" 的方式，对于只含有少数几个碱基的 reads，普通的质控软件是处理不了的（又该怎么办？）。

But，Trimmomatic 有两种模式：Single End Mode 和 Paired End Mode，对于单端测序数据，它和其它软件相比没有明显的优势；但如果是双端测序的数据，Trimmomatic 采用两种去接头方式，更强大，更彻底！


https://www.plob.org/article/12130.html









========================================
|-- FASTXToolkit(http://hannonlab.cshl.edu/fastx_toolkit/)
----------------------------------------
https://github.com/DawnEve/NGS_training/blob/master/day3.markdown
说明书： http://hannonlab.cshl.edu/fastx_toolkit/commandline.html


下载和安装(http://hannonlab.cshl.edu/fastx_toolkit/install_centos.txt) 
http://hannonlab.cshl.edu/fastx_toolkit/download.html
$ axel -n 20 http://hannonlab.cshl.edu/fastx_toolkit/fastx_toolkit_0.0.13_binaries_Linux_2.6_amd64.tar.bz2
$ tar -xjvf fastx_toolkit_0.0.13_binaries_Linux_2.6_amd64.tar.bz2
添加路径：将fastx_toolkit的路径加入.bashrc最后一行。并执行source ~/.bashrc 使配置立即生效
export PATH=/home/wangjl/software/fastx_toolkit/bin/:$PATH



#数据过滤
$ fastq_quality_filter -q 30 -p 100 -i test_1.fq -o test_1_filter.fq -Q 33
$ fastq_quality_filter -q 30 -p 100 -i test_2.fq -o test_2_filter.fq -Q 33
-q 30 最低质量分数是30才保留该碱基
-p 80 是最低合格的百分比，Minimum percent of bases that must have [-q] quality.这个地方有问题//todo

-Q 33防止报错。因为默认使用的phred64，而很多使用的是phred33。

不能输入.gz格式的文件！！变通方式：
$ zcat /home/wangjl/data/scFQ/c12_A1.fa.gz | fastq_quality_filter -q 25 -p 60 -z -o /home/wangjl/data/scFQ_filtered/c12_A1_filtered.fq.gz -Q 33

再次fastqc，
$ fastqc -o ./ -t 15 c12_A1_filtered.fq.gz
发现超过100区域碱基不平衡。
问题：Total Sequences（也就是reads数）从3132226减少到1591872。

怎么去除polyA尾巴？





========================================
|-- cutadapt: 一个python包
----------------------------------------
https://cutadapt.readthedocs.io/en/stable/

安装python3: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh

(1)使用pip安装：
$ pip install --user --upgrade cutadapt
检查版本号：
$ cutadapt --version
1.14



$ cutadapt --help
cutadapt version 2.3 #2019.6.15
#示例
$ cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq
-j CORES, --cores CORES   指定cpu数量
--trim-n              Trim N's on ends of reads. 感觉应该先去两端的N

-a ADAPTER, --adapter ADAPTER
	Sequence of an adapter ligated to the 3' end (paired data: of the first read). 
	The adapter and subsequent bases are trimmed. 
	If a '$' character is appended ('anchoring'), the adapter is only found if it is a suffix of the read.
	去除3'端接头及之后的序列。如果加上$后缀，则只在接头是后缀时去除。
-g ADAPTER, --front ADAPTER
	Sequence of an adapter ligated to the 5' end (paired data: of the first read). 
	The adapter and any preceding bases are trimmed. Partial matches at the 5' end are allowed. 
	If a '^' character is prepended ('anchoring'), the adapter is only found if it is a prefix of the read.
	去除5'端接头及之前的序列。允许5'端部分匹配。如果有^前缀，则只在接头是前缀时去除。
-b ADAPTER, --anywhere ADAPTER
	如果第一个碱基和接头匹配，则和-g参数类似，否则和-a参数类似。常用语挽救失败的文库，如果你知道接头位置，不要使用该参数！

-e RATE, --error-rate RATE
	Maximum allowed error rate as value between 0 and 1	(no. of errors divided by length of matching region).Default: 0.1 (=10%)
	最大允许错误，0到1之间，默认0.1
-n COUNT, --times COUNT
    Remove up to COUNT adapters from each read. Default: 1
	每个序列最多去除几个接头？默认是1

-q [5'CUTOFF,]3'CUTOFF, --quality-cutoff [5'CUTOFF,]3'CUTOFF
	Trim low-quality bases from 5' and/or 3' ends of each read before adapter removal. Applied to both reads if data is paired. 
	If one value is given, only the 3' end is trimmed. 
	If two comma-separated cutoffs are given, the 5' end is trimmed with the first cutoff, the 3' end with the second.
	去除质量分数低的序列。如果只给一个值，则只去除3'端。如果逗号隔开的2个值，则第一个过滤5'端，第二个过滤3'端。

-m LEN[:LEN2], --minimum-length LEN[:LEN2]
    Discard reads shorter than LEN. Default: 0
	丢弃长度太短的序列，默认0不丢弃



(2)按照碱基质量剪切：
cutadapt要求输入文件结尾必须是fq，fastq或者fq.gz，fastq.gz，不能是fa.gz，否则报错。
http://cutadapt.readthedocs.io/en/stable/guide.html
$ cutadapt -q 10 -o output.fastq input.fastq

$ cutadapt -q 25 -m 90 -o /home/wangjl/data/scFQ_filtered/c12_A1_filtered.fastq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
#3'端去掉质量分数小于25的碱基，舍弃小于90bp的序列。


(3)测试 cutadapt 中去除polyA尾巴的参数
去除polyA尾巴 For poly-A trimming, for example, you would write:
$ cutadapt -a "A{20}" -o output.fastq input.fastq

$ cutadapt -a "A{10}" -q 25 -m 30 -o filteredA10_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{20}" -q 25 -m 30 -o filteredA20_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{30}" -q 25 -m 30 -o filteredA30_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{100}" -q 25 -m 30 -o filteredA100_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz

NofA	reads
10	2930148
20	2942041
30	2943094
100	2985093

这个N越小保留下的reads越少。



(4) 去除5'端接头
$ cutadapt -g AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT -q 25 -m 20 -o dAdapter1_CutA_c16_ROW17.fq ../polyA_fq/CutA_c16_ROW17.fq >c16_ROW17.log
如果是另一端的PCR引物测穿了，需要使用其反向互补序列。







========================================
|-- 建库接头序列 contaminant_list.txt
----------------------------------------

This file contains a list of potential contaminants which are frequently found in high throughput sequencing reactions.  
These are mostly sequences of adapters / primers used in the various sequencing chemistries.
该文件包含高通量测序常见的污染源，通常是各种测序试剂中的adapters 和 primers。

Please DO NOT rely on these sequences to design your own oligos, some of them are truncated at ambiguous positions, and none of them are definitive sequences from the manufacturers so don't blame us if you try to use them and they don't work.
不要依靠这些序列设计自己的oligo，有些在不明确的位置被删减了，这些都不是厂家给定义的序列，所以如果做不出来不要责怪我们。

You can add more sequences to the file by putting one line per entry and specifying a name[tab]sequence.  
If the contaminant you add is likely to be of use to others please consider sending it to the FastQ authors, either via a bug report at www.bioinformatics.bbsrc.ac.uk/bugzilla/ or by directly emailing simon.andrews@bbsrc.ac.uk so other users of the program can benefit.

Illumina Single End Adapter 1  ACACTCTTTCCCTACACGACGCTGTTCCATCT
Illumina Single End Adapter 2  CAAGCAGAAGACGGCATACGAGCTCTTCCGATCT
Illumina Single End PCR Primer 1  AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Single End PCR Primer 2  CAAGCAGAAGACGGCATACGAGCTCTTCCGATCT
Illumina Single End Sequencing Primer  ACACTCTTTCCCTACACGACGCTCTTCCGATCT
	
Illumina Paired End Adapter 1   ACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End Adapter 2   CTCGGCATTCCTGCTGAACCGCTCTTCCGATCT
Illumina Paried End PCR Primer 1   AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End PCR Primer 2   CAAGCAGAAGACGGCATACGAGATCGGTCTCGGCATTCCTGCTGAACCGCTCTTCCGATCT
Illumina Paried End Sequencing Primer 1		ACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End Sequencing Primer 2		CGGTCTCGGCATTCCTACTGAACCGCTCTTCCGATCT


more: https://github.com/csf-ngs/fastqc/blob/master/Contaminants/contaminant_list.txt





========================================
|-- 去除接头 trim_galore: 自动检测adapter的质控软件
----------------------------------------
1.简介

_Trim Galore_ is a wrapper around [Cutadapt](https://github.com/marcelm/cutadapt) and [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data.

Trim Galore是对FastQC和Cutadapt的包装。适用于所有高通量测序，包括RRBS(Reduced Representation Bisulfite-Seq ), Illumina、Nextera 和smallRNA测序平台的双端和单端数据。主要功能包括两步：
第一步首先去除低质量碱基，然后去除3' 末端的adapter, 如果没有指定具体的adapter，程序会自动检测前1million的序列，然后对比前12-13bp的序列是否符合以下类型的adapter:

Illumina:   AGATCGGAAGAGC
Small RNA:  TGGAATTCTCGG
Nextera:    CTGTCTCTTATA






2. 下载和安装
(1)#不推荐！会在路径提示符前面添加(base)，洁癖的人不要尝试: conda install -c bioconda trim-galore 


(2)[推荐] 下载安装包安装，解压，配置下环境变量就可以使用：https://github.com/FelixKrueger/TrimGalore
$ git clone https://github.com/FelixKrueger/TrimGalore.git
然后里面有二进制文件，就可以用了。
在~/bin下新建软链接：
$ ln -s /home/wangjl/software/TrimGalore/trim_galore
$ trim_galore -v
version 0.6.2


官方安装步骤:
```bash
# Check that cutadapt is installed
cutadapt --version
# Check that FastQC is installed
fastqc -v
# Install Trim Galore
curl -fsSL https://github.com/FelixKrueger/TrimGalore/archive/0.6.0.tar.gz -o trim_galore.tar.gz
tar xvzf trim_galore.tar.gz
# Run Trim Galore
~/TrimGalore-0.6.0/trim_galore











3.使用

(1)示例
# 处理双端测序结果
echo " trim_galore cut adapters started at $(date)"
trim_galore -q 20 --phred33 --stringency 3 --length 20 -e 0.1 \
            --paired $dir/cmp/01raw_data/$fq1 $dir/cmp/01raw_data/$fq2  \
            --gzip -o $input_data
echo "trim_galore cut adapters finished at $(date)"


# 单端测序结果
$ trim_galore --quality 25 --phred33 --stringency 3 --length 30 --gzip  --fastqc_args "-t 15" --output_dir trim_galore xx.fastq

参数解释(部分参数解释见(2))
$ trim_galore --help
--adapter：输入adapter序列。也可以不输入，Trim Galore!会自动寻找可能性最高的平台对应的adapter。自动搜选的平台三个，也直接显式输入这三种平台，即--illumina、--nextera和--small_rna。

--paired：对于双端测序结果，一对reads中，如果有一个被剔除，那么另一个会被同样抛弃，而不管是否达到标准。
--retain_unpaired：对于双端测序结果，一对reads中，如果一个read达到标准，但是对应的另一个要被抛弃，达到标准的read会被单独保存为一个文件。
--gzip和--dont_gzip：清洗后的数据zip打包或者不打包。

-- trim-n : 移除read一端的reads
-e：允许的错误率




(2) 批量模式
echo "Trim galore reads"
# ls *.gz | while read id; do echo "Trim ${id}"; trim_galore --quality 25 --phred33 --stringency 3 --length 30 --gzip  --fastqc_args "-t 15" --output_dir trim_galore $id; done
参数解释
	--quality 25 #设定Phred quality score阈值，默认为20。
	--phred33  #选择-phred33或者-phred64，表示测序平台使用的Phred quality score。
	--stringency 3 #设定可以忍受的前后adapter重叠的碱基数，默认为1（非常苛刻）。可以适度放宽，因为后一个adapter几乎不可能被测序仪读到。
	--length 30 #设定输出reads长度阈值，小于设定值会被抛弃。
	--gzip  #--gzip和--dont_gzip：清洗后的数据zip打包或者不打包。
	--fastqc_args "-t 15" # 线程数量
	--output_dir #输出目录。需要提前建立目录，否则运行会报错。
		-o：输出文件路径
#




4.质控
$ cd /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/trim/galore
$ multiqc *fastqc.zip -o /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/



refer:
https://www.jianshu.com/p/7a3de6b8e503












========================================
比对工具: RNA Mapper
----------------------------------------

Popular short read aligners
http://homer.ucsd.edu/homer/basicTutorial/mapping.html
more: https://en.wikipedia.org/wiki/List_of_sequence_alignment_software#Short-Read_Sequence_Alignment

Most Popular:
bowtie : fast, works well
bowtie2 : fast, can perform local alignments too

Subread - Very fast, (also does splice alignment)
STAR - Extremely fast (also does splice alignment, requires at least 30 Gb memory)
To be honest, I would probably recommend STAR for almost any application at this point if you have the memory (see below)

BWA - Fast, allows indels, commonly used for genome/exome resequencing




========================================
|-- Bowtie2: A fast and sensitive gapped read aligner
----------------------------------------
What is Bowtie?
http://www.cnblogs.com/emanlee/archive/2011/11/12/2246358.html

Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index (based on the Burrows-Wheeler Transform or BWT) to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. Multiple processors can be used simultaneously to achieve greater alignment speed. Bowtie 2 outputs alignments in SAM format, enabling interoperation with a large number of other tools (e.g. SAMtools, GATK) that use SAM. Bowtie 2 is distributed under the GPLv3 license, and it runs on the command line under Windows, Mac OS X and Linux.

Bowtie 2 is often the first step in pipelines for comparative genomics, including for variation calling, ChIP-seq, RNA-seq, BS-seq. Bowtie 2 and Bowtie (also called "Bowtie 1" here) are also tightly integrated into some tools, including TopHat: a fast splice junction mapper for RNA-seq reads, Cufflinks: a tool for transcriptome assembly and isoform quantitiation from RNA-seq reads, Crossbow: a cloud-enabled software tool for analyzing reseuqncing data, and Myrna: a cloud-enabled software tool for aligning RNA-seq reads and measuring differential gene expression.


http://genomebiology.com/2009/10/3/R25  (paper)
http://bowtie-bio.sourceforge.net/index.shtml (source, bin)
http://www.genome.iastate.edu/bioinfo/resources/manuals/rna_bowtie.txt
http://blog.csdn.net/cherylnatsu/article/details/6801997
http://jiangjinhua.zju.blog.163.com/blog/static/600320061174111903/

官网 http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml


1.安装
https://github.com/BenLangmead/bowtie2


(1) conda安装（不推荐，会有不可测的副作用）
https://anaconda.org/bioconda/bowtie2

$ conda install -c bioconda bowtie2 #需要安装和更新很多包。
# 安装了perl，会导致冲突，又删除了
$ conda remove bowtie2 #卸载失败
$ conda uninstall -c bioconda bowtie2
## 必须同意更新好几个包，才能删除
$ which bowtie2
/usr/bin/which: no bowtie2 in (...



(2) binaries安装（推荐）
http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#building-from-source

1) 安装依赖包
Operating System	/Sync Package List	/Search	/Install
Ubuntu, Mint, Debian	/apt-get update	/apt-cache search tbb	/apt-get install libtbb-dev
Fedora, CentOS	/yum check-update	/yum search tbb	/yum install tbb-devel.x86_64

## yum install tbb-devel.x86_64

$ yum list installed | grep tbb
tbb.x86_64                             4.1-9.20130314.el7              @anaconda
tbb-devel.x86_64                       4.1-9.20130314.el7              @anaconda

2) 下载 binaries package
$ wget https://github.com/BenLangmead/bowtie2/releases/download/v2.3.5.1/bowtie2-2.3.5.1-linux-x86_64.zip
$ unzip bowtie2-2.3.5.1-linux-x86_64.zip 

3) 添加到路径[推荐]
$ vim ~/.bashrc #末尾添加一行
export PATH=/home/wangjl/software/bowtie2-2.3.5.1-linux-x86_64:$PATH
$ source ~/.bashrc

（或者把可执行文件复制到路径中的目录中： bowtie2, bowtie2-align-s, bowtie2-align-l, bowtie2-build, bowtie2-build-s, bowtie2-build-l, bowtie2-inspect, bowtie2-inspect-s and bowtie2-inspect-l.）


4) 检查路径和版本号
$ which bowtie2
~/software/bowtie2-2.3.5.1-linux-x86_64/bowtie2

$ bowtie2 --version
/home/wangjl/software/bowtie2-2.3.5.1-linux-x86_64/bowtie2-align-s version 2.3.5.1
64-bit
Built on 
Wed Apr 17 02:50:12 UTC 2019
Compiler: gcc version 7.3.1 20180303 (Red Hat 7.3.1-5) (GCC) 
Options: -O3 -m64 -msse2 -funroll-loops -g3 -g -O2 -fvisibility=hidden -I/hbb_exe_gc_hardened/include -ffunction-sections -fdata-sections -fstack-protector -D_FORTIFY_SOURCE=2 -fPIE -std=c++98 -DPOPCNT_CAPABILITY -DWITH_TBB -DNO_SPINLOCK -DWITH_QUEUELOCK=1
Sizeof {int, long, long long, void*, size_t, off_t}: {4, 8, 8, 8, 8, 8}






2. 文档 https://github.com/BenLangmead/bowtie2
https://blog.csdn.net/u011262253/article/details/79833969

(1)建立索引
# Building a small index
bowtie2-build example/reference/lambda_virus.fa example/index/lambda_virus
# Building a large index
bowtie2-build --large-index example/reference/lambda_virus.fa example/index/lambda_virus

建立6文件索引：
$ bowtie2-build mm9.fa bowtie2/mm9 --threads 50 #bowtie2/mm9 中mm9是前缀，留空不好查看文件

/home/wangjl/data/ref/human
$ bowtie2-build hg19.fa bowtie2/hg19 --threads 90 #使用90个线程



(2)开始mapping
# Aligning unpaired reads
bowtie2 -x example/index/lambda_virus -U example/reads/longreads.fq
# Aligning paired reads
bowtie2 -x example/index/lambda_virus -1 example/reads/reads_1.fq -2 example/reads/reads_2.fq


我的是单端测序：
$ bowtie2 -p 6 --local -x /home/wangjl/data/ref/mouse/bowtie2/mm9 -U /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/R2_left/c2_ROW01_R2.fastq -S /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bowtie2_mouse/c2_ROW01_R2.sam
 -p 6 使用6个线程
 -S 指定输出文件
 --local	在这种模式下，Bowtie 2不要求整个读取从一端到另一端对齐。相反，为了达到最大可能的对齐分数，可以从末端省略一些字符（“软裁剪”）

$ bowtie2 -p 60 --local -x /home/wangjl/data/ref/human/bowtie2/hg19 -U /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/R2_left/c2_ROW01_R2.fastq -S /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bowtie2_human/c2_ROW01_R2.sam


结论：bowtie2的--local选项，有助于提高比对比率。可能是能去除两端残留的建库index。








========================================
|-- STAR: ultrafast universal RNA-seq aligner 
----------------------------------------

STAR is recommended by the ENCODE project, definitely should be your best option(among Bowtie2,).


RNA-seq比对算法开发了STAR（Spliced Transcripts Alignments to a Reference，STAR）.
该算法使用了未压缩后缀阵列中的连续最大可比对种子搜索，接着种子聚类和缝合过程。

1.paper:https://academic.oup.com/bioinformatics/article/29/1/15/272537
Published: 25 October 2012
STAR被实现为一个单机C++代码。

rna call varients时gatk推荐工具，broad institute都推荐了，还是encode计划时冷泉港内部开发的，特点：超级快速（8min map完6gb的reads）、as支持性好、支持长reads、全转录本、发现嵌合转录本等，有理由看一下。

STAR, 很犀利, ENCODE专属RNA-seq工具. 在准度和时间消耗上, 效果拔群. 因为STAR, 了解一下啥是suffix array. 原来做mapping的, 真的就是ctrl+F的工作... 只是, 这个字符串寻找比较麻烦, 既要容错(insertion/deletion/mismatch), 又要考虑到RNA splicing而导致的genomic gap. 

容错的这个还好, 因为DNA-seq已经打下了夯实基础; 反倒是splicng带来的splicing junction detection问题, 是RNA-seq里专属. 所以在处理DNA-seq和RNA-seq数据做mapping时, 真的不一样. 比如bowtie是unspliced mapper, tophat是spliced mapper, 也难怪bowtie多用于DNA-seq而tophat多用于RNA-seq的mapping步骤. 

STAR 吐槽现有的RNA-seq工具都是DNA-seq工具的延伸, 并非量身打造. 从其算法里的mapping一步来看, 应该属于spliced mapper, 但又不同于把reads打断成k-mer形式. 








2. download and install 安装比对软件 STAR

(1)安装star 2.7：https://github.com/alexdobin/STAR
# Get latest STAR source from releases
wget https://github.com/alexdobin/STAR/archive/2.7.0f.tar.gz
tar -xzf 2.7.0f.tar.gz
cd STAR-2.7.0f

#编译 Compile under Linux
cd STAR/source
make STAR

#然后在~/bin下添加快捷方式
$ ln -s /home/wangjl/data/software/STAR-2.7.0f/source/STAR

#查看版本号
$ STAR --version
2.7.0f


(2)最后使用的是(lab上一致的)老版本2.5.2：
https://github.com/alexdobin/STAR/archive/2.5.2b.tar.gz
https://github.com/alexdobin/STAR

find one on the server:
$ find / -name '*STAR*'
/share/apps/genomics/STAR-2.5.2b

use it directly:
$ cd /home/wangjl/bin/
$ ln -s /share/apps/genomics/STAR-2.5.2b/bin/Linux_x86_64/STAR
$ STAR --version
STAR_2.5.2b


(3)下载参考基因组
1).资源:下载hg19基因组的fasta文件和gtf注释文件
参考基因组 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/
jinlab: $ ls /data1/hou/RNA/refs/hg19


a1) 下载方式1
axel -n 30 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz

a2) 下载方法2
axel -n 40 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz
tar -zxvf chromFa.tar.gz
cd chroms
cat *.fa >hg19.fa



2)下载老鼠mm9基因组
axel -n 20 http://hgdownload.cse.ucsc.edu/goldenPath/mm9/bigZips/mm9.2bit

3)怎么把2bit转变成fasta？使用twoBitToFa工具
https://blog.csdn.net/weixin_40099163/article/details/86151988

mm9.2bit - contains the complete mm9 Mouse Genome
    in the 2bit format.  A utility program, twoBitToFa (available from our src tree), can be used to extract .fa file(s) from this file.  See also:
        http://genome.ucsc.edu/admin/cvs.html - CVS access to the source tree
        http://genome.ucsc.edu/admin/jk-install.html - building the utilities
A pre-compiled version of the command line tool can be found at: http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/

下载
$ axel -n 30 http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa
$ chmod +x twoBitToFa
添加到~/bin下。

运行该软件：
$ twoBitToFa mm9.2bit mm9.fa


4)下载mm10  http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/
cd /home/wangjl/data/ref/mm10
$ axel -n 80 http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.2bit
#下载mm10 gtf和bed文件



(4)从UCSC下载gtf注释文件

选择最新的gtf注释文件，人类和小鼠常在http://www.gencodegenes.org下载，植物的可信基因组见http://plants.ensembl.org









3.generating genome index 生成索引(很耗时)
STAR command line format: STAR --option1-name option1-value(s)--option2-name option2-value(s)

(1)生成hg19的STAR index（放在 nohup CMD & 中运行的）
STAR --runMode genomeGenerate  \
	--runThreadN 20  \
	--genomeDir /home/wangjl/index/STAR/  \
	--genomeFastaFiles /share/reference/genome/hg19/hg19.fa  \
	--sjdbGTFfile /share/reference/genome/hg19/hg19_ucsc_genes.gtf  \
	--sjdbOverhang 100
# 大服务器上:
STAR --runMode genomeGenerate \
	--runThreadN 50 \
	--genomeDir /home/wangjl/data/ref/human/STAR/  \
	--genomeFastaFiles /home/wangjl/data/ref/human/hg19.fa \
	--sjdbGTFfile /home/wangjl/data/ref/human/hg19_ucsc_genes-20190506.gtf \
	--sjdbOverhang 100
参数解释：
	runThreadN 线程数
	runMode 运行模式，genomeGenerate 选项用来产生index;
	genomeDir 指定生成的index保存的位置
	genomeFastaFiles 输入的参考基因组文件fasta。
	sjdbGTFfile 注释gtf文件
	sjdbOverhang : specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. max(ReadLength)-1 或者默认100.

很费时，占用CPU很严重，建议晚上进行。小心被骂。
Jan 17 21:18:32 ..... started STAR run
Jan 17 21:18:32 ... starting to generate Genome files
Jan 17 21:19:47 ... starting to sort Suffix Array. This may take a long time...
Jan 17 21:20:03 ... sorting Suffix Array chunks and saving them to disk...
Jan 17 21:32:57 ... loading chunks from disk, packing SA...
Jan 17 21:34:13 ... finished generating suffix array
Jan 17 21:34:13 ... generating Suffix Array index
Jan 17 21:37:29 ... completed Suffix Array index
Jan 17 21:37:29 ..... processing annotations GTF
Jan 17 21:37:35 ..... inserting junctions into the genome indices
Jan 17 21:39:49 ... writing Genome to disk ...
Jan 17 21:39:50 ... writing Suffix Array to disk ...
Jan 17 21:40:02 ... writing SAindex to disk
Jan 17 21:40:04 ..... finished successfully

推荐使用好理解的路径名，比如 /home/wangjl/data/ref/hg19/index/star

lab server上本来就有该STAR可用的hg19索引:
/data1/hou/RNA/refs/hg19_ERCC92
/data1/hou/RNA/refs/hg19_ERCC92/index/star


(2)生成小鼠的基因组STAR index（放在 nohup CMD & 中运行的）
STAR --runThreadN 50 --runMode genomeGenerate --genomeDir /home/wangjl/data/ref/mouse/STAR/  --genomeFastaFiles /home/wangjl/data/ref/mouse/mm9.fa --sjdbGTFfile /home/wangjl/data/ref/mouse/mm9_ucsc_genes-20190506.gtf --sjdbOverhang 100
[11:05 - 11:21]









4.Run mapping jobs. 推荐直接用(3)
51页手册STARmanual.pdf： https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf

(1) 基本比对，后面还有2个改进版。
test data
[wangjl@nih_jin test3]$ ls -lth /home/wangjl/data/test
total 2.5G
-rwxr-xr-x. 1 wangjl user 287M Jan 17 21:59 c16_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 1.5G Jan 17 21:59 c15_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 100M Jan 17 21:59 c14_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 365M Jan 17 21:59 c13_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 261M Jan 17 21:59 c12_A1.fa.gz

###基本语句to hg19
$ STAR --runThreadN 5  \
	--genomeDir /data1/hou/RNA/refs/hg19_ERCC92/index/star  \
	--readFilesIn /home/wangjl/data/test/c13_A1.fa.gz  \
	--readFilesCommand gunzip -c  \
	--outFileNamePrefix /home/wangjl/data/afterMapping/test3/c14_A1_

Jan 17 22:14:40 ..... started STAR run
Jan 17 22:14:40 ..... loading genome
Jan 17 22:15:10 ..... started mapping
Jan 17 22:17:03 ..... finished successfully
参数解释：
	--runThreadN 线程数
	--genomeDir 基因组index路径
	--readFilesIn 带路径名的fq测序文件
	--readFilesCommand gunzip -c  读取文件的解压命令和参数。也可以使用 --readFilesCommand zcat \
	--outFileNamePrefix 输出文件的前缀，默认是./
	
获得文件: c14_A1_Aligned.out.sam

(2)改进1： 建议添加保留基因组选项（--genomeLoad LoadAndKeep），对于共用一套index的多个比对，能节省很多内存，提高比对的并发数量。
该测序read最长150bp。
$ STAR --runThreadN 10 \
	--genomeDir /data1/hou/RNA/refs/hg19_ERCC92/index/star \
	--readFilesIn /home/wangjl/data/test/c13_A1.fa.gz \
	--readFilesCommand gunzip -c \
	--outFileNamePrefix /home/wangjl/data/afterMapping/test3/c14_A1_ \
	--genomeLoad LoadAndKeep \
	--outSAMtype BAM SortedByCoordinate \
	--sjdbOverhang 149

##
--readFilesIn sample_r1.fq.gz sample_r2.fq.gz \#对于PE序列
--readFilesCommand zcat \#如果是gz文件，也可以这样解压缩
--outBAMsortingThreadN 10 \#输出bam的线程数


Note, if the spike-ins are used, the reference sequence should be augmented with the DNA sequence of the spike-in molecules prior to mapping.
注意：如果有spike-ins，参考序列也应该用spike-in分子DNA序列先扩容。

Note, when UMIs are used, their barcodes should be removed from the read sequence. A common practice is to add the barcode to the read name.
注意：使用UMI时，需要去除barcode。通常做法是把barcode加到read的名字上。




(3)改进2：推荐直接输出bam文件，适合py并发使用。
--outSAMtype BAM SortedByCoordinate 输出格式为BAM并排序过。

$ STAR --runThreadN 30  \
	--outSAMtype BAM SortedByCoordinate  \
	--genomeDir /home/wangjl/data/ref/hg19_mm10_transgenes/starIndex  \
	--readFilesIn /home/wangjl/data/apa/190515/trim_galore/c2_ROW01_R2_trimmed.fq.gz  \
	--readFilesCommand zcat  \
	--genomeLoad LoadAndKeep \  #单一会报错
	--limitBAMsortRAM 20000000000 \ #加入这一行就不报错了
	--outFileNamePrefix  /home/wangjl/data/apa/190515/hg19_mm10/c2_ROW01_


(4)更多参数
除了上面常用的一些参数外，STAR的可选参数其实非常多.

输出BAM文件时，STAR还可以对BAM进行一些预处理，"--bamRemoveDuplicatesType"用于去重("UniqueIdentical","UniqueIdenticalNotMulti")

如果你希望输出信号文件(Wig格式),那么需要额外增加 --outWigType参数，如 --outWigType wiggle read2, 还可以用 --outWigStrand指定是否将两条链合并(Stranded, Unstranded), 默认 --outWigNorm RPM，也就是用RPM进行标准化，可以选择None.

如果你在建立索引或者比对的时候增加了注释信息，那么STAR还能帮你进行基因计数。参数为 --quantMode, 分为转录本水平(TranscriptomeSAM)和基因水平(GeneCounts)，在计数的时候还允许指定哪些哪些read不参与计数，"IndelSoftclipSingleend"和"Singleend"

对于非链特异性RNA-seq，同时为了保证能和Cufflinks兼容，需要添加 --outSAMstrandField intronMotif在SAM中增加XS属性，并且建议加上 --outFilterIntronMotifsRemoveNoncanonical。如果是链特异性数据，那么就不需要特别的参数，Cufflinks用 --library-type声明类型即可=





5. 查看输出文件
log、sam、剪切点注释 三类文件，需要注意的是，sam里第五列 uniquely mapping reads的map质量值是255。


$ ls -lth
total 1.2G
-rw-r--r--. 1 wangjl user 1.9K Jan 17 22:17 c14_A1_Log.final.out
-rw-r--r--. 1 wangjl user  22K Jan 17 22:17 c14_A1_Log.out
-rw-r--r--. 1 wangjl user  364 Jan 17 22:17 c14_A1_Log.progress.out
-rw-r--r--. 1 wangjl user 351K Jan 17 22:17 c14_A1_SJ.out.tab
-rw-r--r--. 1 wangjl user 1.2G Jan 17 22:17 c14_A1_Aligned.out.sam

(1)3个log文件
1)
Log.out: 主要的log文件，对排错和debug很重要。
Log.progress.out: 报告该运行的统计结果，比如处理了多少reads，map上的占百分比。改文件每1min更新一次。
Log.final.out: mapping结束后的map统计结果，对质控很重要。对每个read（单个或双端）分别做统计，然后对全部reads汇总、求平均。
注意：STAR把一个paired-end read计为一个read，不像samtools agstat/idxstats是对每个mate分别计数。
大多信息是关于UNIQUE mappers的，不像samtools agstat/idxstats不区分unique or multi-mappers。
每个splicing都在splices数中计数，这和SJ.out.tab中的汇总一致。

mismatch/indel error rates是按照每个碱基统计的，比如 
total number of mismatches/indels in all unique mappers 除以total number of mapped bases.


2)
#目的：(python on Linux)从star结果文件获取Uniq比对reads数和百分比
import subprocess

#要点： 使用id拼接linux命令。建议都用绝对路径。
def doLinuxCMD(id):
    #cmd1
    cmd="grep 'Uniquely mapped reads number' /home/wangjl/data/apa/190517R/hg19/"+id+"_Log.final.out|cut -d '\t' -f2"
    (status, output)=subprocess.getstatusoutput(cmd)
    rs=str(status)+" "+output;
    
    #cmd2
    cmd2="grep 'Uniquely mapped reads %' /home/wangjl/data/apa/190517R/hg19/"+id+"_Log.final.out|cut -d '\t' -f2"
    (status2, output2)=subprocess.getstatusoutput(cmd2)
    rs2=str(status2)+" "+output2;

    return id+" "+rs+" "+rs2 #返回状态码status2=0表示命令正常执行，其他表示异常，需要查看output推测具体原因

#test 测试
doLinuxCMD('c12_ROW02')  #只需要传入细胞名字即可
##'c12_ROW02 0 2187377 0 72.56%'





(2)1个sam文件
Aligned.out.sam - alignments in standard SAM format.

 - 为了使sam结果和下游Cufflinks or StringTie兼容，要设置 --outSAMattrIHstart 0.(默认是1)
run Cufflinks with the library option --library-type options.

For example, 
$ cufflinks ... --library-type fr-firststrand 
should be used for the standard dUTP protocol, including
Illumina's stranded Tru-Seq. This option has to be used only for Cufflinks runs and not for STAR
runs.
In addition, it is recommended to remove the non-canonical junctions for Cufflinks runs using
 --outFilterIntronMotifs RemoveNoncanonical.

 
(3) "SJ.out.tab"存放的高可信的剪切位点，每一列的含义如下
第一列: 染色体
第二列: 内含子起始（以1为基）
第三列: 内含子结束（以1为基）
第四列：所在链，1(+)，2(-)
第五列: 内含子类型，0表示不是下面的任何一种功能，1表示GT/AG, 2表示:GT/AC,3表示GC/AG,4表示GT/GC,5表示AT/GC,6表示GT/AT
第六列: 是否是已知的注释
第七列: 有多少唯一联配支持
第八列: 有多少多重联配支持
第九列: maximum spliced alignment overhang, 这个比较难以翻译，指的是当短读比对到剪切位点时，中间会被分开，另一边能和基因组匹配的数目，例如ACGTACGT----------ACGT，就是4或者8，取决于方向。

控制过滤的参数为 --outSJfilter*系列，其中 --outSJfilterCountUniqueMin3111表示4类内含子唯一匹配的read支持数至少为3,1,1,1, 而 --outSJfilterCountTotalMin3111则表示4类内含子唯一匹配和多重匹配read的支持数和，至少为3,1,1,1。如果你设置的 --outSJfilterReadsUnique，那么上面两者是等价的，当然默认情况下是 All










refer:
STAR:
https://www.cnblogs.com/Dicor/p/4004819.html
http://www.mamicode.com/info-detail-1163133.html
http://www.bio-info-trainee.com/727.html
d:/ STARmanual.pdf



========================================
|-- Mapping QC: 用RSeQC对比对后的转录组数据进行质控(An RNA-seq Quality Control Package)
----------------------------------------
RSeQC包是一个python软件


1.目的： 
There are many ways to measure the mapping quality, including: amount of reads mapping to rRNA/tRNAs, proportion of uniquely mapping reads, reads mapping across splice junctions, read depth along the transcripts. 
Reference: * RSeQC: quality control of RNA-seq experiments Bioinformatics (2012) 28 (16): 2184-2185. doi: 10.1093/bioinformatics/bts356




2.安装
$ wget -b https://sourceforge.net/projects/rseqc/files/RSeQC-2.6.4.tar.gz
安装报错，需要python2。算了，还是使用conda切换为python2.7环境，然后
$ pip install RSeQC






3.使用(输入排序后的bam格式文件，要先samtools index产生.bai文件)：http://dldcc-web.brc.bcm.edu/lilab/liguow/CGI/rseqc/_build/html/
中文教程： https://www.jianshu.com/p/f9da70fcaf8d
官方文档: http://rseqc.sourceforge.net/

python <RSeQCpath>/geneBody_coverage.py -i input.bam -r genome.bed -o output.txt
python <RSeQCpath>/bam_stat.py -i input.bam -r genome.bed -o output.txt
python <RSeQCpath>/split_bam.py -i input.bam -r rRNAmask.bed -o output.txt


(1)geneBody_coverage.py
参考基因组bed文件下载 https://www.jianshu.com/p/e0676631bd35

为了保证染色体名字一致(主要是chr1和1的区别)，要下载参考基因组fa来源一致的bed文件。
一共三家UCSC，NCBI，ensemble。
比如从UCSC下载hg19的bed文件：
https://genome.ucsc.edu/cgi-bin/hgTables


运行：
$ geneBody_coverage.py -r hg19.refseq.bed  -i Pairend_nonStrandSpecific_36mer_Human_hg19.bam -o output
	-r 参考基因组bed格式[required]
	-i Alignment file in BAM or SAM format
	-o Prefix of output files(s). [required] 必须要设置前缀
	
#查看帮助文档
$ geneBody_coverage.py --help
-i INPUT_FILES, --input=INPUT_FILES
	Input file(s) in BAM format. 
	"-i" takes these input:
	1) a single BAM file. 
	2) "," separated BAM files. 
	3) directory containing one or more bam files. 
	4) plain text file containing the path of one or more bam file (Each row is a BAM file path). 
	All BAM files should be sorted and indexed using samtools.
#

1)单个bam文件
$ geneBody_coverage.py -r /data/jinwf/wangjl/ref/human/hg19_ucsc_genes-20190509.bed  -i /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bamR2Lh/c2_ROW02.sorted.bam  -o /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/mappingQC_human/c2_ROW02

2)多个bam文件时，文件之间用逗号隔开:
$ cd /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bamR2Lh/
$ geneBody_coverage.py -r /data/jinwf/wangjl/ref/human/hg19_ucsc_genes-20190506.bed  -i c2_ROW01.sorted.bam,c2_ROW02.sorted.bam  -o ../mappingQC_human/c2_ROW0102

3)输入-i也可以提供包含bam文件的文件夹：
$ geneBody_coverage.py -r /home/wangjl/data/ref/hg19/hg19_ucsc_genes-20190509.bed -i ./ -o ./test_

4)-i 后跟文本文件，文本内容是bam的路径，一行一个bam的路径。


提示: 运行需要的时间很长，一般要过夜。为防止以外中断，请在tmux内运行。










========================================
|-- blast: 给定序列，从一个fasta库中查找最相近的序列
----------------------------------------
Wang Zhiwei: 芯片数据，如何根据序列注释出基因名字(gene symbols)？[2020.3.21]
比如: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL18109

hint: https://shengxin.ren/article/442

Me:

1. 找到文章的方法
https://gut.bmj.com/content/63/11/1700.long

Microarray processing and statistical analysis

LncRNA expression profiling was performed using the Agilent human lncRNA+mRNA array V.2.0 platform. After a filtering procedure, 8900 human lncRNAs (annotated by GENCODE(V13) database, lincRNAs from Cabili et al,21 and the University of California Santa Cruz database) were selected for the following analysis (see online supplementary methods). First, quantile normalisation of the microarray data (containing the 8900 lncRNAs and all mRNAs in the microarray) of all 119 paired tumour–normal samples was carried out. Then, the data was log 2-scale transformed. Missing values were imputed using the random Forest unsupervised classification algorithm (see online supplementary methods). The data of the 60 sample pairs in the independent cohort were processed independently in the same way.

(1)只有第2句是如何注释的(GENCODE, Cabili et al, 和UCSC)。注释后过滤，得到8900个lncRNA供后续分析。
After a filtering procedure, 
8900 human lncRNAs 
(annotated by GENCODE(V13) database, lincRNAs from Cabili et al,21 and the University of California Santa Cruz database) 
were selected for the following analysis (see online supplementary methods).



(2)文献21是
Cabili MN, Trapnell C, Goff L, et al. Integrative annotation of human large intergenic noncoding RNAs reveals global properties and specific subclasses. Genes Dev 2011;25:1915–27.


(3)补充材料1 
https://gut.bmj.com/content/gutjnl/suppl/2014/02/12/gutjnl-2013-305806.DC1/gutjnl-2013-305806supp1.pdf

1) Microarray fabrication
30 The Agilent human lncRNA+mRNA Array v2·0 was designed with four identical arrays per slide (4 x
31 180K format). Each array contained probes interrogating about 39,000 human lncRNAs and about
32 32,000 human mRNAs. Each RNA was detected by two probe repeats. The array also contained 4974
33 Agilent control probes


Then, we employed the blast program to map the
40 probes uniquely to the annotated lncRNA sequences, and 8900 lncRNAs with at least one unique 
1 probe were retrieved. For each of the 8900 lncRNAs, t

2)使用pubmed搜索 blast，第一个是网页版
https://blast.ncbi.nlm.nih.gov/Blast.cgi
其中有:
Standalone and API BLAST
Download BLAST
Get BLAST databases and executables

点击进入下载页面
https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=Download
看到标题: Download BLAST Software and Databases


3)软件下载: ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/
BLAST+ user manual, https://www.ncbi.nlm.nih.gov/books/NBK279690/
the BLAST Help manual, https://www.ncbi.nlm.nih.gov/books/NBK1762/

$ cd /home/wangjl/data/soft
## wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.10.0+-x64-linux.tar.gz
$ axel -n 30 ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.10.0+-x64-linux.tar.gz
## 并行下载更快


## 下载很慢，但是我发现我已经安装过
	$ blastn -version
	blastn: 2.6.0+
	Package: blast 2.6.0, build Jan 15 2017 17:12:27
	安装位置:
	$ whereis blastn
	blastn: /usr/bin/blastn
## 版本太老，好像不能下载官方库了。决定还是重新安装吧


解压后，进入bin目录: /data/wangjl/soft/ncbi-blast-2.10.0+/bin

加入到path中；
$ vim ~/.bashrc
末尾加一行并保存:
export PATH="/data/wangjl/soft/ncbi-blast-2.10.0+/bin":$PATH

更新path
$ source ~/.bashrc

查看版本号，已经是最新版了
$ blastp -version
blastp: 2.10.0+
 Package: blast 2.10.0, build Dec  3 2019 18:03:18



4)库
Download the databases you need,(see database section below), or create your own. Start searching.

ftp://ftp.ncbi.nlm.nih.gov/blast/
ftp://ftp.ncbi.nlm.nih.gov/blast/db/


查看有哪些库
$ perl update_blastdb.pl --showall
	Connected to NCBI
	16S_ribosomal_RNA
	18S_fungal_sequences
	28S_fungal_sequences
	Betacoronavirus
	ITS_RefSeq_Fungi
	ITS_eukaryote_sequences
	LSU_eukaryote_rRNA
	LSU_prokaryote_rRNA
	SSU_eukaryote_rRNA
	landmark
	nr 蛋白库
	nt 核酸库
	patnt
	pdbaa
	pdbnt
	ref_euk_rep_genomes
	ref_prok_rep_genomes
	ref_viroids_rep_genomes
	ref_viruses_rep_genomes
	refseq_protein
	refseq_rna
	swissprot
	taxdb
#
Contents of the /blast/db/ directory
The pre-formatted BLAST databases are archived in this directory. The names of these databases and their contents are listed below.
########################
File Name        #  Content Description   
16SMicrobial.tar.gz          #  Bacterial and Archaeal 16S rRNA sequences from BioProjects 33175 and 33117
FASTA/        #  Subdirectory for FASTA formatted sequences
README        #  README for this subdirectory (this file)
Representative_Genomes.*tar.gz        #  Representative bacterial/archaeal genomes database
cdd_delta.tar.gz          #  Conserved Domain Database sequences for use with stand alone deltablast
cloud/          #  Subdirectory of databases for BLAST AMI; see http://1.usa.gov/TJAnEt
env_nr.*tar.gz        #  Protein sequences for metagenomes
env_nt.*tar.gz        #  Nucleotide sequences for metagenomes
est.tar.gz        #  This file requires est_human.*.tar.gz, est_mouse.*.tar.gz, and est_others.*.tar.gz files to function. It contains the est.nal alias so that searches against est (-db est) will include est_human, est_mouse and est_others. 
est_human.*.tar.gz        #  Human subset of the est database from the est division of GenBank, EMBL and DDBJ.
est_mouse.*.tar.gz        #  Mouse subset of the est databasae
est_others.*.tar.gz           #  Non-human and non-mouse subset of the est database
gss.*tar.gz           #  Sequences from the GSS division of GenBank, EMBL, and DDBJ
htgs.*tar.gz          #  Sequences from the HTG division of GenBank, EMBL,and DDBJ
human_genomic.*tar.gz         #  Human RefSeq (NC_) chromosome records with gap adjusted concatenated NT_ contigs
nr.*tar.gz        #  Non-redundant protein sequences from GenPept, Swissprot, PIR, PDF, PDB, and NCBI RefSeq
nt.*tar.gz        #  Partially non-redundant nucleotide sequences from all traditional divisions of GenBank, EMBL, and DDBJ excluding GSS,STS, PAT, EST, HTG, and WGS.
other_genomic.*tar.gz         #  RefSeq chromosome records (NC_) for non-human organisms
pataa.*tar.gz         #  Patent protein sequences
patnt.*tar.gz         #  Patent nucleotide sequences. Both patent databases are directly from the USPTO, or from the EPO/JPO via EMBL/DDBJ
pdbaa.*tar.gz         #  Sequences for the protein structure from the Protein Data Bank
pdbnt.*tar.gz         #  Sequences for the nucleotide structure from the Protein Data Bank. They are NOT the protein coding sequences for the corresponding pdbaa entries.
refseq_genomic.*tar.gz        #  NCBI genomic reference sequences
refseq_protein.*tar.gz        #  NCBI protein reference sequences
refseq_rna.*tar.gz        #  NCBI Transcript reference sequences
sts.*tar.gz           #  Sequences from the STS division of GenBank, EMBL,and DDBJ
swissprot.tar.gz          #  Swiss-Prot sequence database (last major update)
taxdb.tar.gz          #  Additional taxonomy information for the databases listed here providing common and scientific names
tsa_nt.*tar.gz        #  Sequences from the TSA division of GenBank, EMBL,and DDBJ
vector.tar.gz         #  Vector sequences from 2010, see Note 2 in section 4.
wgs.*tar.gz           #  Sequences from Whole Genome Shotgun assemblies





如何下载某个库?
NCBI提供了一个非常智能化的脚本update_blastdb.pl来自动下载所有blast数据库。
$ perl update_blastdb.pl nt

更优化的命令
$ nohup perl update_blastdb.pl --decompress nt >out.log 2>&1 &
自动在后台下载，然后自动解压。（下载到一半断网了，再运行会接着下载，而不会覆盖已经下载好的文件）





(4) blast搜索过程

1) blast构建索引 (makeblastdb)
https://www.ncbi.nlm.nih.gov/books/NBK279688/

如果我们下载的是已经建好索引的数据库，可以省去makeblastdb的过程。

$ makeblastdb -in mature.fa -input_type fasta -dbtype nucl -title miRBase -parse_seqids -out miRBase -logfile File_Name

-in 后接输入文件，你要格式化的fasta序列
-dbtype 后接序列类型，nucl为核酸，prot为蛋白
-title 给数据库起个名，好看~~(不能用在后面搜索时-db的参数)
-parse_seqids 推荐加上，现在有啥原因还没搞清楚
-out 后接数据库名，自己起一个有意义的名字，以后blast+搜索时要用到的-db的参数
-logfile 日志文件，如果没有默认输出到屏幕




2) 资源消耗 
常见的命令参数有下面几个：
	-query <File_In> 要查询的核酸序列
	-db <String> 数据库名字
	-out <File_Out> 输出文件
	-evalue <Real> evalue阈值
	-outfmt <String> 输出的格式

$ blastx -query test.merged.transcript.fasta -db nr -out test.blastx.out

其中fasta文件只有19938行。
可是运行起来耗费了很多资源：
	平均内存消耗：51.45G；峰值：115.37G
	cpu：1个
	运行时间：06:00:24（你敢信？这才是一个小小的test）
所以我强烈推荐用diamond替代blast来做数据库搜索。(??)


3) blast结果解读
每一个合格的序列比对都会给出一个这样的结果（一个query sequence比对到多个就有多个结果）：

	>AAB70410.1 Similar to Schizosaccharomyces CCAAT-binding factor (gb|U88525).
	EST gb|T04310 comes from this gene [Arabidopsis thaliana]
	Length=208
	 
	 Score = 238 bits (607),  Expect = 7e-76, Method: Compositional matrix adjust.
	 Identities = 116/145 (80%), Positives = 127/145 (88%), Gaps = 2/145 (1%)
	 Frame = +1
	 
	Query  253  FWASQYQEIEQTSDFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR  432
				FW +Q++EIE+T+DFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR
	Sbjct  39   FWENQFKEIEKTTDFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR  98
	 
	Query  433  SWNHTEENKRRTLQKNDIAAAITRNEIFDFLVDIVPREDLKDEVLASIPRGTLPMGAPTE  612
				SWNHTEENKRRTLQKNDIAAA+TR +IFDFLVDIVPREDL+DEVL SIPRGT+P  A
	Sbjct  99   SWNHTEENKRRTLQKNDIAAAVTRTDIFDFLVDIVPREDLRDEVLGSIPRGTVPEAA-AA  157
	 
	Query  613  GLPYYYMQPQHAPQVGAPGMFMGKP  687
				G PY Y+    AP +G PGM MG P
	Sbjct  158  GYPYGYLPAGTAP-IGNPGMVMGNP  181　　

结果解读网上很多，这里不啰嗦了。

以下是我在同样条件下测试的diamond：
	平均内存消耗：11.01G；峰值：12.44G
	cpu：1个（571.17%）也就是会自动占用5-6个cpu
	运行时间：00:26:15
	而且diamond注明了，它的优势是处理>1M 的query，量越大速度越快。

diamond的简单用法：
diamond makedb --in nr.fa -d nr
diamond blastx -d nr -q test.merged.transcript.fasta -o test.matches.m8
但是diamond使用有限制，只能用于比对蛋白数据库。





############
# 实例测试
############
(5) 获得序列文件
1)准备库文件所需的fasta
$ grep -v '^[!^#]' GPL18109_family.soft >GPL18109.txt  #去掉注释行
$ awk '$6!="" {print $0}' GPL18109.txt > GPL18109_seq.txt #只保留有序列的行
$ cat GPL18109_seq.txt | awk 'NR>1{print ">"$1"|"$2"|"$3"|"$4"\n"$6}' > GPL18109.fa #一行转为2行，fasta格式
$ cat GPL18109_seq.txt | awk 'NR>1{print ">"$1"\n"$6}' > GPL18109.fa #一行转为2行，fasta格式
行数 143032

2)建库(时间: 2s)
$ makeblastdb -in GPL18109.fa -input_type fasta -dbtype nucl -title probeDB -parse_seqids -out probeDB -logfile probeDB.log
输出log为
$ cat probeDB.log
	Building a new DB, current time: 03/21/2020 21:51:53
	New DB name:   /data/wangjl/blast_data/probeDB
	New DB title:  probeDB
	Sequence type: Nucleotide
	Keep MBits: T
	Maximum file size: 1000000000B
	Adding sequences from FASTA; added 71516 sequences in 0.626761 seconds.

3)比对
##   tail GPL18109.fa >test.fasta #造数据，并为每一行>后添加前缀AA_
$ blastn -query test.fasta -db probeDB -out test.blastn.out


4)查看比对结果
$ cat test.blastn.out | grep -P "^(Query=|>)"
Query= AA_4064|521|33|Y4-New #这相当于未知序列和编号
>4064   #这相当于库中找到的条目id

Query= AA_4970|518|261|Y5-New
>4970
Query= AA_4677|519|167|Y6-New
>4677
Query= AA_5412|517|57|Y7-New
>5412
Query= AA_4497|519|188|Y8-New
>4497


(6) 准备lincRNA库文件所需的fasta格式，统一用hg38;
1) gencode, Release 33 (GRCh38.p13)
https://www.gencodegenes.org/human/
最相近的似乎是
Long non-coding RNA transcript sequences: Nucleotide sequences of long non-coding RNA transcripts on the reference chromosomes

$ cd /data/wangjl/lincRNADB
$ wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/gencode.v33.lncRNA_transcripts.fa.gz

## 2020-03-21 22:16:33 (822 KB/s) - ‘gencode.v33.lncRNA_transcripts.fa.gz’ saved [14647143]

#行数
$ zcat gencode.v33.lncRNA_transcripts.fa.gz |wc
1137813 1137813 70303664


$ gunzip gencode.v33.lncRNA_transcripts.fa.gz ##/home/wangjl/data/lincRNADB ## 1137813行
$ makeblastdb -in gencode.v33.lncRNA_transcripts.fa -input_type fasta -dbtype nucl -title genCodeDB -parse_seqids -out genCodeDB -logfile genCodeDB.log
## 报错 BLAST Database creation error: Near line 1, the local id is too long.  Its length is 110 but the maximum allowed local id length is 50.  Please find and correct all local ids that are too long. 
## 就是第一行太长了，最多50个字符，而这个已经100个了
$ head gencode.v33.lncRNA_transcripts.fa
>ENST00000473358.1|ENSG00000243485.5|OTTHUMG00000000959.2|OTTHUMT00000002840.1|MIR1302-2HG-202|MIR1302-2HG|712|
GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCCTACCCGTGCTTTCT

大于号开头的共 48438 行;
使用python替换掉这些序列: row后面编号
替换后字典文件:
$ head gencode.v33.lncRNA_transcripts_codeNameMap.txt
Row1    ENST00000473358.1       ENSG00000243485.5       OTTHUMG00000000959.2    OTTHUMT00000002840.1    MIR1302-2HG-202 MIR1302-2HG     712
Row2    ENST00000469289.1       ENSG00000243485.5       OTTHUMG00000000959.2    OTTHUMT00000002841.2    MIR1302-2HG-201 MIR1302-2HG     535

fa文件:
$ head gencode.v33.lncRNA_transcripts_short.fa
>Row1
GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCCTACCCGTGCTTTCT

建库: 2s
$ makeblastdb -in gencode.v33.lncRNA_transcripts_short.fa -input_type fasta -dbtype nucl -title genCodeDB -parse_seqids -out genCodeDB -logfile genCodeDB.log

未知信息的序列:(来自于基因芯片) 143032行
$ head ../blast_data/GPL18109.fa
>12414
TTTACCTCGGTGTCCTACCAGCAAGGGGTCCTGTCTGCCACCATCCTCTATGAGATCCTG
>13811
TCTACAGTCTTGAAGAACGGGTTGAAAACAACAGTGTGCCAAGTCGCTTCTCACCTGAAT


比对: (20s) 
$ blastn -query ../blast_data/GPL18109.fa -db genCodeDB -out GPL18109.blastn.out

提取结果:
$ cat GPL18109.blastn.out | grep -P "^(Query=|ROW)" >GPL18109.blastn.out2 #144384行
$ head GPL18109.blastn.out2
Query= 12414 #这一行是未知序列，下面没有>表示没有在库中查到，***** No hits found *****
Query= 13811
Query= 12228
Query= 33906
Query= 653 #这一行是未知序列
ROW37443       111        5e-25 #这行是查到的结果
Query= 108
ROW29246       111        5e-25 #如果查到多个序列，默认取第一个
ROW29245       111        5e-25
...
Query= 79840  
ROW35254       111        5e-25
ROW40969       84.2       1e-16 #第一个e值最小
ROW31327       84.2       1e-16
ROW16715       84.2       1e-16


原比对结果
	Query= 108
	Length=60
												   Score        E
	Sequences producing significant alignments:   (Bits)     Value
												  
	ROW29246                                       111        5e-25
	ROW29245                                       111        5e-25


	>ROW29246
	Length=298

	 Score = 111 bits (60),  Expect = 5e-25
	 Identities = 60/60 (100%), Gaps = 0/60 (0%)
	 Strand=Plus/Plus

	Query  1    ATGAGCAGTGACTTTCTACTCTCCATCCAGCAGCCAACAATAGACTTCGTCATTCAGATG  60
				||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
	Sbjct  196  ATGAGCAGTGACTTTCTACTCTCCATCCAGCAGCCAACAATAGACTTCGTCATTCAGATG  255


使用python把未知序列和已知序列做一一对应，在库中查到多个的，取第一个，查不到的放弃;
13089 行输出
$ head GPL18109_gencode.csv #第一列是芯片的第一列，后面的是gencode fasta第一行的信息
653,Row37443,ENST00000563906.1,ENSG00000261538.1,OTTHUMG00000175441.1,OTTHUMT00000429982.1,AC096996.2-201,AC096996.2,426
108,Row29246,ENST00000638100.1,ENSG00000283138.1,OTTHUMG00000191673.1,OTTHUMT00000489239.1,AC006207.1-202,AC006207.1,298







2) UCSC 
http://genome.ucsc.edu/cgi-bin/hgTables

TUCP(Transcripts with Unknown Coding Potential)
?


ref:
[1]构建NCBI本地BLAST数据库 (NR NT等) | blastx/diamond使用方法 | blast构建索引 | makeblastdb
https://www.cnblogs.com/leezx/p/6425620.html
[2]五分钟搞定一个芯片的重注释，让那些没有genesymbol的数据再次好用
https://shengxin.ren/article/442



========================================
RNA表达量的定量
----------------------------------------

========================================
|-- 定量比对结果 HTseq-count
----------------------------------------
1. 概述
https://www.cnblogs.com/timeisbiggestboss/p/7171535.html

HTSeq:一个用于处理高通量数据（High-throughout sequencing)的python包。
HTSeq包有很多功能类，熟悉python脚本的可以自行编写数据处理脚本。
另外，HTSeq也提供了两个脚本文件能够直接处理数据:htseq-qa(检测数据质量)和htseq-count（reads计数）。
文档：http://htseq.readthedocs.io/

htseq-count用于reads计数的轻便软件。貌似所有能转换为sam格式文件的输出都可以用htseq-count计数。

htseq-count的输入文件
输入为sam/bam格式的文件，如果是paired-end数据必须按照reads名称排序（sort by name）。官方推荐了msort，不过我用起来感觉不是很方便（也可能是使用方法不当），于是我采用了samtools先对bam文件（tophat2的输出结果为bam）排序，再转换为sam。
命令：samtools sort -n file.bam #sort bam by name
      samtools view -h bamfile.bam>samfile.sam
其实 htseq-count 加上 -f bam 参数就可以使用bam格式的输入。




2.下载和安装
$ pip -V
pip 9.0.1 from /home/wangjl/software/anoconda3/lib/python3.6/site-packages (python 3.6)

$ pip insatll htseq
几秒钟后安装好了。

版本号
$ htseq-count -h
...
version 0.11.2.


(2) 下载gtf数据
https://htseq.readthedocs.io/en/release_0.11.1/count.html
Q: I have used a GTF file generated by the Table Browser function of the UCSC Genome Browser, and most reads are counted as ambiguous. Why?
A: In these files, the gene_id attribute incorrectly contains the same value as the transcript_id attribute and hence a different value for each transcript of the same gene. Hence, if a read maps to an exon shared by several transcripts of the same gene, this will appear to htseq-count as and overlap with several genes. Therefore, these GTF files cannot be used as is. Either correct the incorrect gene_id attributes with a suitable script, or use a GTF file from a different source.

从UCSC Table Browser下载的gtf文件不能直接用到htseq-count中，因为gene_id和transcript_id一样，导致同一个gene有不同的value。所以，如果一个read map到多个转录本共享的exon上时，htseq-count就认为和几个基因重叠了。所以，这些gtf文件不能直接使用，要么使用脚本纠正错误的gene_id，或者从不同的途径下载GTF文件。

chr1    hg19_knownGene  exon    13221   14409   0.000000        +       .       gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; 
chr1    hg19_knownGene  exon    11874   12227   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  exon    12646   12697   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  exon    13221   14409   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  start_codon     12190   12192   0.000000        +       .       gene_id "uc010nxq.1"; transcript_id "uc010nxq.1"; 

修改为：
chr1	stdin	transcript	11874	14409	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; 
chr1	stdin	exon	11874	12227	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "1"; exon_id "uc001aaa.3.1";
chr1	stdin	exon	12613	12721	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "2"; exon_id "uc001aaa.3.2";
chr1	stdin	exon	13221	14409	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "3"; exon_id "uc001aaa.3.3";
chr1	stdin	transcript	11874	14409	.	+	.	gene_id "DDX11L1"; transcript_id "uc010nxr.1"; 
chr1	stdin	exon	11874	12227	.	+	.	gene_id "DDX11L1"; transcript_id "uc010nxr.1"; exon_number "1"; exon_id "uc010nxr.1.1";


(2.1)怎么修改？







(2.2)
https://www.gencodegenes.org/human/release_30lift37.html
下载gtf文件和Gene symbol吧。

如何修改gtf文件？
https://www.gencodegenes.org/pages/data_format.html
python版： https://github.com/openvax/gtfparse







3. 用法：
(1)常用
$ htseq-count -h
usage: htseq-count [options] alignment_file gff_file
...
<alignment_file> :contains the aligned reads in the SAM format.
	Make sure to use a splicing-aware aligner such as TopHat.
	To read from standard input, use - as <alignment_file>.

$ cd /home/wangjl/data/afterMapping/quantify
$ htseq-count ../c12_A1_Aligned.out.sam /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid.gtf > htseq_c12_A1.sam.count 2>htseq_c12_A1.sam.count.log
$ ls -lth
total 276K
-rw-r--r--. 1 wangjl user 270K Jan 19 16:58 htseq_c12_A1.sam.count
-rw-r--r--. 1 wangjl user 1.8K Jan 19 16:58 htseq_c12_A1.sam.count.log


警告！如果是bam文件，一定要加-f bam ，否则获得的表达数据全是0！


(2)更多参数
usage: htseq-count [options] alignment_file gff_file

-f {sam,bam}  (default: sam)
-r {pos,name}  (default: name) 比对文件的排序方式。PE数据必须按照pos或者name排序，且必须明确指定。SE忽略该设置。
-s {yes,no,reverse}  (default: yes) reads是否匹配到同一条链上。 #此处关于选项-s为我自己的认识，不一定对
    #数据是否来源于链特异性测序，链特异性是指在建库测序时，只测mRNA反转录出的cDNA序列，而不测该cDNA序列反向互补的另一条DNA序列；换句话说就是，链特异性能更准确反映出mRNA的序列信息
    #我们知道在gff/gtf中第7列是+-信息，+表示来源于参考基因组序列正链，-表示参考基因组序列的反向互补链
    #sam/bam文件的第2列是flag信息，也可以看出比对到正链还是负链
    #stranded=no，无链特异性，一条reads通过flag列知道比对到+还是-链后，不管是不是和gff/gtf相匹配，都算是这个feature中的
    #stranded=yes, 且se测序，要和gff/gtf相匹配，才算是这个feature中的
    #stranded=yes, 且pe测序，要和gff/gtf相匹配，才算是这个feature中的
    #stranded=reverse，是yes的相反，这时不是和gff/gtf相匹配了，而是恰好相反，可能源于另一种链特异性，只测cDNA序列反向互补的另一条DNA序列
-a MINAQUAL (default: 10)
    #忽略比对质量低于此值的比对结果
-t feature type 外显子为最小的定义单位，对基因计数，只需要将包含的外显子计数相加即可。 
    #feature type (3rd column in GFF file) to be used, all features of other type are ignored (default, suitable for Ensembl GTF files: exon)
    #没想到这个还能自己设置
-i IDATTR 最终的计数单位，一般为基因。 默认为：gene_id   也可以设置转录本，但由于模型问题，计数效果不佳。
    #GFF attribute to be used as feature ID (default, suitable for Ensembl GTF files: gene_id)
-m {union,intersection-strict,intersection-nonempty} (default: union) 计数模型，统计reads的时候对一些比较特殊的reads定义是否计入。具体说明如官网图所示。


note:
加上 -i gene_name 是不是就可以输出gene symbol了？测试一下，确实可以。
然后使用 gencode.v30lift37.metadata.HGNC 第二列的HGNC基因3.7万个取交集： awk '{print $2}' gencode.v30lift37.metadata.HGNC|sort|uniq -c|wc
37409   74818  586179





4.输出结果
$ ls *count
SRR3286802.count  SRR3286803.count  SRR3286804.count  SRR3286805.count  SRR3286806.count  SRR3286807.count

#基于相同gff/gtf得到的计数文件，行数相同，第一列（基因名）相同
$ wc -l *count
  37889 SRR3286802.count
  37889 SRR3286803.count
  37889 SRR3286804.count
  37889 SRR3286805.count
  37889 SRR3286806.count
  37889 SRR3286807.count

#且最后5列统计了整个计数过程没有使用到的reads
$ tail -n 5 SRR3286802.count
__no_feature    237560
__ambiguous 1846779
__too_low_aQual 0
__not_aligned   1323985
__alignment_not_unique  2015872

####
their alignment did not overlap any gene (no_feature).
their alignment overlapped with more than one gene (ambiguous);
their alignment quality was lower than the user-specified threshold (too_low_aQual);
they did not align at all (not_aligned);
based on the NH tag in the BAM file, they aligned to more than one place in the reference genome (alignment_not_unique);





http://www.bio-info-trainee.com/244.html





========================================
|-- 定量比对结果 featureCounts: a ultrafast and accurate read summarization program
----------------------------------------
1. featurecounts具有count速度快，兼容性好的特点。
http://blog.sciencenet.cn/home.php?mod=space&uid=2609994&do=blog&id=985692

发现Subread是个功能很全面的软件，而且还有相对应的R包Rsubread。二进制包直接可用。
http://www.360doc.com/content/18/0112/01/50153987_721213961.shtml


软件的作者认为其软件的优点在于（我就复制黏贴了）：

It carries out precise and accurate read assignments by taking care of indels, junctions and structural variants in the reads
It takes only half a minute to summarize 20 million reads（真是快。。。）
It supports GTF and SAF format annotation
It supports strand-specific read counting
It can count reads at feature (eg. exon) or meta-feature (eg. gene) level
Highly flexible in counting multi-mapping and multi-overlapping reads. Such reads can be excluded, fully counted or fractionally counted（这点跟HTSeq-count不一样了，其对于多重比对的reads并不是只采用全部丢弃的策略，按照其说法是更加灵活的对待）
It gives users full control on the summarization of paired-end reads, including allowing them to check if both ends are mapped and/or if the fragment length falls within the specified range（可让使用者更加个性化的使用）
Reduce ambuiguity in assigning read pairs by searching features that overlap with both reads from the pair
It allows users to specify whether chimeric fragments should be counted（考虑的有点周到）
Automatically detect input format (SAM or BAM)
Automatically sort paired-end reads. Users can provide either location-sorted or namesorted bams files to featureCounts. Read sorting is implemented on the fly and it only incurs minimal time cost


2. 可用性和实现：featureCounts作为Subread（http://www.sourceforge.net/projects/subread）或Rsubread（http://www.bioconductor.org/packages/release/bioc/html/Rsubread.html）软件包的一部分
https://sourceforge.net/projects/subread/

$ wget -b https://sourceforge.net/projects/subread/files/subread-1.6.0/subread-1.6.0-Linux-x86_64.tar.gz/download
$ mv download subread-1.6.0-Linux-x86_64.tar.gz
$ tar zxvf subread-1.6.0-Linux-x86_64.tar.gz
$ vim ~/.bashrc 
在该文件结尾增加一行
export PATH=/home/wangjl/software/subread-1.6.0-Linux-x86_64/bin:$PATH
保存后加载改文件，使其生效。
$ source ~/.bashrc

$ featureCounts -v
featureCounts v1.6.0


3. 使用：
$ featureCounts -h 参数有点多。

官方简单教程如下：
$featureCounts -T 6 -p -t exon -g gene_id -a ~/annotation/mm10/gencode.vM13.annotation.gtf -o SRR3589959_featureCounts222.txt SRR3589959.bam

主要的参数：
-a 输入GTF/GFF基因组注释文件
-p 这个参数是针对paired-end数据
-F 指定-a注释文件的格式，默认是GTF
-g 从注释文件中提取Meta-features信息用于read count，默认是gene_id
-t 跟-g一样的意思，其是默认将exon作为一个feature
-o 输出文件
-T 多线程数

其他参数介绍只能看文档了，不常用的话也是记不住的，要用时再去翻就行
运行中和运行后有两张图可以看看，主要讲了其运行中的一些信息，如下：




例子：
# include multimapping
<featureCounts_path>/featureCounts -O -M -Q 30 -p -a genome.gtf -o outputfile input.bam
# exclude multimapping
<featureCounts_path>/featureCounts -Q 30 -p -a genome.gtf -o outputfile input.bam




测试：
$ featureCounts -O -M -Q 30 -p -a genome.gtf -o outputfile input.bam
-O Assign reads to all their overlapping meta-features (or features if -f is specified).
-M Multi-mapping reads will also be counted. For a multi-mapping read, all its reported alignments will be counted. The 'NH' tag in BAM/SAM input is used to detect multi-mapping reads.
-Q <int>    The minimum mapping quality score a read must satisfy in order to be counted. For paired-end reads, at least one end should satisfy this criteria. 0 by default.
-p        If specified, fragments (or templates) will be counted instead of reads. This option is only applicable for paired-end reads.
-a <string>   Name of an annotation file. GTF/GFF format by default.
        See -F option for more format information. Inbuilt annotations (SAF format) is available in 'annotation' directory of the package.


$ featureCounts -T 5 -O -M -a /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid.gtf -o /home/wangjl/data/afterMapping/quantify2/fC_c12_A1.sam.count ../c12_A1_Aligned.out.sam 2>fC_c12_A1.sam.count.log

输出文件的解释：
[wangjl@nih_jin quantify2]$ ls -lth
total 19M
-rw-r--r--. 1 wangjl user  19M Jan 19 21:54 fC_c12_A1.sam.count
-rw-r--r--. 1 wangjl user 3.6K Jan 19 21:54 fC_c12_A1.sam.count.log
-rw-r--r--. 1 wangjl user  330 Jan 19 21:54 fC_c12_A1.sam.count.summary

(1).从log也就是屏幕输出可见，输出为
[wangjl@nih_jin quantify2]$ cat *log

==========     _____ _    _ ____  _____  ______          _____
=====         / ____| |  | |  _ \|  __ \|  ____|   /\   |  __ \
  =====      | (___ | |  | | |_) | |__) | |__     /  \  | |  | |
    ====      \___ \| |  | |  _ <|  _  /|  __|   / /\ \ | |  | |
      ====    ____) | |__| | |_) | | \ \| |____ / ____ \| |__| |
==========   |_____/ \____/|____/|_|  \_\______/_/    \_\_____/
  v1.6.0

//  featureCounts setting  \\
||                                                                            ||
||             Input files : 1 SAM file                                       ||
||                           S ../c12_A1_Aligned.out.sam                      ||
||                                                                            ||
||             Output file : /home/wangjl/data/afterMapping/quantify2/fC_ ... ||
||                 Summary : /home/wangjl/data/afterMapping/quantify2/fC_ ... ||
||              Annotation : /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_ge ... ||
||      Dir for temp files : /home/wangjl/data/afterMapping/quantify2         ||
||                                                                            ||
||                 Threads : 5                                                ||
||                   Level : meta-feature level                               ||
||              Paired-end : no                                               ||
||         Strand specific : no                                               ||
||      Multimapping reads : counted                                          ||
|| Multi-overlapping reads : counted                                          ||
||   Min overlapping bases : 1                                                ||
||                                                                            ||
\\  http://subread.sourceforge.net/  //

//  Running  \\
||                                                                            ||
|| Load annotation file /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid. ... ||
||    Features : 742585                                                       ||
||    Meta-features : 28610                                                   ||
||    Chromosomes/contigs : 152                                               ||
||                                                                            ||
|| Process SAM file ../c12_A1_Aligned.out.sam...                              ||
||    Single-end reads are included.                                          ||
||    Assign reads to features...                                             ||
||    Total reads : 3353091                                                   ||
||    Successfully assigned reads : 2732203 (81.5%)                           ||
||    Running time : 0.04 minutes                                             ||
||                                                                            ||
||                         Read assignment finished.                          ||
||                                                                            ||
|| Summary of counting results can be found in file "/home/wangjl/data/after  ||
|| Mapping/quantify2/fC_c12_A1.sam.count.summary"                             ||
||                                                                            ||
\\  http://subread.sourceforge.net/  //

Successfully assigned reads : 2732203 (81.5%)  说明有81.5%定位到基因上了。
其余的为什么没有定位上？请看summary文件。

(2) $ cat *summary
Status  ../c12_A1_Aligned.out.sam
Assigned        2732203
Unassigned_Unmapped     0
Unassigned_MappingQuality       0
Unassigned_Chimera      0
Unassigned_FragmentLength       0
Unassigned_Duplicate    0
Unassigned_MultiMapping 0
Unassigned_Secondary    0
Unassigned_Nonjunction  0
Unassigned_NoFeatures   620888
Unassigned_Overlapping_Length   0
Unassigned_Ambiguity    0

(3)运行速度？没的说，仅仅Running time : 0.04 minutes！比HTseq快了一个数量级。
fC_c12_A1.sam.count 文件包含了很多杂乱的信息，如果想了解每个基因上的count数，则只需要提取出第1列和第7列的信息
$ cut -f 1,7 fC_c12_A1.sam.count |grep -v '^#' >fC_c12_A1.sam.count.lite








========================================
|-- 定量比对结果 RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome //todo1
----------------------------------------
1. 
paper: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-323
https://deweylab.github.io/RSEM/

安装
wget https://github.com/deweylab/RSEM/archive/v1.3.1.tar.gz
mv v1.3.1.tar.gz RSEM-v1.3.1.tar.gz
tar zxvf RSEM-v1.3.1.tar.gz
cd RSEM-1.3.1/
#To compile RSEM, simply run
make
#To install RSEM, simply put the RSEM directory in your environment's PATH variable. Alternatively, run
make install
or 
#make install DESTDIR=/home/my_name prefix=/software #will install RSEM executables to /home/my_name/software/bin.
make install DESTDIR=/home/ prefix=/wangjl #$ ls ~/bin 已经安装了
rsem-calculate-expression #- Estimate gene and isoform expression from RNA-Seq data.


docs: https://github.com/deweylab/RSEM




2. 使用笔记
RSEM，老牌的工具依旧是笔者的第一选择
原创： 老牛哥哥  生信草堂  8月17日


Alignment-based的转录本定量-RSEM
http://www.bioinfo-scrounger.com/archives/482











========================================
|-- 使用Salmon对RNAseq进行直接定量 //todo How?
----------------------------------------
无需mapping，直接对RNA结果fq文件进行定量。
手册：https://combine-lab.github.io/salmon/getting_started/



使用 salmon 直接对 fq 进行定量。
注意：salmon 产生估计的read计数和估计的转录本每百万( transcripts per million (tpm))，
按照我们的经验，后者过于纠正scRNASeq中的长基因的表达，所以推荐使用read计数。

$ cd /home/wangjl/data/afterMapping/quantify2



1. Salmon is a tool for wicked-fast transcript quantification from RNA-seq data. 
官网：https://combine-lab.github.io/salmon/
文档：http://salmon.readthedocs.io/en/latest/salmon.html

$ cd /home/wangjl/software
$ wget -b https://github.com/COMBINE-lab/salmon/releases/download/v0.9.1/Salmon-0.9.1_linux_x86_64.tar.gz
$ tar xzvf Salmon-0.9.1_linux_x86_64.tar.gz
$ vim ~/.bashrc
结尾增加一行：
export PATH=/home/wangjl/software/Salmon-latest_linux_x86_64/bin:$PATH
保存后执行该文件：
$ source ~/.bashrc 

$ salmon -h
Salmon v0.9.1

Usage:  salmon -h|--help or
        salmon -v|--version or
        salmon -c|--cite or
        salmon [--no-version-check] <COMMAND> [-h | options]

Commands:
     index Create a salmon index
     quant Quantify a sample
     swim  Perform super-secret operation
     quantmerge Merge multiple quantifications into a single file

例子1：
$ salmon quant -i salmon_transcript_index -1 reads1.fq.gz -2 reads2.fq.gz -p #threads -l A -g genome.gtf --seqBias --gcBias --posBias


例子2：
#!/bin/bash
for fn in data/DRR0161{25..40};
do
samp=`basename ${fn}`
echo "Processing sample ${samp}"
salmon quant -i athal_index -l A \
         -1 ${fn}/${samp}_1.fastq.gz \
         -2 ${fn}/${samp}_2.fastq.gz \
         -p 8 -o quants/${samp}_quant
done 
其中
-i 是index位置
-l A 是自动判断文库类型（链特异与否）
-p 指定线程
-o 输出文件位置
输入read文件：-r, -1, -2


(1)生成索引
$ salmon index -t athal.fa.gz -i athal_index

找不到人的transcriptome，没法生成索引。
https://github.com/COMBINE-lab/salmon/issues/186
作者留言说怎么下载。

1).表达组 human transcriptome 下载 
https://www.gencodegenes.org/releases/current.html
我提的github issue: https://github.com/COMBINE-lab/salmon/issues/186

$ wget -c ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_27/gencode.v27.transcripts.fa.gz
不要用axel多线程下载，这个网站太敏感，会禁止访问，欧洲人就是没有美国人大气。







========================================
RNAseq分析: 使用 Limma, DEseq2, edgeR, 筛选差异表达基因DEG
----------------------------------------

详情参考本博客专题 R / 分析差异表达基因DEG。






========================================
|-- 使用DEXSeq分析NGS数据中的exon表达差异 //todo
----------------------------------------
1. 官网: http://bioconductor.org/packages/release/bioc/html/DEXSeq.html

Inference of differential exon usage in RNA-Seq 区分RNAseq中差异表达的外显子

The package is focused on finding differential exon usage using RNA-seq exon counts between samples with different experimental designs. It provides functions that allows the user to make the necessary statistical tests based on a model that uses the negative binomial distribution to estimate the variance between biological replicates and generalized linear models for testing. The package also provides functions for the visualization and exploration of the results.
该包聚焦于使用RNAseq的外显子count数，在不同实验设计的样品间，发现差异外显子，
提供一个基于负二项分布的模型，和广义线性模型的检验，估算生物学重复之间的变异，进行必要的统计学检验。
该包也提供对结果的探索和可视化函数。

安装： 
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("DEXSeq")


文档： 
browseVignettes("DEXSeq") #弹出网页包含pdf文档和示例代码。由于端口会变，所以使用服务器Rstudio时看不到这些文件。





2.sam文件转换成counts数据
#第一步：计数

(1)把gtf文件转为gff文件
# 找到DEXSeq提供的python角本的路径。
system.file('python_scripts', package='DEXSeq')
# [1] "/home/wangjl/R/x86_64-pc-linux-gnu-library/3.6/DEXSeq/python_scripts"
#$ python ~/projects/RLib.3.01/DEXSeq/python_scripts/dexseq_prepare_annotation.py Homo_sapiens.GRCh37.75.fixed.gtf DEXSeq.hg19.gene.gff
## # 注意的是，gtf文件必须与mapping的基因组一致，尤其是染色体的名字要一致，注意mapping的bam文件和gtf中是chr1还是1.

> pythonScriptsDir = system.file( "python_scripts", package="DEXSeq" )
> list.files(pythonScriptsDir)
## [1] "dexseq_count.py"              "dexseq_prepare_annotation.py"

官方推荐句子：python /path/to/library/DEXSeq/python_scripts/dexseq_prepare_annotation.py Drosophila_melanogaster.BDGP5.72.gtf Dmel.BDGP5.25.62.DEXSeq.chr.gff


因为一直报错，有人说是版本问题，就改用py2试试。需要重新安装py2的HTSeq包。
$ pip2 install --user HTSeq
$ cd /home/wangjl/data/ref/hg19
$ python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_prepare_annotation.py gencode.v30lift37.basic.annotation.gtf gencode.v30lift37.basic.annotation.gff


这里需要注意的是，gtf文件必须与mapping的基因组一致，尤其是染色体的名字要一致，比如说如果mapping时有chr，gtf文件中的染色体一定也需要有chr。这里运行python是在terminal中，而不是R中。


(2).而后使用dexseq_count.py来计数每个exon上的reads数。 Counting reads
#$ python ~/projectGREEN/RLib.3.01/DEXSeq/python_scripts/dexseq_count.py \
#    -p no -s yes -a 10 -f bam ~/DEXSeq/DEXSeq.hg19.gene.gff bam.file out.counts
#参数-p指出mapping文件是否是pair end文件，默认no。
#参数-s表示是否是stranded，默认为yes。
#-f指输入文件的格式，默认为sam。bam需要安装 pysam包。
# -a to specify the minimum alignment quality，sam文件第五列。跳过低于该值的。

# 在运行计数结束之后，需要检查一下最后四行，看看empty的多不多，如果超过20%，可能需要检查一下mapping的结果
#，当然也可能是计数文件准备错误，比如mapping结果没有index等等。如果以上都不是，那可能是polyA太多了。




$ cd /home/wangjl/data/apa/190705PAS/test/
$ samtools view -h c01_ROW07.bam >c01_ROW07.sam
$ python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py -p no -s yes -a 10 \
/home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  c01_ROW07.sam c01_ROW07.counts





(3)批量化计数
/home/wangjl/data/apa/190705PAS/MAPQ255/c01_ROW07.bam 

同步化的syncHeLa:
c01_ROW24
c01_ROW35
c01_ROW31

非同步化的nonSyncHeLa:
c12_ROW10
c12_ROW16
c12_ROW17

$ cat sync.id | while read id; do echo ${id}; samtools view -h /home/wangjl/data/apa/190705PAS/MAPQ255/${id}.bam > ${id}.sam; \
python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  ${id}.sam ${id}.counts; done;

$ cat non.id | while read id; do echo ${id}; samtools view -h /home/wangjl/data/apa/190705PAS/MAPQ255/${id}.bam > ${id}.sam; \
python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  ${id}.sam ${id}.counts; done;










3.读入R环境中
# 在R中读入计数数据，需要准备好计数文件，实验设计，以及前面用到的gff文件。
#在这里，我们使用Bioconductor中已有的pasilla数据来示例。
library("DEXSeq")
#1.count file names
inDir <- system.file("extdata", package="pasilla")
countFiles <- list.files(inDir, pattern="fb.txt$", full.names=TRUE)
basename(countFiles)
## [1] "treated1fb.txt"   "treated2fb.txt"   "treated3fb.txt"   "untreated1fb.txt" "untreated2fb.txt" 
## [6] "untreated3fb.txt" "untreated4fb.txt"


#2.gff
gffFile <- list.files(inDir, pattern="gff$", full.names=TRUE) #Dmel.BDGP5.25.62.DEXSeq.chr.gff
##注意，如果是自己的数据的话，比如之前示例使用的是DEXSeq.hg19.gene.gff，这里就用DEXSeq.hg19.gene.gff

##实验设计
sampleTable <- data.frame(row.names=c(paste("treated", 1:3, sep=""), paste("untreated", 1:4, sep="")),
                          condition=rep(c("knockdown", "control"), c(3, 4)))
sampleTable
##            condition
## treated1   knockdown
## treated2   knockdown
## treated3   knockdown
## untreated1   control
## untreated2   control
## untreated3   control
## untreated4   control


dxd <- DEXSeqDataSetFromHTSeq(
  countFiles,
  sampleData=sampleTable,
  design= ~sample + exon + condition:exon,
  flattenedfile=gffFile)
## converting counts to integer mode
dim(dxd) #[1] 70463    14
head(dxd)

#查看矩阵
dim(featureCounts(dxd)) #[1] 70463     7
head( featureCounts(dxd), 5 )


# 第三步：获得差异表达数据
#只需要一步
dxr <- DEXSeq(dxd) #耗时
head(dxr)

## 对于结果dxr，可以直接视为data.frame来操作。也可以使用as.data.frame来转换它。
## 结合使用plotDEXSeq就可以查看自己感兴趣的目标基因中的exon的表达情况。
head(unique(dxr$groupID))
plotDEXSeq(dxr, geneID='FBgn0000014')
plotDEXSeq(dxr, geneID='FBgn0010909')
plotDEXSeq( dxr, "FBgn0010909", displayTranscripts=TRUE, legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )



#
########################begin 可选的三步法
## 同DESeq一样，它分为三个步骤：normalization, Dispersion estimation 以及 testing for differential exon usage。
dxd2 <- estimateSizeFactors(dxd) #第一步
dxd3 <- estimateDispersions(dxd2) #第二步，耗时。此时可以使用plotDispEsts(dxd)来观察离散情况
plotDispEsts(dxd3)
## Figure 1: Fit Diagnostics
## The initial per-exon dispersion estimates (shown by black points), the fitted 
## mean-dispersion values function (red line), and the shrinked values in blue
dxd4 <- testForDEU(dxd3) #第三步: differential exon usage (DEU)
#dxd5 <- estimateExonFoldChanges(dxd4, fitExptoVar="condition")
##  Error in estimateExonFoldChanges(dxd4, fitExptoVar = "condition") : 
## unused argument (fitExptoVar = "condition")
dxd5 <- estimateExonFoldChanges(dxd4)
dxr2 <- DEXSeqResults(dxd5) #可以使用plotMA(dxr1)来查看结果
head(dxr2)
#
dxr[1:4,1:4]
dxr2[1:4,1:4] #一模一样
########################end




plotMA(dxr2)
mcols(dxr2)$description #描述意义

#  how many exonic regions are significant with a false discovery rate of 10%:
table ( dxr2$padj < 0.1 )
## FALSE  TRUE 
## 42985   233

##may also ask how many genes are affected
table ( tapply( dxr2$padj < 0.1, dxr2$groupID, any ) )
## FALSE  TRUE 
## 5220   166

plotMA( dxr2, cex=0.8 ) ## Figure 2: MA plot
## Mean expression versus log2 fold change plot. Significant hits at an FDR=0.1 are coloured in red.

#查看样本注释信息
sampleAnnotation(dxd)

## 其他技术和实验因素 6


#具体基因外显子的可视化
plotDEXSeq( dxr2, "FBgn0010909", legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 ) ## Figure 3: Fitted expression
## The plot represents the expression estimates from a call to testForDEU(). 
##  Shown in red is the exon that showed significant differential exon usage.

# 每个样品 with normalized count values of each exon in each of the samples.
plotDEXSeq( dxr2, "FBgn0010909", expression=FALSE, norCounts=TRUE, 
            legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )

# after subtraction of overall changes in gene expression.
plotDEXSeq( dxr2, "FBgn0010909", expression=FALSE, splicing=TRUE,
            legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )
#
#把所有的显著性差异的基因的相关图片全部画一遍
DEXSeqHTML( dxr, FDR=0.1, color=c("#FF000080", "#0000FF80") )
getwd()








# 不知道干啥用的
wh = (dxr2$groupID=="FBgn0010909")
stopifnot( sum(dxr2$padj[wh] < formals(plotDEXSeq)$FDR)==1 )


##
# 查看gene
head( geneIDs(dxd) )
## [1] "FBgn0000003" "FBgn0000008" "FBgn0000008" "FBgn0000008" "FBgn0000008" "FBgn0000008"

# 查看外显子
head( exonIDs(dxd) )
## [1] "E001" "E001" "E002" "E003" "E004" "E005"


## Overlap operations
interestingRegion = GRanges( "chr2L", IRanges(start=3872658, end=3875302) )
subsetByOverlaps( x=dxr, ranges=interestingRegion )
#
#This functions could be useful for further downstream analysis.
findOverlaps( query=dxr2, subject=interestingRegion )
## queryLength: 70463 / subjectLength: 1




#
library(ggplot2)
ggplot(as.data.frame(dxr2), aes( log10(exonBaseMean), log2fold_knockdown_control) )+ geom_point()

library(ggplot2)
ggplot(as.data.frame(dxr2), aes( log10(exonBaseMean), dispersion) )+ geom_point()







refer:
1.https://www.plob.org/article/6960.html
2.http://www.bio-info-trainee.com/bioconductor_China/software/DEXSeq.html
3.http://www.biotrainee.com/thread-1220-1-1.html

4. #软件工具#使用DEXSeq分析NGS数据中的exon表达差异 泊客  云生信学生物信息学  2016-10-26




========================================
|-- 加权基因共表达网络分析 (WGCNA, Weighted correlation network analysis)
----------------------------------------
https://mp.weixin.qq.com/s?__biz=MzI5MTcwNjA4NQ==&mid=2247485220&idx=1&sn=007188964e7c43d75dcd0b11b880bbfa&scene=21#wechat_redirect





========================================
bedtools : a powerful toolset for genome arithmetic 强有力的基因组算法瑞士军刀
----------------------------------------
最新版的官网：
http://bedtools.readthedocs.io/en/latest/index.html

旧版本的pdf：
https://insidedna.me/tool_page_assets/pdf_manual/bedtools.pdf

Collectively, the bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF. While each individual tool is designed to do a relatively simple task (e.g., intersect two interval files), quite sophisticated analyses can be conducted by combining multiple bedtools operations on the UNIX command line.

bedtools is developed in the Quinlan laboratory at the University of Utah and benefits from fantastic contributions made by scientists worldwide.



安装方法：
#apt-get install bedtools #Debian/Ubuntu. 
#yum install BEDTools #Fedora/Centos

或者：
$ wget https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz
$ tar -zxvf bedtools-2.25.0.tar.gz
$ cd bedtools2
$ make
$ make prefix=~/ install #安装到家目录下的bin文件夹中。


查看版本号：
$ bedtools -version 
## bedtools v2.25.0

查看帮助：
$ bedtools -help 


使用方法：
# bedtools sorted
$ bedtools intersect \
           -a ccds.exons.bed -b aln.bam.bed \
           -c \
           -sorted

# bedtools unsorted
$ bedtools intersect \
           -a ccds.exons.bed -b aln.bam.bed \
           -c

# bedmap (without error checking)
$ bedmap --echo --count --bp-ovr 1 \
         ccds.exons.bed aln.bam.bed

# bedmap (no error checking)
$ bedmap --ec --echo --count --bp-ovr 1 \
         ccds.exons.bed aln.bam.bed

# bam to bed(bed文件比bam文件坐标小1)
$ bedtools bamtobed -i reads.bam | head -3
-i 后面跟着输入的bam文件




更多用法：
http://bedtools.readthedocs.io/en/latest/content/example-usage.html
高级用法：
http://bedtools.readthedocs.io/en/latest/content/advanced-usage.html

Report the number of genes that each alignment overlaps.
$ bedtools intersect -a reads.bed -b genes.bed -c



========================================
samtools用法 – Utilities for the Sequence Alignment/Map (SAM) format
----------------------------------------
代码： https://github.com/samtools/samtools
文档： http://www.htslib.org/doc/samtools.html


1.安装

(1) 不一定work...
第一步，从github下载所需版本samtools软件包
https://sourceforge.net/projects/samtools/files/samtools/1.5/

第二步，解压，此处以目前最新版samtools-1.5为例：
tar -jxf samtools-1.5.tar.bz2
cd samtools-1.5

第三步，编译，安装：
make
make prefix=/opt/biosoft/samtools-1.5 install

第四步，加入环境变量
echo 'export PATH=$PATH:/opt/biosoft/samtools-1.5/bin' >> /etc/profile

现在你已经不需要刚刚下载和解压的软件包了，愉快的删除吧。
cd ../ && rm -rf samtools-1.5 samtools-1.5.tar.bz2


bcftools安装几乎完全一样。需要注意的是，此处我用了root账户。对于普通用户，可以把软件安装在自己有读写权限的目录下，也就是说，要更改prefix=xxx和/etc/profile至你自己的文件目录和文件。
https://samtools.github.io/bcftools/howtos/install.html




(2)CentOS 亲测可用
从github获取最新版本的包
git clone git://github.com/samtools/samtools.git
git clone https://github.com/samtools/htslib.git #并列放到同一个文件夹下

进入samtools文件夹，执行命令：
autoheader            # Build config.h.in (this may generate a warning about
                      # AC_CONFIG_SUBDIRS - please ignore it).
autoconf -Wno-syntax  # Generate the configure script
./configure           # Needed for choosing optional functionality #如果samtool并列没有HTSlib会报错。
make
##make install #error install: cannot create regular file ‘/usr/local/bin/samtools’: Permission denied
make prefix=~/ install #或在~/bin建软链接

检查版本号：
$ samtools --version
samtools 1.9-69-gb217a91


(3) Ubuntu上安装 https://blog.csdn.net/cuicanlianyue/article/details/79458594

git clone git://github.com/samtools/samtools.git
git clone https://github.com/samtools/htslib.git

需要先安装两个库
sudo apt-get install libbz2-dev #进行编译时出现error：建立HTSlib需要libbzip2文件，需要安装
sudo apt-get install liblzma-dev #进行编译时出现error：需要liblzma文件

然后安装hslib
autoheader
autoconf
./configure 
make
make prefix=~/ install
然后同样命令安装samtools





========================================
|-- samtools常用功能：sam-bam格式转化、排序索引
----------------------------------------
Version: 1.9-69-gb217a91 (using htslib 1.9-149-gf5b75ff)

常用语句：
samtools view -bS input.sam >aln.bam 	#1.转换为bam
samtools sort -o aln.sorted.bam aln.bam #2.排序
samtools index aln.sorted.bam 		#3.建索引


适用于1.3及之后的版本。一步sam转bam并排序。老版本需要2步：1先转bam，2再排序。
$ nohup samtools sort -@ 8 -o ERR188044_chrX.bam ERR188044_chrX.sam 2>ERR188044_sam.err &


常用参数: 
$ samtools sort --help
Usage: samtools sort [options...] [in.bam]
-@, --threads INT     线程数 Number of additional threads to use [0]
-o FILE    Write final output to FILE rather than standard output 输出文件



服务器自带的古老的 0.1.19 可能只支持这么写
Usage:   samtools sort [options] <in.bam> <out.prefix>
最后一个是参数是输出文件的前缀，如：
$ samtools sort c12_ROW03.Mm.bam c12_ROW03.Mm.s
吐槽：CentOS7自带的软件版本都太古老！可靠===过时，不适用于追新的科研。还是用Ubuntu省劲，源都比较新。





$ samtools view -@ 10 -bS -F 4 input.sam > aln.bam
## -F INT   only include reads with none of the FLAGS in INT present [0]

$ samtools depth aln.sorted.bam >depth_reads.txt
$ wc -l depth_reads.txt > Coverage-aln_reads.txt


更多命令：http://www.htslib.org/doc/samtools.html






2.sam 和 bam 格式转换
https://www.cnblogs.com/emanlee/p/4316581.html

(1)SAM转换为BAM
samtools view -bS input.sam >out.bam
samtools view -b -S NA12878.sam > NA12878_2.bam

$ samtools view -bS ../c12_A1_Aligned.out.sam -o c12_A1.bam -@ 10

-b 输出BAM format
-S 输入格式自动检查(sam,bam,cram)。 如果@SQ 缺省， 要写-t (?)
-t FILE:  FILE listing reference names and lengths (see long help) [null]
-o File 输出文件名
-@ 额外使用的线程数

所以如果没有@SQ
samtools faidx ref.fa
samtools view -bt ref.fa.fai out.sam > out.bam


(2)BAM转换为SAM
samtools view -h -o out.sam input.bam
samtools view -h NA12878.bam >NA12878_2.sam
# 参数
 -h       include header in SAM output
 -H       print SAM header only (no alignments)

 -@, --threads INT:  Number of additional threads to use [0]
 -o FILE  output file name [stdout] 输出文件名字[默认是输出到屏幕]
 -O, --output-fmt FORMAT[,OPT[=VAL]]...
        Specify output format (SAM, BAM, CRAM) 指定输出文件的格式




3.排序
$ samtools sort c12_A1.bam -o c12_A1.sorted.bam -@ 10
$ samtools sort -T /tmp/aln.sorted -o aln.sorted.bam aln.bam

# -T 是指定临时文件
-o File 输出到文件而不是标准输出
-@ 线程数


4.建索引
$ samtools index aln.sorted.bam

或者批量化
$ head getIndex.sh
for id in `cat id.txt`
do
  samtools index ${id}.sorted.bam;
done





5. 找snp
对于sort和index过的bam文件
$ samtools pileup -vcf  ref.fa  aln.bam | tee raw.txt | samtools.pl varFilter -D100 > flt.txt
以上命令是寻找最大深度为100的SNP，raw.txt是原始SNP的文件

The -D option of varFilter controls the maximum read depth
$ awk '($3=="*"&&$6>=50)||($3!="*"&&$6>=20)' flt.txt > final.txt
以上是根据sam文件的第三列和第六例进行质量控制。这个根据自己设定的阈值，进行筛选。





========================================
|-- samtools其他功能: 可视化、统计、去重
----------------------------------------
4. tview 直观显示reads比对到基因组的情况，和基因组浏览器有点类似。
-d display		输出类型
-p chr:pos 		直接定位到该位置
-s STR		只显示该sample或group的reads

利用sort进行排序，再利用index建立索引后 samtools tview xx.sort.bam 



(1) 截取bam子集
view  从bam/sam文件中提取/打印部分比对结果。默认为所有的区域，也可以染色体区域（1-based，须sort并index）。
例如：
samtools view -bt ref_list.txt -o aln.bam aln.sam.gz
samtools view aln.sorted.bam chr2:20,100,000-20,200,000
samtools view aln.sorted.bam chr2:20,100,000-20,200,000 > sample.sam #到igv中查看

例：
samtools view -b -h B.addgroup.bam chr2 chr3 chr5 >B_others.bam
以空格分隔要截取的染色体数据，这样，得到比对到chr2，chr3，和chr5的部分bam文件，保存在B_others.bam文件中。

samtools view -b -h 225.sort.bam chr14 >sample.bam
samtools view -b -h 225.sort.bam chr7 >sample.bam
samtools index sample.bam 
## 然后可以用IGV载入了。
## 太慢了，可以考虑多线程 -@ 20



(2)
samtools tview -p B05:53425172 accepted_hits.bam Bju.genome.fa
“.” 比对到正链;
“，” 表示比对到负链;
“<”或“>” 表示reference skip   RNA-seq当中内含子剪切;
"ATCGN"  表示正向mismatch;
"atcgn"  表示反向mismatch;
‘+[0-9]+[ACGTNacgtn]+’ insertion;
‘-[0-9]+[ACGTNacgtn]+’ 表示deletion;
“^”标记reads起始;
“$”标记reads segment结尾;

实例






5.flagstat 对reads的比对情况统计

samtools flagstat xx.sort.bam 





6.depth 每个碱基位点的测序深度
-a 输出所有的碱基深度，包括0
-b/-r 控制深度的范围(后面跟染色体)
-f bam文件名字
-l 设置read长度阈值
-d/-m 最大覆盖深度
-q 碱基质量阈值
-Q 比对质量阈值

samtools depth -a -r chr3 x.sort.bam 
#显示chr3染色体上所有碱基的测序深度，
第一列chr名字，第二列碱基位置，第三列测序深度。





7.mpileup 对参考基因组每个位点做碱基堆积，用于call SNP和INDEL。主要是生成BCF、VCF文件或者pileup一个或多个bam文件。比对记录以在@RG中的样本名作为区分标识符。如果样本标识符缺失，那么每一个输入文件则视为一个样本。

用法：
生成一个简单的vcf文件
samtools mpileup -vu test.sort.bam

如果有参考基因组的话
samtools mpileup -vuf genome.fasta  test.sort.bam



7.dict 建立参考基因组字典
samtools dict  test.sort.bam sequences.fa 




8.fastq bam文件转换为fastq
samtools fastq test.bam



9.idxstats 检索和打印与输入文件相对应的index file里的统计信息
Usage: samtools idxstats <in.bam>

用法：
samtools idxstats test.sort.bam
结果返回一个表格，4列。
第一列：染色体名
第二列：序列长度
第三列：比对上的reads数
第四列：未比对数目



10. stats 对bam文件做详细统计,其统计结果可用mics/plot-bamstats作图
samtools stats test.bam

输出的信息比较多，部分如下：
Summary Numbers，raw total sequences，filtered sequences, reads mapped, reads mapped and paired,reads properly paired等信息
Fragment Qualitites：根据cycle统计每个位点上的碱基质量分布
Coverage distribution：深度为1，2，3，，，的碱基数目
ACGT content per cycle：ACGT在每个cycle中的比例
Insert sizes：插入长度的统计
Read lengths：read的长度分布






11.rmdup 将由PCR duplicates 获得的reads去掉，并保留高比对质量的reads
-s    rmdup for SE reads
-S    treat PE reads as SE in rmdup (force -s)
用法：
samtools rmdup -sS test.bam  output.bam

单端测序结果去除PCR重复
$ samtools rmdup -s tmp.sorted.bam tmp.rmdup.bam

仔细探究samtools的rmdup是如何行使去除PCR重复reads功能的
http://www.bio-info-trainee.com/2003.html

只需要开始-s的标签， 就可以对单端测序进行去除PCR重复。其实对单端测序去除PCR重复很简单的~，因为比对flag情况只有0,4,16，只需要它们比对到染色体的起始终止坐标一致即可，flag很容易一致。

但是对于双端测序就有点复杂了~

很明显可以看出，去除PCR重复不仅仅需要它们比对到染色体的起始终止坐标一致，尤其是flag，在双端测序里面一大堆的flag情况，所以我们的94741坐标的5个reads，一个都没有去除！

这样的话，双端测序数据，用samtools rmdup效果就很差，所以很多人建议用picard工具的MarkDuplicates 功能~~~
The optimal solution depends on many factors - the consensus seems to be the the picard markduplicates could be the best current solution.
最优的方案依赖很多因素，最一致的似乎就是picard markduplicates 可以达到目前最好的结果。

The appropriateness of duplicate removal depends on coverage - one would want to only remove artificial duplicates and keep the natural duplicates.
去除重复的适当性取决于覆盖度——人们只希望去除人工重复并保留自然重复。

MarkDuplicates is "more correct" in the strict sense. Rmdup is more efficient simply because it does handle those tough cases. Rmdup works for single-end, too, but it cannot do paired-end and single-end at the same time. It does not work properly for mate-pair reads if read lengths are different.
MarkDuplicates功能在严格模式下更“正确”，rmdup更高效仅仅是因为它确实处理这些事情。rmdup对于SE数据有效，但是对于同时有PE和SE的无效，对于长度不等的 mate-pair数据也无效。






========================================
|-- Python包pysam: htslib(samtools) interface for python
----------------------------------------
https://pysam.readthedocs.io/en/latest/index.html
Pysam is a python module for reading, manipulating and writing genomic data sets.

1.
>>> line2
<pysam.libcalignedsegment.AlignedSegment object at 0x7f68cb20f888>
>>> samfile
<pysam.libcalignmentfile.AlignmentFile object at 0x7f68ca3800d0>

2. line2的方法名：
['__class__', '__copy__', '__deepcopy__', '__delattr__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'aend', 'alen', 'aligned_pairs', 'bin', 'blocks', 'cigar', 'cigarstring', 'cigartuples', 'compare', 'flag', 'get_aligned_pairs', 'get_blocks', 'get_cigar_stats', 'get_overlap', 'get_reference_positions', 'get_reference_sequence', 'get_tag', 'get_tags', 'has_tag', 'infer_query_length', 'infer_read_length', 'inferred_length', 'is_duplicate', 'is_paired', 'is_proper_pair', 'is_qcfail', 'is_read1', 'is_read2', 'is_reverse', 'is_secondary', 'is_supplementary', 'is_unmapped', 'isize', 'mapping_quality', 'mapq', 'mate_is_reverse', 'mate_is_unmapped', 'mpos', 'mrnm', 'next_reference_id', 'next_reference_name', 'next_reference_start', 'opt', 'overlap', 'pnext', 'pos', 'positions', 'qend', 'qlen', 'qname', 'qqual', 'qstart', 'qual', 'query', 'query_alignment_end', 'query_alignment_length', 'query_alignment_qualities', 'query_alignment_sequence', 'query_alignment_start', 'query_length', 'query_name', 'query_qualities', 'query_sequence', 'reference_end', 'reference_id', 'reference_length', 'reference_name', 'reference_start', 'rlen', 'rname', 'rnext', 'seq', 'setTag', 'set_tag', 'set_tags', 'tags', 'template_length', 'tid', 'tlen', 'tostring']

3. 常用方法
line2.get_tags()[2]
a=line2

#获取所有标签，并给出第n个。缺点：这些数据并不是标准化的，标签位置不完全一样。
a.get_tags()[9] #('CB', 'ACACCCTCATCGGACC-1')
a.get_tags()[12] #('UB', 'ATACATGGTA')

line.get_tag("NH")!=1 #标签NH是否为1
line.has_tag("CB") #是否有CB标签

line.cigarstring # 匹配情况 1S74M
line.flag #16 表示负链， 0表示正链



4.实例代码：从bam文件中逐行读取，NH不等于1的不要，不包含键CB或UB的不要，CB不在预定列表内的不要，
通过三种过滤的条目，保存到一个bam文件。

$ cat filterBAMby3rules_B116.py
################
#第3个脚本，按照NH、CB_UB、cell barcode list过滤bam文件
################
import re
import time
import os


######### 需要修改的文件名
projectName="B116"
path_in = '../hg19_B116/outs/'
fcb=open("B116"+"_CellBarCode_list.txt",'r') #cell barcode部分
########

#读取cb
cbset=set();#cb保存的地方
for lineR in fcb.readlines():
    arr=re.split(" ",lineR.strip())
    cbset.add( arr[0] )
print(len(cbset)) #5973 cell barcode


#读取bam文件
import pysam
samfile = pysam.AlignmentFile(path_in+"possorted_genome_bam.bam", "rb")
#写入bam文件
samOut=pysam.AlignmentFile(projectName+"_NH_CB_list_filtered.bam", "wb",template=samfile)

print("begin for") #
i=0
for line in samfile:
    i=i+1
    ######################
    #进度条
    ######################
    if i%1000000==0:
        tstr=time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        print( tstr+" Processing line:",i)
    #if i>10000:
    #    pass
        #break

    #########
    #三次过滤
    #1
    if line.get_tag("NH")!=1:
        continue;
    #2
    if (not line.has_tag("CB")) or (not line.has_tag("UB")):
        continue;
    #3
    cb=line.get_tag("CB")
    if cb not in cbset:
        continue;

    #########
    #写入文件
    samOut.write(line)


#关闭文件
fcb.close()
samfile.close()
samOut.close()

print("===the End===")











========================================
提取参考基因组某位置的碱基(4种方法)： 根据基因组坐标获得碱基序列(注意bed文件是0-based)
----------------------------------------

1. 利用samtools faidx
samtools faidx 常常用来对参考基因组建立索引，但它还有个鲜为人知的功能，就是序列提取，如下：
-i, --reverse-complement Reverse complement sequences. 反向互补


(1)实例1：(+链上)获得bam文件后面的几个碱基
$ samtools view rmdup_c16_ROW17.bam |head -n 25008|tail -n 1
E00300:165:H3CMMALXX:5:1118:27082:64896	0	chr14	56078824	255	115M	*	0	0	GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAG	AAFFFKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKFFKKKKKKKFKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK	NH:i:1	HI:i:1	AS:i:113	nM:i:0


$ bedtools bamtobed -i rmdup_c16_ROW17.bam |head -n 25008|tail -n 1
chr14	56078823	56078938	E00300:165:H3CMMALXX:5:1118:27082:64896	255	+

结论： bed的坐标是0-based，比原始bam文件(和genome坐标)小1。 



#samtools根据sam坐标获取序列：起点bed比bam小1， 终点56078824+115-1=56078938 同bed。
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr14:56078824-56078938
>chr14:56078824-56078938
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCA
AAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag
##去掉换行符和第一行，多行变一行。和bam文件中的目标序列一致: 
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr14:56078824-56078938 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}' 
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag

查看下游10nt在genome上是什么碱基？(start=oldEnd 算一个， end=start+10-1)
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr14:56078938-56078947 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}' 
gaaaagaaaa #不能包含这个上一个的结束，此后的序列，则应该末尾坐标+1。

#start=oldEnd+1; end=start+10-1
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr14:56078939-56078948 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}'
aaaagaaaaa  #该位置后面在基因组上有4个A。



2)搜索原始RNAseq raw data，发现该序列后面是polyA。不过polyA前面多事GA或TA，而refer后面也有A，不过没有RNA那么多，也就是无法判断从哪里断开的。
$ grep -i GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag c16_ROW17.10A.fq 
CCTTCAATAGTTATTACAGTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGAAAAAAAAAAAAAAAAA
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGAAGG
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGTAAAAAAAAAAAAAAAATAAAAATAAAAAAAATTT
AGTTATTACAGTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGTAAAAAAAAAAAAAAAAAAAAAAAA
CAGTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCAAAACAGATAAAAAGAAAGCAGTAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA





(2)实例2：(-链上)获得bam文件后面的几个碱基
$ samtools view rmdup_c16_ROW17.bam |head -n 10000|tail -n 1
E00300:165:H3CMMALXX:5:1110:4208:28294	16	chr11	65651393	255	49M	*	0	0	CCAAACTCAGAGCAACTTTATTGTCAGCATGGGCGGAGCGTTGGGAGGC	KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKFFFAA	NH:i:1	HI:i:1	AS:i:46	nM:i:1
[wangjl@bio_svr1 MAPQ255]$

$ bedtools bamtobed -i rmdup_c16_ROW17.bam |head -n 10000|tail -n 1
chr11	65651392	65651441	E00300:165:H3CMMALXX:5:1110:4208:28294	255	-

#samtools根据sam坐标获取序列：起点bed比bam小1， 终点 65651393+49-1=65651441 同bed。
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr11:65651393-65651441
>chr11:65651393-65651441
CCAAACTCAGAGCAACTTTATTGTCAGCGTGGGCGGAGCGTTGGGAGGC

CCAAACTCAGAGCAACTTTATTGTCAGCA (from RNA seq). RNA结果其实和refer相差一个碱基


查看下游10nt在genome上(因为是负链，其实是genome上游)是什么碱基？(end=oldStart-1, start=start-10+1)
$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr11:65651389-65651398 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}'
GTCTCCAAAC #测试发现向右多延伸5个碱基，出现CCAAAC结尾，3'端多6个碱基。

$ samtools faidx /home/wangjl/data/ref/hg19/hg19.fa chr11:65651383-65651392 | grep -v ">" | awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}'
GGACCAGTCT
#右端没有出现genomic polyT。


2)bam是按照refer的顺序写的RNA序列，序列保留了RNA的突变信息。+基因和RNA相同，-链基因则和RNA序列反向互补。
搜原始RNA raw data时，需要求bam序列的反向互补序列。
$ grep GCCTCCCAACGCTCCGCCCATGCTGACAATAAAGTTGCTCTGAGTTTGG c16_ROW17.10A.fq 
GCCTCCCAACGCTCCGCCCATGCTGACAATAAAGTTGCTCTGAGTTTGGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAATCGGCAGATGCAGATGGGAAGAGCGTCGGGTGGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATTA







(3)根据 序列ID 查找序列(fq/sam/bed)
$ head -n 1272166 c16_ROW17.fq|tail -n 1
GTAGATGCTATAATAAAAATAGCTGTTTGGTAACCATAGTTTCACTTGTTCAAAGCTGTGTAATCGTGGGGGTACCATCTCAACTGCTTTTGTATTCATTGTATTAAAAGAATCTGTTTAAACAACAAAAAAAAAAAAAAAAAAAAAAAA
$ head -n 1272165 c16_ROW17.fq|tail -n 1
@E00300:165:H3CMMALXX:4:2209:10774:12666 2:N:0:TAGGCATG

$ grep E00300:165:H3CMMALXX:4:2209:10774:12666 2:N:0:TAGGCATG  c16_ROW17.sam
grep: 2:N:0:TAGGCATG: No such file or directory #说明到空格name就断开了
c16_ROW17.sam:E00300:165:H3CMMALXX:4:2209:10774:12666	0	chr5	10265001	255	126M	*	0	0	GTAGATGCTATAATAAAAATAGCTGTTTGGTAACCATAGTTTCACTTGTTCAAAGCTGTGTAATCGTGGGGGTACCATCTCAACTGCTTTTGTATTCATTGTATTAAAAGAATCTGTTTAAACAAC	AAFFFKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKFKKKKKKKKKKKKKKKKKKKKKKKKKKFKKKKKKKKKKKKKKKKKAFKKKK	NH:i:1	HI:i:1	AS:i:124	nM:i:0

$ grep E00300:165:H3CMMALXX:4:2209:10774:12666 h_c16_ROW17.bed 
chr5	10265000	10265126	E00300:165:H3CMMALXX:4:2209:10774:12666	255	+











2. 利用bedtools getfasta (只能输出到文件，但可以批量处理)
bedtools说明文档中对getfasta的描述是“Extract DNA sequences into a fasta file based on feature coordinates.”显而易见，bedtools getfasta的功能就是根据坐标信息提取序列信息。操作如下：


需要准备bed文件，至少三列chr start end。
$ cat pos.bed 
chr14	56078823	56078938	E00300:165:H3CMMALXX:5:1118:27082:64896	255	+

$ bedtools getfasta -fi /home/wangjl/data/ref/hg19/hg19.fa -bed pos.bed -s -fo result.txt
bedtools getfasta有三个必选参数：
-fi即参考基因组fasta文件；
-bed即需要提取的位置坐标信息，格式：chr\tstart\tend；
-fo：输出文件。
-s	Force strandedness. If the feature occupies the antisense, strand, the sequence will be reverse complemented. 根据第六列信息，如果是-则给出反向互补序列。
	- By default, strand information is ignored. 
	不加-s则默认忽略掉链方向，也就是都按照+处理。
有一点需要说明，bedtools接收的是bed文件，而bed文件是0-based。
要获取chr1:n位点的序列，就需要减去1，前闭后开区间 chr1:(n-1)-n

$ cat result.txt 
>chr14:56078823-56078938(+)
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag


(2)如果位置文件有很多行bed格式数据，则能批量获取reads。









3. 利用Python3的pysam模块
import pysam 
ref=pysam.FastaFile("/home/wangjl/data/ref/hg19/hg19.fa")
chr,start,end='chr14','56078823','56078938' #如果是bed文件坐标，则直接用；如果是sam文件坐标，start要减1。
print(ref.fetch(chr)[int(start)-0: int(end)])

## 
GTAATTTTCCTCTTCTTCTGGCTTTTCATGAAAGAAACATTATATGATGAAGTTCTTGCAAAACAGAAAAGAGAACAAAAGCTTATTCCTACCaaaacagataaaaagaaagcag








4.使用UCSC的web接口
http://genome.ucsc.edu/cgi-bin/das/hg19/dna?segment=chr14:56078824,56078938
说明： 使用的起始位置是bam文件中的，比bed文件大1。end=start+115-1，和bed一致。

去除空格后，和上文序列一致。
gtaattttcctcttcttctggcttttcatgaaagaaacattatatgatgaagttcttgcaaaacagaaaagagaacaaaagcttattcctaccaaaacagataaaaagaaagcag








5. 性能比较
三种方法耗时如下：
 	  samtools	bedtools	python
time	0.002s	0.034s	2.077s
可以看出，samtools和bedtools的性能很好，python的性能就比较尴尬了。

其实，个人比较推荐bedtools，比较容易进行批量处理，把想处理的位置信息写到输出文件，然后就可以轻松的进行序列提取。


refer: https://blog.csdn.net/whenfree/article/details/85305616




========================================
IGV使用方法，及导入本地fa和gtf文件
----------------------------------------
IGV User Guide ： http://software.broadinstitute.org/software/igv/UserGuide

目的：
从gencode数据库下载基因注释文件，并且用IGV去查看你感兴趣的基因的结构，比如TP53,KRAS,EGFR等等。



1. 安装: 

IGV 官网http://software.broadinstitute.org/software/igv/download～ Broadinstitute出品
Java version8 下载： https://www.java.com/en/download/mac_download.jsp
IGV内置的物种基因组及基因组来源：http://software.broadinstitute.org/software/igv/Genomes
完整的官方帮助文档：http://software.broadinstitute.org/software/igv/book/export/html/6


Windows用户下载，解压后，点击igv.bat文件即可启动；





2.导入参考基因组及注释文件
(1)Genomes - load genoe fom file---选择上述生成的hg19.fa文件。


# 最好先用samtools 的faidx构建一个.fai索引文件
# for hg19 genome
cd ~/reference/genome/hg19
wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz
gunzip hg19.fa.gz && rm hg19.fa.gz
samtools faidx hg19.fa


# for hg38 genome
cd ~/reference/genome/hg38
wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz
gunzip hg38.fa.gz && rm hg38.fa.gz
samtools faidx hg38.fa


把fa和它的索引.fai放在本地，然后只需要通过Genomes=>Load Genome from File，导入FASTA文件





(2)能导入系统自带的，也能从外部文件导入gtf文件

wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.annotation.gtf.gz
gunzip gencode.v32.annotation.gtf.gz

菜单IGV tools - sort gtf文件，
File => Load from file => 选择解压后、sort后的GTF文件，这是为了能看到基因的信息（IGV底部转录本信息）。
基因组和GTF都有了，就可以载入bam文件查看了


不过好像不能按照基因名查找，但是查看bam文件是绰绰有余了
//todo 如何能按照基因名查找呢？因为它只是个加入的track，如果能通过Genome菜单导入就好了，但是总是报错。





(3) 自定义 gtf文件要满足的条件
向IGV导入gencode hg19 的 gtf文件
https://software.broadinstitute.org/software/igv/GFF
A General Feature Format (GFF) file is a simple tab-delimited text file for describing genomic features. There are several slightly but significantly different GFF file formats. IGV supports the GFF2, GFF3 and GTF file formats.

GTF files must have a .gtf file extension for IGV. 要有.gtf后缀.
See the Computational Genomics Laboratory web site (http://mblab.wustl.edu/GTF2.html) for a description of the GTF file format.




### 载入方式
1)打开IGV菜单Tools->igvtools, 
command中先选择Sort，input中输入gtf绝对路径(gencode.v30lift37.annotation.gtf)，鼠标单击外面，自动生成output file，点击run按钮，大约1min后生成.sorted.gtf文件；
然后使用command中的Index命令，input填写刚生成的gtf文件，点击run，1min后生成.idx文件。
2)然后点击菜单 File - load from file. 选择刚成成的sorted.gtf即可载入轨道。上下拖动到合适的位置。
3)右击左侧文件名，选择expanded，即可看到更多转录本的信息。

不能选这个 genome->Load genome from file， gencode.v30lift37.annotation.sorted.gtf，报错： Could not create index file: xx.gtf.fai 这个是针对hg19.fa文件的。





### IGV官方介绍怎么制作自己的参考文件：
http://software.broadinstitute.org/software/igv/NewGenomeMgmt
Creating a .genome File
1)Click Genomes>Create .genome File. IGV displays the a window where you enter the information.
2)填写一个名字geneCodeV30HG19，这个名字将会显示到IGV左上角。
填写hg19.fa文件路径，gene栏填写对应的gtf文件路径，点击创建，选择地址保存。
大概5min结束，能在硬盘上看到文件 geneCodeV30HG19.genome,54M.
3)IGV左上角下拉框，选择刚生成的genome即可。
但是，还是不行，loading Genome... 等了半天，没有报错，但是还是没载入。//todo2





Feature display name: To override the default setting for which field is used to label the features in the IGV track, add the following line to the file:
##displayName=<field name>

Coloring features: To specify a color for a given feature, you can add this to the file as shown in the following example. Color values can be in either hexadecimal or RGB (r, g, b) format.









3. 使用方法



(1) IGV能干啥
它是一款本地的探索基因组数据的可视化浏览器，有多个系统版本，支持多种不同类型的输入格式，包括芯片测序、二代测序、基因组注释文件等。推荐使用BAM与SAM格式，主要格式见下表

数据来源	文件格式
序列比对	SAM/BAM
显示覆盖率	TDF
拷贝数	    SNP、CN
基因表达	GCT、RES
基因注释	GFF3/GTF、BED
突变数据	MUT
追踪参考基因组覆盖度、测序深度（UCSC）	WIG、BW


(2)
把测序数据用bwa mem比对到参考基因组得到sam格式，最后用samtools转换格式sam to bam


















========================================
gtf文件结合 Gviz R包进行可视化 //todo
----------------------------------------
使用refGenome加上dplyr玩转gtf文件
原创： 生信技能树  生信技能树  2018-12-05








========================================
判断测序文库的链特异性: IGV法, RSeQC统计法
----------------------------------------

#################
#一、IGV法
#################

1.官网
http://www.igv.org/
http://software.broadinstitute.org/software/igv/

官方培训文档： http://www.igv.org/workshops/




#IGV color: A(009600)green; T(FF0000)red; G(D17105)orange; C(0000FF)blue



2. 安装 
(1)
略


3.首先，需要辨明正反链，正义/反义链，编码链，模板链的概念。

DNA 的正链和负链，就是那两条反向互补的链。参考基因组给出的那个链就是所谓的正链（forword），另一条链是反链（reverse）。但是这正反一定不能和正义链（sense strand）反义链（antisense strand）混淆，两条互补的DNA链其中一条携带编码蛋白质信息的链称为正义链，另一条与之互补的称为反义链。但是携带编码信息的正义链不是模板，只是因为它的序列和RNA相同，正义链也是编码链。而反义链虽然和RNA反向互补，但它可是真正给RNA当模板的链，因此反义链也是模板链。

总结两点
- 正义链（sense strand）= 编码链（coding strand）= 非模板链
- forword strand 上可以同时有sense strand 和 antisense strand。因为这完全是两个不同的概念。


(2)最后谈谈正链和正义链。
正链一般是固定的，DNA双链中上面这条链就是正链（+）。
正义链是相对的，它是依据基因的转录方向看的，向右转录（👉）的基因，正链即是它的正义链或有义链或非模板链，向左转录（👈）的基因，负链即是它的正义链。





4.操作步骤
(1).使用star比对获得bam文件
然后对bam文件 sort和index。
如果是star获得的sort过的bam，只需要再用samtools index即可。


(2).使用IGV看比对结果
1) 向IGV中导入参考基因组
Genomes -> Creat a .genome file…
在弹出窗口中加入基因组序列fasta文件与基因组结构注释gff/gtf文件

2)导入比对文件
File -> Load from File…
比对的bam/sam文件需要具有索引
左侧右键，勾上“show coverage track”，即可看到比对上的reads数目和方向。



可能还需要选择菜单 Tools - run igvtools, 把bam转变成tdf文件。
关掉igv，再重新load bam文件，才能看到reads的峰值。




3)检验转录组文库是否具有链特异性

(右键track, 选color alignments by | read strand )
在IGV的Read strand模式中，显示的reads分为红蓝两色，其中红色代表read方向与DNA正链方向相同(5’ -> 3’)，蓝色代表read方向与DNA正链方向相反


如果是双端测序：在First-of-pair strand模式中，红色代表成对的reads中，第一链的方向与正链相同(5’ -> 3’)，蓝色代表成对的reads中，第一链的方向与正链相反(5’ -> 3’)。这对于展示链特异性的文库特别有帮助。
也就是说双末端测序，第一次测得是反义链，第二次测得是正义链。这应该就是链特异性建库。



### For a given transcript, non-directional libraries will show a mix of red and blue reads aligning to the locus.
Directional libraries will show reads of one color in the direction matching the transcript orientation.
### 对于非链特异性的文库，匹配到同一个基因的reads会表现出红蓝混合的情况；
### 对于链特异性的文库，匹配到同一个基因的reads则会表现出与转录本方向相匹配的颜色。




refer:
https://sr-c.github.io/2018/11/06/STAR/
http://www.omicsclass.com/article/300


什么是链特异性建库？https://www.jianshu.com/p/a63595a41bed





#################
#二、使用RSeQC统计
#################
http://rseqc.sourceforge.net/#infer-experiment-py


$ infer_experiment.py -r /home/wangjl/data/ref/hg19/hg19_ucsc_genes-20190509.bed -i c16_ROW17.bam
Reading reference gene model /home/wangjl/data/ref/hg19/hg19_ucsc_genes-20190509.bed ... Done
Loading SAM/BAM file ...  Total 200000 usable reads were sampled


This is SingleEnd Data
Fraction of reads failed to determine: 0.0717
Fraction of reads explained by "++,--": 0.9136
Fraction of reads explained by "+-,-+": 0.0147






========================================
绘制seqlogo图
----------------------------------------
利用ggseqlogo https://blog.csdn.net/kMD8d5R/article/details/79554875
R包: http://www.bioconductor.org/packages/release/bioc/html/seqLogo.html


web tool: http://www.cbs.dtu.dk/biotools/Seq2Logo/
http://www.sohu.com/a/288877744_777125




paper: http://genome.cshlp.org/content/14/6/1188.full.
docs: http://weblogo.berkeley.edu/
中文实例： http://www.dxy.cn/bbs/topic/11662491?from=recommend

发现motif需要的软件： http://homer.ucsd.edu/homer/introduction/install.html




========================================
|-- MEME 查找motif
----------------------------------------

1.安装
http://meme-suite.org/doc/download.html


(1).下载
$ wget http://meme-suite.org/meme-software/5.0.5/meme-5.0.5.tar.gz
$ axel -n 10 http://meme-suite.org/meme-software/Databases/motifs/motif_databases.12.18.tgz
$ axel -n 10 http://meme-suite.org/meme-software/Databases/gomo/gomo_databases.3.2.tgz
$ axel -n 10 http://meme-suite.org/meme-software/Databases/cismapper/cismapper_databases.1.tgz


(2).安装
http://meme-suite.org/doc/install.html?man_type=web
MEME套装包含几个命令行工具，和一个可选的web服务。web需要  批处理器 batch scheduler: SGE and PBS

tar zxf meme-5.0.5.tar.gz
cd meme-5.0.5
./configure --prefix=$HOME/meme --with-url=http://meme-suite.org/ --enable-build-libxml2 --enable-build-libxslt
make
make test
make install


(3). 添加路径
edit ~/.bashrc to add the following line:
$ export PATH=$HOME/meme/bin:$PATH 

以上只安装了命令行工具。
输出的html文件中有指向官网的链接。
The --enable-build-libxml2 --enable-build-libxslt flags use the bundled versions of libxml2, libxslt and libexslt. You can omit these flags and use the system installed copies but ensure that they are compatible as otherwise linker errors will occur.
如果报错，可能是缺少必须的软件： http://meme-suite.org/doc/install.html?man_type=web#prerequisite

$ source ~/.bashrc
$ meme -version
5.0.5



(4)安装motif数据库
1)下载 见上文url
2)解压缩 tar xzf motif_databases.X.tgz #X是版本号
$ tar xzf motif_databases.12.18.tgz 

3)
默认的数据库目录在 安装目录/db/fasta_databases
mv motif_databases ~/meme/db/motif_databases





2. 运行 Starting meme
$ meme c12_ROW03.40ntUP.fasta -dna -oc . -nostatus -time 18000 -mod zoops -nmotifs 3 -minw 6 -maxw 50 -objfun classic -revcomp -markov_order 0

参数意义：
[-oc <output dir>]	输出文件夹 name of directory for output files, will replace existing directory
[-nostatus]		不要在终端输出进度 do not print progress reports to terminal
[-time <t>]		quit before <t> CPU seconds consumed
[-mod oops|zoops|anr]	distribution of motifs
[-nmotifs <nmotifs>]	最多找几个？ maximum number of motifs to find
[-minw <minw>]		最小宽度 minimum motif width
[-maxw <maxw>]		最大宽度 maximum motif width
[-objfun classic|de|se|cd|ce]	objective function (default: classic)
[-revcomp]		allow sites on + or - DNA strands
[-markov_order <order>]	最多使用几个马尔科夫模型？(maximum) order of Markov model to use or create

$ cd /home/wangjl/data/apa/190610APA/03_fasta
$ meme c12_ROW03.40ntUP.fasta -dna -oc ./meme -mod zoops -nmotifs 3 -minw 6 -maxw 20 -objfun classic -revcomp -markov_order 0




报错： Can't locate XML/Parser/Expat.pm in @INC (you may need to install the XML::Parser::Expat module) 
可能需要root运行
# cpan -i XML::Parser::Expat




$ meme 60_pas.fasta -dna -oc ./meme -mod zoops -nmotifs 6 -minw 6 -maxw 10 -objfun classic -markov_order 0

ZOOPS (zero or one occurence per sequence)计数







========================================
|-- HOMER 查找motif (Software for motif discovery and next-sequencing analysis)
----------------------------------------
http://homer.ucsd.edu/homer/


实例:

输入61nt的fasta文件。
$ findMotifs.pl 60_pas.fasta fasta homer_output/ -len 6,7,8,9,10 -p 20  #19:56-21:40
# 查找长度为6,7,8,9,10 的motif，使用20个线程





1. HOMER (v4.10, 5-16-2018) 使用Perl和C++写的。
Software for motif discovery and next generation sequencing analysis

HOMER (Hypergeometric Optimization of Motif EnRichment) is a suite of tools for Motif Discovery and next-gen sequencing analysis.  It is a collection of command line programs for unix-style operating systems written in Perl and C++. HOMER was primarily written as a de novo motif discovery algorithm and is well suited for finding 8-20 bp motifs in large scale genomics data.  HOMER contains many useful tools for analyzing ChIP-Seq, GRO-Seq, RNA-Seq, DNase-Seq, Hi-C and numerous other types of functional genomics sequencing data sets.


2. 安装
http://homer.ucsd.edu/homer/introduction/install.html

前置安装，需要sudo安装 
gcc
g++
make
perl
zip/unzip
gzip/gunzip
wget

需要NGS工具: 
samtools
R (with Bioconductor packages DESeq2, edgeR)
HOMER no longer requires ghostscript and weblogo

(1) 下载页
http://homer.ucsd.edu/homer/download.html
$ mkdir homer
$ wget http://homer.ucsd.edu/homer/configureHomer.pl
$ perl configureHomer.pl -install

Software Installed.  If not done so already, add the homer programs to your executable path.
Add this line to your .bash_profile or .bashrc file (or other depending on your shell):
$ vim ~/.bashrc
export PATH=$PATH:/home/wangjl/data/software/homer/.//bin/
$ source ~/.bashrc

Simply typing "findMotifs.pl" should work before running Homer.


更新 perl configureHomer.pl -update
改变目录 perl configureHomer.pl -install homer (this is good for forcing the software to reinstall - preferred if you think there is something wrong) 



(2) 常用的配置命令
perl /path-to-homer/configureHomer.pl [options]

我们看下这些options都有哪些：

-list 列出可用的包
-install 安装homer需要用到的数据包
-version 安装homer包时，可以指定包版本
-remove 移除包
-update 更新所有包到最新版本
-reinstall 强制重新安装所有已经安装过的包
-all 安装所有包
-getFacts (add humor to HOMER - to remove delete contents of data/misc/)
-check 检查第三方软件：samtools, DESeq2, edgeR
-make 重新配置和编译可执行文件
-sun SunOS系统，使用gmake 和 gtar代替make 和 tar
-keepScript 不更新configureHomer.pl
-url 安装时，使用的资源地址，默认：http://homer.ucsd.edu/homer/
 Hubs & BigWig settings (with read existing settings from config.txt if upgrading):
-bigWigUrl  (Setting for makeBigWigs.pl)
-bigWigDir  (Setting for makeBigWigs.pl)
-hubsUrl  (Setting for makeMultiWigHub.pl)
-hubsDir  (Setting for makeMultiWigHub.pl)
 Configuration files: 下载 update.txt，更新config.txt


(3) 下载数据包
$ pwd  # /home/wangjl/data/software/homer

查询可用的包列表
$ perl configureHomer.pl -list


下载hg19的人类基因组
$ perl configureHomer.pl -install hg19    (to download the hg19 version of the human genome)



HOMER Known Motifs - Genome-wide predictions and UCSC Track
Human hg19 UCSC BigBed Track 170917 (load as a custom track) - [primary BED file]
http://homer.ucsd.edu/homer/data/motifs/homer.KnownMotifs.hg19.170917.bigBed.track.txt
http://homer.ucsd.edu/homer/data/motifs/homer.KnownMotifs.hg19.170917.bed.gz




3. 运行 Running findMotifs.pl with FASTA files:
(1)找到的语句和用途
1)
语法 
findMotifs.pl <targetSequences.fa> fasta <output directory> -fasta <background.fa> [options]
第二个参数必须是物种, 不一定要和fasta文件匹配，甚至可以就是一个简单的占位符"fasta"。

$ findMotifs.pl chuckNorrisGenes.fa human analysis_output/ -fasta normalHumanGenes.fa

还有其他参数控制motif查找。 findMotifs.pl 将会进行GC标准化，默认是自动标准化。 
$ findMotifs.pl lpsInducedGenes.pl mouse LPSMotifResults/ -start -400 -end 100 -len 8,10 -p 4 
## This will search for motifs of length 8 and 10 from -400 to +100 relative to the TSS, using 4 threads (i.e. 4 CPUs)
# 更多参数： http://homer.ucsd.edu/homer/microarray/index.html



2)
RNA Motif Analysis
http://homer.ucsd.edu/homer/motif/rnaMotifs.html
$ findMotifs.pl mir1-downregulated.genes.txt human-mRNA MotifOutput/ -rna -len 8



(2) 输入bed文件
$ findMotifsGenome.pl ChIP-Seq_H3K4Me3_1_homer.bed hg19 ChIP-Seq_H3K4Me3_1_motifDir/ -len 8,10,12
# 参数解释
-输入文件：awk处理好的Homer Peak/Positions file
-参考基因组：这里是hg19
-输出文件：给一个路径和输出文件的名字
-len：motif大小设置，默认8,10,12；越大需要的计算资源越多

上述命令（找motif）每一样品需要运行30-40分钟后，得到文件夹ChIP-Seq_H3K4Me3_1_motifDir,文件夹会有一个网页结果。

常用参数：
-bg：自定义背景序列；
-size: 用于motif寻找得片段大小，默认200bp；-size given 设置片段大小为目标序列长度；越大需要得计算资源越多；
-len：motif大小设置，默认8,10,12；越大需要得计算资源越多；
-S：结果输出多少motifs, 默认25；
-mis：motif错配碱基数，默认2bp；
-norevopp：不进行反义链搜索motif；
-nomotif：关闭重投预测motif；
-rna: 输出RNA motif，使用RNA motif数据库；
-h：使用超几何检验代替二项式分布；
-N：用于motif寻找得背景序列数目，default=max(50k, 2x input)；耗内存参数


2)使用 annotatePeaks.pl 对peaks进行注释
annotatePeaks.pl <Homer Peak/Positions file> <genome>  1>output.peakAnn.xls 2>output.annLog.txt 

使用实例：注释ChIP-Seq_H3K4Me3_1_homer.bed的 peaks 使用命令：
$ annotatePeaks.pl ChIP-Seq_H3K4Me3_1_homer.bed hg19 1>ChIP-Seq_H3K4Me3_1_peakAnn.xls 2>$ChIP-Seq_H3K4Me3_1_annLog.txt 









4.HOMER Motif 分析基本步骤和结果分析
Homer主要被用于 ChIP-Seq 分析，但是核酸序列motif寻找问题都可以尝试使用。


(1)预处理
一  提取序列
提供的数据是基因组位置信息，就需要提取对应的DNA信息；提供基因号时，需要选择启动子区域。对应着就是我们前面用awk 处理bed文件，最后得到要求的那四列。
	第一列: 染色体
	第二列: 起始位置
	第三列: 终止位置
	第四列: 链的方向(+/- or 0/1, where 0="+", 1="-")
 二  背景选择
未指定背景序列时，HOMER 会自动选择。（上面chipseq处理的时候就没有指定背景序列）
对基因组某些区域进行分析时，从基因组随机选择GC含量一致的序列作为背景序列。
对启动子进行分析时，除用来分析外的所有启动子将被作为背景。
自定义背景使用参数-bg
 三  GC 标准化
目标序列（对应着上面的就是Homer Peak/Positions file）和背景序列会基于GC含量按5%作为bin 查看GC含量的分布。背景序列会得到权值，从而使得其GC含量分布与目标序列一致。
ChIP-Seq 实验得到序列GC含量。
 四  自动标准化
需要分析的序列除了GC含量会带来误差，其他的生物学现象，外显子中密码子偏好性或测序实验中偏好性都会影响分析。对于足够强的偏差，HOMER 会自动追踪目标序列和背景中显著差异的特征序列，并通过调整背景序列的权重来平衡输入数据和背景中短寡聚核酸序列不平衡。
短寡聚核酸序列长度可以通过参数-nlen <#>指定。





(2)重头预测Motifs
默认情况下，HOMER 调用homer2 进行motif 分析；通过参数"-homer1" 可以指定老版本工具。

一  将输入序列解析为寡聚核苷酸序列
将输入序列按照motif 长度期望值解析为寡聚核苷酸序列，以及创建Oligo 数据表。Oligo 数据表中记录着每条oligo 在目标序列和背景中被发现的次数。

二  Oligo 自动标准化

三  全局搜索阶段
Oligo 表格信息构建好之后，HOMER 对富集的Oligo 进行全局搜索。如果一个Motif是富集的，那么属于这个Motif的Oligo 也应该会富集。首先，HOMER 会搜索可能富集的Oligo 。HOMER 允许错配 。
使用参数-mis <#> 调节允许的错配数目

四  Mask and Repeat
当最优oligo被优化成motif后，motif 对应的序列从要分析的数据中移除，接下来再分析最优的…..直到 25(默认值，"-S <#>")个motifs 被发现。
比如，我们这里处理chipseq时的情况

五   计算已知Motifs是否富集
3.5.1 导入Motif库
为了搜索输入数据中已知Motifs ，HOMER 可以输入已知Motifs 数据，可以使用HOMER 默认的 ("data/knownTFs/known.motifs")，也可以是自己构建("-mknown") 。

3.5.2 筛选每一个Motif
对于每个motif，HOMER 计算丰度（包含motif的序列/background sequences）， ZOOPS (zero or one occurence per sequence)计数以及使用超几何检验或二项式计算显著性。

















========================================
|-- 升级版Jaspar数据库: 转录因子与DNA结合位点motif最全面的公开数据
----------------------------------------

https://mp.weixin.qq.com/s?__biz=MzI5MTcwNjA4NQ==&mid=2247486375&idx=1&sn=111912aa21e9cc8b09cd139d1a60c26e
http://jaspar.genereg.net/


========================================
MACS(Model-based Analysis of ChIP-Seq) 的安装
----------------------------------------

1. MACS 1.4.2
http://liulab.dfci.harvard.edu/MACS/Download.html



2. MACS2
(1)下载和安装
https://pypi.org/project/MACS2/
https://github.com/taoliu/MACS/releases

好像可以用pip安装
$ pip install MACS2 #报错，必须python2.7
## 再试
$ python2 -V ## Python 2.7.5
$ pip2 install --user MACS2 ##Successfully installed MACS2-2.1.2

查看版本号
$ macs2 --version
macs2 2.1.2



(2)都是输入bam文件，能输入BED文件吗？可以
The BED format can be found at [UCSC genome browser website](http://genome.ucsc.edu/FAQ/FAQformat#format1).

chr7    127471196  127472363  Pos1  0  +
chr7    127472363  127473530  Pos2  0  -


The essential columns in BED format input are the 1st column "chromosome name", the 2nd "start position", the 3rd "end position", and the 6th, "strand".

1) For BED format, the 6th column of strand information is required by MACS. And please pay attention that the coordinates in BED format is zero-based and half-open (http://genome.ucsc.edu/FAQ/FAQtracks#tracks1).
bed的坐标是从0开始的，前闭后开区间。





(3) 运行句子
$ macs2 callpeak -t ChIP.bam -c Control.bam -f BAM -g hs -n test -B -q 0.05
参数解释：
-t/--treatment FILENAME
这是MACS唯一必须的参数，文件可以是 --format 选项指定的任何格式。如果有多个比对文件，可以将它们指定为 -t A B C 。MACS 会将所有这些文件合并在一起。

-c/--control
	control 或 mock(非特异性抗体，如IgG)组
	control: input DNA，没有经过免疫共沉淀处理；
	mock: 1）未使用抗体富集与蛋白结合的DNA片段 2）非特异性抗体，如IgG
-n/--name
	MACS 输出文件名前缀。
-f/--format FORMAT
	声明输入文件的格式，目前 MACS 能够识别的格式有 ELAND、BED、ELANDMULTI、ELANDEXPORT、ELANDMULTIPET（双端测序）、SAM、BAM、BOWTIE、BAMPE、BEDPE。除了BAMPE和BEDPE需要额外声明，其他格式都可以用 AUTO 自动检测。
-g/--gsize
	有效基因组大小(可比对基因组大小)；基因组中有大量重复序列测序测不到，实际上可比对的基因组大小只有原基因组90% 或 70%；人类默认值是– 2.7e9（UCSC human hg18 assembly）
-s/--tsize
	测序读长；如果不设定，MACS 利用输入的 treatment 文件前10个序列自动检测；设定会覆盖自动检测的标签大小。
-q/--qvalue
	q 值（最小的 FDR）的阈值，默认是 0.05 。可以根据结果进行修正，q 值是 p 值经 Benjamini-Hochberg-Yekutieli 修正后的值。
-p/--pvalue
	p 值，如果 -p 设定，MACS2会使用 p 值代替 q 值。
--verbose
	隐藏MACS运行过程信息，设置0；想了解各条染色体peak信息，设置为3或>3的数。
#
https://github.com/taoliu/MACS/





(4) 实战、输出解读
https://www.jianshu.com/p/edfe4ac6b085

macs2 callpeak -t pas.bed -f BED --outdir out_dir2 -n pas_ -g hs -q 0.05









========================================
系统发育树、聚类方法
----------------------------------------
1.
#当order = "hclust"时，可使用hclust.method选择层次聚类的方法
#hclust.method可以为“complete”, “ward”, “single”, “average”, “mcquitty”, “median”, “centroid”

corrplot(matrix,order = "hclust",addrect  = 3,hclust.method="ward.D2")







========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------








========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------








========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------


