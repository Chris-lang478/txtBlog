shell-in-seq


好的脚本事半功倍，并充满了简洁美。
好的脚本值得收藏，并反复研究。


另一个笔记: http://www.biomooc.com/linux/linux-freq.html



========================================
使用find和grep做关键词文件文本搜索，高亮显示
----------------------------------------
1.关键词搜索
$ find . -name "*sh"|xargs grep "hg19_mm10" --color=auto

$ ls *txt | xargs grep "hg19_mm10" --color=auto



2.使用正则表达式搜索
$ find . -name "*sh"|xargs grep "drop.*seq" --color=auto
##./procedure/1208/mef_filterBAM_20180322.sh:# Using drop-seq tools to split bam -> hg19, mm10, and calculate species 
##./procedure/1208/mef_filterBAM_20180322.sh:	java -jar /home/hou/data/apa/p2/procedure/1208/Drop-seq_tools-1.13/jar/dropseq.jar FilterBAM INPUT=$files OUTPUT=hg19/${files%%.*}_$TargetFile.bam REF_SOFT_MATCHED_RETAINED=HUMAN





========================================
查看fastq/sam/bam 文件的常用语句
----------------------------------------

1.查看某一个reads，并显示匹配行后的三行
示例：$ zcat pbmc8k_S1_L007_R1_001.fastq.gz | grep -A3 ST-K00126:314:HFYL2BBXX:7:2103:14996:4725
解释：
 - gzip, gunzip, zcat - compress or expand files
 - grep, egrep, fgrep - print lines matching a pattern
	-A NUM, --after-context=NUM
              Print NUM lines of trailing context after matching lines.  Places a line containing a group separator (described under --group-separator) between contiguous groups of matches.  With the -o or --only-matching option,  this
              has no effect and a warning is given.

实例：
$ zcat ../B116/B116_S1_L005_R1_001.fastq.gz | grep -A3 E00500:97:H7NHJCCXY:5:1209:21978:71946



2.预览bam文件
$ samtools view possorted_genome_bam.bam|less

id  NH
E00500:97:H7NHJCCXY:5:1209:21978:71946 3
E00500:97:H7NHJCCXY:5:1212:18213:37928 4
E00500:97:H7NHJCCXY:5:2116:9516:13334 7
E00500:97:H7NHJCCXY:5:1113:27285:35713 8


3.查找bam文件中匹配的行：
$ samtools view possorted_genome_bam.bam|grep E00500:97:H7NHJCCXY:5:1209:21978:71946



4.查找bam文件的第五列的值的分布情况：
$ samtools view possorted_genome_bam.bam| head -n 10000000|cut -f5|sort|uniq -c
1097428 0
  80075 1
8496404 255
 326093 3







========================================
求匹配某关键词的文件大小总和
----------------------------------------

可以使用管道
ls -l | grep 20130606 | awk '{c += $5}END{print c/1024/1024 "MB"}'

如果文件数量不那么多可以使用
du -m *20130606* | awk '{c+=$1}END{print c}'

这两个命令显示的单位是MB，如果要显示GB可以print c 的c再除以一个1024


========================================
cat合并不同lane的同一个样品数据为单个fastq文件
----------------------------------------

cat S294_05B_CHG011307-Mix2-40sc-9_L006_R2.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L007_R2.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L008_R2.fastq.gz >../column9_R2.fastq.gz

cat S294_05B_CHG011307-Mix2-40sc-9_L006_R1.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L007_R1.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L008_R1.fastq.gz >../column9_R1.fastq.gz


========================================
每隔4行显示一行(查看fastq文件的序列): sed /awk /perl 等实现
----------------------------------------
目的：显示fastq文件的序列，也就是第二行。此后，每隔四行显示一行。


1. sed 命令

$ seq 100|sed '2~4!d'  ## d是删除，!d是不删除。
$ seq 100|sed -n '2~4p'


实例
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '{n;p;n;n}'
或者
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '2~4p'

如果要打印行号：
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '2~4{=;p}'


# 解释
first~step: Match every step'th line starting with line first.  

For example, ``sed -n 1~2p'' will print all the odd-numbered lines in the input stream, and the address 2~5 will match every fifth line, starting with the second.  first can be zero; in this case, sed operates as if it were equal to step.  (This  is an extension.)




2. awk 命令 

$ seq 100 | awk '(NR%4==2)'
$ seq 100 | awk 'NR%4==2'


$ zcat /path/to/CRCSZ05N_S1_L001_R1_001.fastq.gz | head -n 100 | awk 'NR%4==2' 
解释: 
在awk中行号为NR，是从1开始计数的。NR%4余数为2就是第二行，默认动作是打印出来。


如果要打印行号 
$ zcat /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz | head -n 100 | awk 'NR%4==2{print NR"\t"$0}' 
2	ANTCTCCGTTCTTCATTTTGTCATTGGATTAAATATAATATTTTTTTTTTTTTTTTTTATTAATATTTTTTAAAAGAATTATTTTTATATTTAAAATGCCTATTAATTTTATTTAACAAAAATAAAATATTTTTTTTTTTATATCTTTAA
6	TNTCGCCGTAGGCAGTATATGTAATTCTTTATAGATATTATTTTTTTTTTTTTTTTTTTTTTTTTACTTGATTTCTTTGATTATCATGGCCGAGGCACAATGCACCACCACAAACTCTGCCTTTCTGTTGTTAATTGTCAGATTCACTGT







========================================
shell循环高级练习
----------------------------------------
1. 批量解压缩
ls *.gz|while read id ; do gunzip $id;done




2. 批量对bam sort and index
$ cat cids2.txt | while read id;do 
	echo $id; 
	samtools index ${id}.rmdup.bam
done;



# sort
$ echo syncHeLa normalHeLa B0 B1 | xargs -n 1|while read id; do 
	echo $id;
	samtools sort -o ${id}.sort.bam ${id}.bam;
done;

# index
$ echo syncHeLa normalHeLa B0 B1 | xargs -n 1|while read id; do 
	echo $id;
	samtools index ${id}.sort.bam;
done;





3. $(basename)函数：去掉文件路径，只要文件名
$ ls ../R2bam/*bam | head
../R2bam/AAACCCAGTCCAATCA-1.bam
../R2bam/AAACCCAGTCTGTCCT-1.bam
../R2bam/AAACGCTCATGGACAG-1.bam

$ ls ../R2bam/*bam | while read id; do 
	bc=$(basename $id ".bam"); 
	echo $bc; 
done






refer: https://www.jianshu.com/p/f9da70fcaf8d



========================================
新建文件夹，如果有就删除再建
----------------------------------------

TargetFile=mm10
if [ ! -d $TargetFile ]; then ##如果有同名文件怎么办？ todo
    mkdir $TargetFile
else
      rm -rf $TargetFile
      mkdir $TargetFile
fi



========================================
下载大量测序结果，公共数据
----------------------------------------
可以在ftp的README中找到具体下载地址，写入文本文件，使用awk批处理下载这些数据。
上面chrX_data.tar.gz是这些文件的chrX上reads的子集。

$ cat list.txt
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188245/ERR188245_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188428/ERR188428_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188337/ERR188337_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188401/ERR188401_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188257/ERR188257_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188383/ERR188383_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR204/ERR204916/ERR204916_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188234/ERR188234_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188273/ERR188273_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188454/ERR188454_1.fastq.gz
...


$ awk '{print "wget -b "$1}' list.txt | bash
估计要下载几个小时。



========================================
******** 脚本欣赏 ********
----------------------------------------


========================================
脚本欣赏1：复制部分文件到指定文件夹(for循环变量从文件读取、字符串的截取)
----------------------------------------
#!/bin/bash
# Using drop-seq tools to split bam -> hg19, mm10, and calculate species 
# human 108052
# version 0.1

bamFiles=`cat mouse_cells.txt` #先引用本地文件
# echo $bamFiles

cd /home/hou/data/apa/p2/procedure/1208/mef/demultiplex/R2/trim_galore/hg19_mm10/mm10 #接下来都在这个目录下

# mm10_clean

# TargetFile=mm10
# if [ ! -d $TargetFile ]; then
#     mkdir $TargetFile
# else
#       rm -rf $TargetFile
#       mkdir $TargetFile
# fi

#循环内复制文件，从当前mm10目录下把部分bam文件复制到mm10_clean下。
for files in $bamFiles
do
	cp $files*.bam mm10_clean/${files%%_*}.bam
	# echo $files*.bam 
	echo "Finish " $files #复制一个提醒一次
done



========================================
脚本欣赏2：移动文件(shell数组、循环)
----------------------------------------
#!/bin/bash


file_list=( "c12_B4" "c12_C3" "c12_C4" "c12_D3" "c12_E2" "c12_E3" "c13_A5" "c13_C1" "c13_C3" "c13_C4" "c13_D5" "c13_E4" "c13_F1" "c13_G1" "c13_H4" "c13_H5" "c14_C1" "c14_C3" "c14_G2" "c15_A2" "c15_C1" "c15_C5" "c16_A2" "c16_A4" "c16_C3" "c16_D4" "c19_A1" "c19_C1" "c19_G3"  )

for f in ${file_list[@]}
do 
    mv $f*.gz needstar
done



========================================
使用管道输入STAR文件名
----------------------------------------
./procedure/1208/reame.sh:

ls *.gz |while read id;do STAR --runThreadN 12 --outSAMtype BAM SortedByCoordinate --genomeDir /home/hou/data/RNA/refs/hg19_mm10_transgenes_reference/starIndex --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  hg19_mm10/${id%%.*}_; done




========================================
使用star比对，并统计结果到文件
----------------------------------------

基本句型：
1. ls xx | while read id; do xxCMD; done
比如：$ ls *csv|while read id;do  ls -lt $id; done





#!/bin/bash
##########################
## c3
cd /home/hou/data/apa/p2/CHG011307/combine/c3/trimed && mkdir -p star/hg19 star/mm10

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/hg19_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/hg19/${id%%.*}_ ; done

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/mm10_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/mm10/${id%%.*}_ ; done

ls star/mm10/*_Log.final.out | while read id; do  echo $id >> star/mm10/mm10_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/mm10/mm10_mapping_ratio ; done

ls star/hg19/*_Log.final.out | while read id; do  echo $id >> star/hg19/hg19_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/hg19/hg19_mapping_ratio ; done

##########################
## c6
cd /home/hou/data/apa/p2/CHG011307/combine/c6/trimed && mkdir -p star/hg19 star/mm10

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/hg19_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/hg19/${id%%.*}_ ; done

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/mm10_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/mm10/${id%%.*}_ ; done

ls star/mm10/*_Log.final.out | while read id; do  echo $id >> star/mm10/mm10_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/mm10/mm10_mapping_ratio ; done

ls star/hg19/*_Log.final.out | while read id; do  echo $id >> star/hg19/hg19_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/hg19/hg19_mapping_ratio ; done






========================================
多行变一行，不留空格
----------------------------------------

造一个测试文件
$ cat test.txt 
@This is a book.
AATAAA
+
Last Line.


1. 采用awk
# 换行符变成空格
$ awk BEGIN{RS=EOF}'{gsub(/\n/," ");print}' test.txt
@This is a book. AATAAA + Last Line.

# 换行符变成空字符呢？
$ awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}' test.txt
@This is a book.AATAAA+Last Line.

说明：awk默认将记录分隔符（record separator即RS）设置为\n，此行代码将RS设置为EOF（文件结束），也就是把文件视为一个记录，然后通过gsub函数将\n替换成空格，最后输出。



更简单的形式，
$ awk '{printf $0}' test.txt  #但是行结尾没有换行符，导致和下一行命令提示在一行了。
#加上行结尾换行
$ awk '{printf $0}END{print}' test.txt
@This is a book.AATAAA+Last Line.Last Line.




2. 采用sed

$ sed ':a ; N;s/\n// ; t a ; ' test.txt
@This is a book.AATAAA+Last Line.

$ sed ':a;N;s/\n//;ta;' test.txt #不留空格效果一样
@This is a book.AATAAA+Last Line.

说明：sed默认只按行处理，N可以让其读入下一行，再对\n进行替换，这样就可以将两行并做一行。
但是怎么将所有行并作一行呢？可以采用sed的跳转功能。
:a 在代码开始处设置一个标记a，在代码执行到结尾处时利用跳转命令t a重新跳转到标号a处，重新执行代码，这样就可以递归的将所有行合并成一行。

$ sed ':b;N;s/\n//;t b;' test.txt
@This is a book.AATAAA+Last Line.


3. cat test.txt | xargs
@This is a book. AATAAA + Last Line. #不好，换行符变成了空格
说明：这可能是最简单的一种方式。不符合预期效果



refer: https://blog.csdn.net/hjxhjh/article/details/17264739


========================================
批量 重命名 (改名字)
----------------------------------------
1. 后缀名 由 .gz 改为 fastq.gz

for file in ` ls | grep .gz`
do
  newfile=`echo $file | sed 's/gz$/fastq.gz/' `
  echo $file $newfile;
  mv $file $newfile
done




ref:
https://blog.csdn.net/xuleisdjn/article/details/79024586


========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



