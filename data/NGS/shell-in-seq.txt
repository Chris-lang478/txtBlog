shell-in-seq



========================================
1. 给从事生物信息学分析者的一些建议
----------------------------------------
一个良好的工作习惯在任何情况下对提高工资效率来说都是很重要的。


========================================
|-- 推荐的工具(笔记/网盘/文献/编辑器/终端/图片)
----------------------------------------
1. 笔记应用
好记性不如个烂笔头。
生信要点繁多，需要一个电子化、不断完善的笔记系统。
PKM：Personal Knowledge Manager

有道云笔记（国内的）
印象笔记/Evernote （国外用的比较多）
OneNote（微软的）
为知笔记 （国内的）


当然，也可以自己编写一套自己顺手的知识管理系统，比如我使用PHP写的简单强大的txtBlog系统 https://github.com/dawneve/txtBlog



2. 网盘工具(很多网盘倒闭了)
文件存储，这可以弥补上述笔记应用的不足。保存一些文件、软件等。
QQ群文件
dropbox（国外用的比较多，国内现在访问受限）
box（企业级网盘）
Google Driver
OneDrive
金山快盘
360云盘
百度云盘


3.文献管理应用
管理好文献很重要，尤其是在前期关于某一研究领域的知识积累和后期的文章写作。
Endnote（很优秀的文献管理工具，里面的好多功能你可能还没发现）


4.文本编辑 和 集成开发工具（IDE-Integrated Development Environment）
轻量级的：
VIM
Notepad++
sublime3
UltraEdit
EditPlus

重量级的IDE
Eclipse（这是基于Java的一款IDE，可以安装一些常用插件：Perl、Python、PHP等）
RStudio（本地做R开发的利器，有很好的代码美化、项目管理功能，Rgui 弱爆了！）
Vim/Emacs（在Linux终端中使用的比较多的编辑工具，也有些对应的windows版，刚开始不习惯，用多了就会喜欢上的，同样也可以安装很多插件）


5.终端模拟软件
git
Xshell（强大的安全终端模拟软件，支持各种协议）
SecureCRT（同样功能强大的终端模拟软件）
Putty（小巧易用）


6.图片工具
PowerPoint（PPT，用途其实很强大，在拼图等操作中）
picPick(专业屏幕截图软件)
Adobe Photoshop/illustrator（专业的）
美图秀秀（如果要求不是那么专业，基本的操作也是可以胜任的，不仅仅是美图自拍！！！）


7.代码管理
git
github(免费的仓库都是public的)
bitbucket(免费提供private仓库)


========================================
|-- 行为习惯(分类/命名/做笔记/习惯)
----------------------------------------
1.做好分类工作
曾经有人说“用一句话概括这个世界——这个世界就是由分类来组成的”。我们想想——男的-女的、老的-少的、高的-矮的、DNA-RNA-Protein、第一个样本-第二个样本等等。试想下杂乱无章的工作习惯怎么会有高效率呢。所以在分析数据的时候对原始数据进行好的分类，对结果进行好的分类，这样能够提高我们的工作效率，具体几点：

在Linux服务器上，尽量将不能类别的数据放在不同的文件夹下，并逐级分类，这样清晰的组织目录能够很好地提高工作效率，节省无用的时间消耗。

Endnote文献组、浏览器书签、笔记目录、网盘内容等同样要做好分类。





2.好的命名习惯
一个好的文件命名是既能很好的表示文件的内容又能简洁明了。试想下你弄个1.fa 2.fa除了你当时还记得表示的是什么意思，别人看了怎么知道是什么意思呢，可能隔了几点你自己都不知道是什么意思。所以平时不要图简单图方便，而不考虑后续的分析和不站着别人的角度来思考。

数据文件的命名要简明扼要（不冗长也不要太简单）

脚本的命名要表示脚本的意图（符合一定的习惯，每个单词首字母都大写等习惯）




3. 做好笔记记录
在做生物信息学分析过程中，有时不只是自己一个人的事，可能你做的东西会返工也有可能是会给别人看的。所以及时的做好分析流程的记录是很重要的。

不要因为简单而不去记录（有时候可能就是这个简单的操作导致分析的错误，而你又恰好没记录，那就找不到错误的所在）

记录时应该有个好的逻辑条理（不要简单的放句命令在那，应该记录下命令的目的）

记录时应该按照一定的规范（使得笔记整洁明了，内容丰富）


4. 好的学习习惯
在学习一个新的东西时，不要一开始什么东西都去请教别人，一定要是自己先思考，自己对知识有一个了解（哪怕不深入），再不懂的地方去请教别人，这样对别人是一种尊重也是自己加强记忆的方法。



========================================
|-- 思维方式(如何提高效率/写出目标/)
----------------------------------------
这里提到思维方式，是因为好多人可能也是在做些东西，别人怎么做，他也怎么做，可是不知道为什么该这么做，或者也不懂大体的思路，就是那样稀里糊涂的做着。这就需要在遇到问题时怎么去分析问题，解决问题。所以在这里就建议，在碰到一个新的领域或者新的任务时，首先要做的事情是对相关的背景知识有个大体的了解，知道一些常规的处理流程，再进而慢慢的细化，到最后对这个问题心里已经有了一个比较清晰的解决思路了，这时最好用笔记记录下来。

做足前期准备（弄清楚问题是什么）

清晰的思路（怎么一步步解决问题）


refer:
http://ju.outofmemory.cn/entry/162253


========================================
|-- 生物信息分析文件夹-目录命名和组织结构
----------------------------------------
目的：看名字就知道是什么，清晰明了！

1.
一个项目的文件夹里
	依据子任务的划分、分析的步骤再分文件夹，或者使用日期命名。
		ReadMe.txt记录分析主要过程、主要分析步骤。
		script/ 放脚本
		pdf/ 放pdf图片文件
		其他子文件夹放中间数据，重要的阶段性数据在上一层加软链接。


2. 例子






========================================
|-- 参考基因组文件夹怎么组织最好？
----------------------------------------
1.参考其他人的方式
/home/hou/data/RNA/refs/mm10/

/home/hou/data/RNA/refs/hg19/hg19.fa
/home/hou/data/RNA/refs/hg19/hg19_ucsc_genes.gtf 



2. 我现在使用的

参考基因组文件夹的命名
/home/wangjl/data/ref/hg19/index/star/  #看路径就知道是hg19的参考基因组，star的index 




========================================
使用find和grep做关键词文件文本搜索，高亮显示
----------------------------------------
1.关键词搜索
$ find . -name "*sh"|xargs grep "hg19_mm10" --color=auto

$ ls *txt | xargs grep "hg19_mm10" --color=auto



2.使用正则表达式搜索
$ find . -name "*sh"|xargs grep "drop.*seq" --color=auto
##./procedure/1208/mef_filterBAM_20180322.sh:# Using drop-seq tools to split bam -> hg19, mm10, and calculate species 
##./procedure/1208/mef_filterBAM_20180322.sh:	java -jar /home/hou/data/apa/p2/procedure/1208/Drop-seq_tools-1.13/jar/dropseq.jar FilterBAM INPUT=$files OUTPUT=hg19/${files%%.*}_$TargetFile.bam REF_SOFT_MATCHED_RETAINED=HUMAN





========================================
查看fastq/sam/bam 文件的常用语句
----------------------------------------

1.查看某一个reads，并显示匹配行后的三行
示例：$ zcat pbmc8k_S1_L007_R1_001.fastq.gz | grep -A3 ST-K00126:314:HFYL2BBXX:7:2103:14996:4725
解释：
 - gzip, gunzip, zcat - compress or expand files
 - grep, egrep, fgrep - print lines matching a pattern
	-A NUM, --after-context=NUM
              Print NUM lines of trailing context after matching lines.  Places a line containing a group separator (described under --group-separator) between contiguous groups of matches.  With the -o or --only-matching option,  this
              has no effect and a warning is given.

实例：
$ zcat ../B116/B116_S1_L005_R1_001.fastq.gz | grep -A3 E00500:97:H7NHJCCXY:5:1209:21978:71946



2.预览bam文件
$ samtools view possorted_genome_bam.bam|less

id  NH
E00500:97:H7NHJCCXY:5:1209:21978:71946 3
E00500:97:H7NHJCCXY:5:1212:18213:37928 4
E00500:97:H7NHJCCXY:5:2116:9516:13334 7
E00500:97:H7NHJCCXY:5:1113:27285:35713 8


3.查找bam文件中匹配的行：
$ samtools view possorted_genome_bam.bam|grep E00500:97:H7NHJCCXY:5:1209:21978:71946



4.查找bam文件的第五列的值的分布情况：
$ samtools view possorted_genome_bam.bam| head -n 10000000|cut -f5|sort|uniq -c
1097428 0
  80075 1
8496404 255
 326093 3







========================================
求匹配某关键词的文件大小总和
----------------------------------------

可以使用管道
ls -l | grep 20130606 | awk '{c += $5}END{print c/1024/1024 "MB"}'

如果文件数量不那么多可以使用
du -m *20130606* | awk '{c+=$1}END{print c}'

这两个命令显示的单位是MB，如果要显示GB可以print c 的c再除以一个1024


========================================
cat合并不同lane的同一个样品数据为单个fastq文件
----------------------------------------

cat S294_05B_CHG011307-Mix2-40sc-9_L006_R2.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L007_R2.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L008_R2.fastq.gz >../column9_R2.fastq.gz

cat S294_05B_CHG011307-Mix2-40sc-9_L006_R1.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L007_R1.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L008_R1.fastq.gz >../column9_R1.fastq.gz


========================================
sed每隔4行显示一行(查看fastq文件的序列)
----------------------------------------
目的：显示fastq文件的序列，也就是第二行。此后，每隔四行显示一行。

1.语句：
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '{n;p;n;n}'
或者
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '2~4p'

如果要打印行号：
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '2~4{=;p}'



2.解释
first~step: Match every step'th line starting with line first.  

For example, ``sed -n 1~2p'' will print all the odd-numbered lines in the input stream, and the address 2~5 will match every fifth line, starting with the second.  first can be zero; in this case, sed operates as if it were equal to step.  (This  is an extension.)




========================================
shell循环高级练习
----------------------------------------

ls *.gz|while read id ; do gunzip $id;done


refer: https://www.jianshu.com/p/f9da70fcaf8d



========================================
新建文件夹，如果有就删除再建
----------------------------------------

TargetFile=mm10
if [ ! -d $TargetFile ]; then ##如果有同名文件怎么办？ todo
    mkdir $TargetFile
else
      rm -rf $TargetFile
      mkdir $TargetFile
fi





========================================
脚本欣赏1：复制部分文件到指定文件夹(for循环变量从文件读取、字符串的截取)
----------------------------------------
#!/bin/bash
# Using drop-seq tools to split bam -> hg19, mm10, and calculate species 
# human 108052
# version 0.1

bamFiles=`cat mouse_cells.txt` #先引用本地文件
# echo $bamFiles

cd /home/hou/data/apa/p2/procedure/1208/mef/demultiplex/R2/trim_galore/hg19_mm10/mm10 #接下来都在这个目录下

# mm10_clean

# TargetFile=mm10
# if [ ! -d $TargetFile ]; then
#     mkdir $TargetFile
# else
#       rm -rf $TargetFile
#       mkdir $TargetFile
# fi

#循环内复制文件，从当前mm10目录下把部分bam文件复制到mm10_clean下。
for files in $bamFiles
do
	cp $files*.bam mm10_clean/${files%%_*}.bam
	# echo $files*.bam 
	echo "Finish " $files #复制一个提醒一次
done



========================================
脚本欣赏2：移动文件(shell数组、循环)
----------------------------------------
#!/bin/bash


file_list=( "c12_B4" "c12_C3" "c12_C4" "c12_D3" "c12_E2" "c12_E3" "c13_A5" "c13_C1" "c13_C3" "c13_C4" "c13_D5" "c13_E4" "c13_F1" "c13_G1" "c13_H4" "c13_H5" "c14_C1" "c14_C3" "c14_G2" "c15_A2" "c15_C1" "c15_C5" "c16_A2" "c16_A4" "c16_C3" "c16_D4" "c19_A1" "c19_C1" "c19_G3"  )

for f in ${file_list[@]}
do 
    mv $f*.gz needstar
done



========================================
使用管道输入STAR文件名
----------------------------------------
./procedure/1208/reame.sh:

ls *.gz |while read id;do STAR --runThreadN 12 --outSAMtype BAM SortedByCoordinate --genomeDir /home/hou/data/RNA/refs/hg19_mm10_transgenes_reference/starIndex --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  hg19_mm10/${id%%.*}_; done




========================================
使用star比对，并统计结果到文件
----------------------------------------

基本句型：
1. ls xx | while read id; do xxCMD; done
比如：$ ls *csv|while read id;do  ls -lt $id; done





#!/bin/bash
##########################
## c3
cd /home/hou/data/apa/p2/CHG011307/combine/c3/trimed && mkdir -p star/hg19 star/mm10

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/hg19_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/hg19/${id%%.*}_ ; done

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/mm10_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/mm10/${id%%.*}_ ; done

ls star/mm10/*_Log.final.out | while read id; do  echo $id >> star/mm10/mm10_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/mm10/mm10_mapping_ratio ; done

ls star/hg19/*_Log.final.out | while read id; do  echo $id >> star/hg19/hg19_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/hg19/hg19_mapping_ratio ; done

##########################
## c6
cd /home/hou/data/apa/p2/CHG011307/combine/c6/trimed && mkdir -p star/hg19 star/mm10

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/hg19_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/hg19/${id%%.*}_ ; done

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/mm10_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/mm10/${id%%.*}_ ; done

ls star/mm10/*_Log.final.out | while read id; do  echo $id >> star/mm10/mm10_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/mm10/mm10_mapping_ratio ; done

ls star/hg19/*_Log.final.out | while read id; do  echo $id >> star/hg19/hg19_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/hg19/hg19_mapping_ratio ; done






========================================
多行变一行，不留空格
----------------------------------------

造一个测试文件
$ cat test.txt 
@This is a book.
AATAAA
+
Last Line.


1. 采用awk
# 换行符变成空格
$ awk BEGIN{RS=EOF}'{gsub(/\n/," ");print}' test.txt
@This is a book. AATAAA + Last Line.

# 换行符变成空字符呢？
$ awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}' test.txt
@This is a book.AATAAA+Last Line.

说明：awk默认将记录分隔符（record separator即RS）设置为\n，此行代码将RS设置为EOF（文件结束），也就是把文件视为一个记录，然后通过gsub函数将\n替换成空格，最后输出。



更简单的形式，
$ awk '{printf $0}' test.txt  #但是行结尾没有换行符，导致和下一行命令提示在一行了。
#加上行结尾换行
$ awk '{printf $0}END{print}' test.txt
@This is a book.AATAAA+Last Line.Last Line.




2. 采用sed

$ sed ':a ; N;s/\n// ; t a ; ' test.txt
@This is a book.AATAAA+Last Line.

$ sed ':a;N;s/\n//;ta;' test.txt #不留空格效果一样
@This is a book.AATAAA+Last Line.

说明：sed默认只按行处理，N可以让其读入下一行，再对\n进行替换，这样就可以将两行并做一行。
但是怎么将所有行并作一行呢？可以采用sed的跳转功能。
:a 在代码开始处设置一个标记a，在代码执行到结尾处时利用跳转命令t a重新跳转到标号a处，重新执行代码，这样就可以递归的将所有行合并成一行。

$ sed ':b;N;s/\n//;t b;' test.txt
@This is a book.AATAAA+Last Line.


3. cat test.txt | xargs
@This is a book. AATAAA + Last Line. #不好，换行符变成了空格
说明：这可能是最简单的一种方式。不符合预期效果



refer: https://blog.csdn.net/hjxhjh/article/details/17264739


========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



