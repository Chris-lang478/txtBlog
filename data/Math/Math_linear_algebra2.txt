线性代数2

国内课本之外的内容



========================================
MIT 的线性代数
----------------------------------------
https://www.bilibili.com/video/BV1ix411f7Yp?p=2




========================================
|-- 矩阵的列意义: 向量的线性组合
----------------------------------------
方程组
x+y=3
x-2y=-3

写成 列向量 形式
x[1 1]T + y[1 -2]T=[3 -3]T

x=1, y=2




========================================
|-- 高斯消元法解线性方程组: 矩阵行变换相当于左乘一个变换矩阵
----------------------------------------
1. 比如矩阵A=
[1 1]
[1 -2]

(1) 第一次行变换
第二行减去第一行，得到U1=
[1 1]
[0 -3]

相当于A左乘一个矩阵P1=
[1 0]
[-1 1]

P1.A=U1



(2) 第二次行变换
U1第二行除以-3，得到U2=
[1 1]
[0 1]

相当于U1左乘以矩阵P2=
[1 0]
[0 -1/3]


(3) 第三次变换
U2的 r1-r2,得到U3=
[1 0]
[0 1]

相当于U2左乘矩阵P3=
[1 -1]
[0 1]


(4)总结 
P3.U2=U3=E 
P3.(P2.U1)=E 矩阵乘法有结合律，所以括号可以去掉。但是矩阵乘法没有交换律！ 
P3.P2.P1.A=E

P=P3.P2.P1= 自己演算
[2/3 1/3]
[1/3 -1/3]

P.A=E

就是越后期的行变换矩阵，越在更左边。





2. 怎么交换列呢？
右乘一个矩阵。




3. 矩阵A左乘以置换矩阵P得到单位矩阵E，怎么把单位矩阵E变回A矩阵呢？
如果P^-1.P=E, 则 称P^-1是P的逆矩阵。
根据定义，由 P.A=E 则 A^-1=P

注意:
P.A=E 
P.E=A^-1

这就揭示了逆矩阵的一种解法: 对于增广矩阵 A|E 做行变换P，当A变为单位矩阵E时，后面的E就变成了A^-1






## 英语名词
permutation matrix 置换矩阵
identity matrix 单位矩阵





========================================
|-- 当前进度 
----------------------------------------

https://www.bilibili.com/video/BV1ix411f7Yp?p=2
2020.9.13 over;

https://www.bilibili.com/video/BV1ix411f7Yp?p=3









========================================
****** 矩阵专题 (附带R语言代码) ******
----------------------------------------

对矩阵进行特征值分解、奇异值分解、LU分解、QR分解。






========================================
|-- 专题: 矩阵的3大关系(等价/相似/合同)
----------------------------------------
1. 等价
(1)定义: 若A经过有限次的初等变换化为B，就说A和B等价。
(包括初等行变换、初等列变换)
有限次。


(2) 判别法
除了定义法，还经常使用判别法
A和B同型，则A和B等价。 <==> r(A)=r(B)






2. 矩阵相似 
(1) 定义: A和B为n阶矩阵，如果存在可逆的矩阵P，使得 P^-1.A.P=B，则称A和B相似。
记作 A~B。


(2) 矩阵相似的性质
性质1:
1) A~A
2) A~B, 则 B~A;
3) A~B, B~C, 则 A~C;


性质2: A~B，则 r(A)=r(B); 反之不成立。
反例:
A=
|1 0|
|0 2|

B=
|1 -1|
|0 1|

满足 r(A)=r(B)=2;
但是特征值不等
|A-L.E|=0, L1=1, L2=2;
|B-L.E|=0, L1=L2=1;
相似矩阵的特征值是相等的。
所以这里A和B不相似。


性质3: A~B, 则 |A-L.E|=|B-L.E|; 反之不成立。

性质4: A~B, 则 f(A)~f(B);

性质5: A~B, 则 
	1) tr(A)=tr(B); # 因为矩阵的迹等于特征值的乘积；相似矩阵的特征值相同。
	2) |A|=|B|; #因为矩阵的行列式，等于特征值的乘积。
	3) A^T ~ B^T;
	4) 若 A、B可逆，则 A^-1 ~ B^-1, 还有 A*~B*; 
#


(3) 矩阵相似的判别法 (diffucult)

1) 矩阵A和B相似 A ~ B, 则 A和B特征值相同，但是反之不成立。

定理: |A-L.E|=|B-L.E| (就是特征值相同)，且A和B都可以对角化，则A~B。

证明: |A-L.E|=|B-L.E| 可知 A和B特征值相同。设为L1, ..., Ln;
因为A和B可以对交化，则存在可逆矩阵P1和P2，使得
 P1^-1.A.P1=diag(L1,..,Ln)
 P2^-1.B.P2=diag(L1,..,Ln)
所以 P1^-1.A.P1=P2^-1.B.P2, 左乘 P2,右乘P2^-1，得
P2.P1^-1.A.P1.P2^-1=P2.P2^-1.B.P2.P2^-1=B
B=(P2.P1^-1).A.(P1.P2^-1)=(P1.P2^-1)^-1.A.(P1.P2^-1) = P^-1.A.P
其中 P=P1.P2^-1; 怎么证明P可逆呢？因为P1，P2可逆，所以P可逆。  //todo
根据定义 A~B。证毕。





3. 合同 
(1) 矩阵合同的概念，产生于二次型。

f = X^T.A.X = Y^T.(P^T.A.P).Y
其中，A是对称矩阵，而变换后为标准二次型，也就是 P^T.A.P 为对角矩阵 diag(L1,...,Ln)。
等号两边相等，所以叫合同。

定义: A和B为n阶矩阵，若存在可逆矩阵P，使得 P^T.A.P=B，则称A和B合同。记作 A≌B (全等三角形符号)


(2) 判别法
A^T=A, B^T=B, 对称是前提条件！！一定要先保证！
对称矩阵合同 <==> 特征值的正、负、0个数相同。 有专业术语说是正负惯性系数相同。

---> 特征值相同是矩阵相似要求的，矩阵合同不要求特征值相同。

例1: A=
[0 1]
[1 0]
B=diag(-1,4)
判断A和B的关系?
求特征值即可，
|A-L.E|=
|-L 1|
|1 -L|
=(1-L)
|1 1|
|0 -L-1|=-(1-L)(1+L)=0
L1=1, L2=-1;

|B-L.E|=
|-1-L 0|
|0 4-L|=-(1+L)(4-L)=0
L1=-1, L2=4; #这个其实不用算了，对角矩阵的特征值就是对角线元素！

特征值不同，所以不相似。
但是惯性系数相同，所以合同。
















4. 矩阵等价、相似、合同 三种关系的关系 
https://blog.csdn.net/huangmingleiluo/article/details/104211738
https://blog.csdn.net/qq_36468195/article/details/89604688

## 矩阵等价、相似和合同之间的区别：
1)等价，相似和合同三者都是等价关系。
2)矩阵相似或合同必等价，反之不一定成立。
3)矩阵等价，只需满足两矩阵之间可以通过一系列可逆变换，也即若干可逆矩阵相乘得到。
4)矩阵相似，则存在可逆矩阵P使得，AP=PB。
5)矩阵合同，则存在可逆矩阵P使得，P^TAP=B。
6)当上述矩阵P是正交矩阵时，即PT=P(-1)，则有A，B之间既满足相似，又满足合同关系。


## 数学定义
如果A和B矩阵等秩，则等价。存在矩阵P和Q，使得 PAQ=B，则称A,B等价。

A,B均为n阶方阵，若存在可逆矩阵P,使B=P*(-1)AP,则A,B相似。

A,B均为n阶矩阵，若存在可逆矩阵P,使B=P(T)AP,则A,B合同。
	矩阵合同相当于对一个矩阵实施一系列对应的初等行列变换，
	当且仅当P为正交矩阵即P*(-1)=P(T)，才有矩阵相似与合同等价。但是A,B合同，可以直接推出A,B等价。
#



矩阵相似满足两个条件
 - 两个矩阵特征值一样
 - 若特征值相同①一个可对角化一个不可则不相似②两个都可对角化一定相似

矩阵合同满足两个条件
 - A的转置等于A B的转置等于B
 - AB的特征值正负个数一致
#









========================================
|-- 专题: 特征根综合练习
----------------------------------------

1. 回顾 
L1*L2*...*Ln=det(A), 矩阵特征值的乘积等于行列式。

r(A)=n  <==> Li!=0, 1<=i<=n;


Q: 如果前r个特征值不为0，后面都是0，也就是 L1!=0, ..., Lr!=0, L(r+1)=L(r+2)=...=Ln=0,
因为|A|=0，所以 r(A)<n; 
那么 r(A)=r 是否成立？不成立。可举反例，再探究原因。

A: 
反例: A=
|0 1 -1|
|0 0 1|
|0 0 2|
计算行列式 |A-L.E|=
|-L 1 -1|
|0 -L 1|
|0 0 2-L|
=L^2*(2-L)=0
所以 L1=L2=0, L3=2;
有1个特征值不是0，但是秩 r(A)=2.


why? 因为 A 不可对角化。


证明: 当A可对角化，有r个特征值不为0，其余都是0，则 r(A)=r.




例: A3x3，A^T=A,  A^2=2A, r(A)=2, E-A ~ B, 求矩阵B。
解:
1) 令 AX=L.X, 由 A^2=2*A, 则 (A^2-2A)X=(L^2-2*L)X=0, 
因为X!=0, 所以 L(L-2)=0, L1=0, L2=2;

2) 因为 A^T=A, 则A可对角化。
r(A)=2，则A的3个特征值中2个不能是0，则L=2是2重，也就是L2=L3=2;
E-A 的特征值，可以由A的特征值算出来 L1=1-0=1, L2=L3=1-2=-1;

所以 E-A ~ diag(1, -1, -1);







========================================
|-- 矩阵的QR分解 [正交性]
----------------------------------------

矩阵的QR分解和LU分解的目的都是为了便于矩阵计算。

0. 前置知识：

正交矩阵：若n阶方阵A满足 A^T.A=E, 则称A为正交矩阵，简称 正交阵 （复数域上称为酉矩阵）
	- A是正交矩阵的充要条件：A的列(行)向量都是单位向量，且两两正交。
#



一个矩阵如果满足i>j+1时aij=0，则将这个矩阵成为上Hessenberg阵。上Hessenberg阵。
x x x x x
x x x x x
0 x x x x
0 0 x x x
0 0 0 x x






1. 定义: A=Q.R

将矩阵分解成一个正规正交矩阵Q与上三角形矩阵R，所以称为QR分解法。
该算法对对称矩阵和非对称矩阵都适用。

QR（正交三角）分解法是求一般矩阵全部特征值的最有效并广泛应用的方法，一般矩阵先经过正交相似变化成为Hessenberg矩阵，然后再应用QR方法求特征值和特征向量。
它是将矩阵分解成一个正规正交矩阵Q与上三角形矩阵R，所以称为QR分解法，与此正规正交矩阵的通用符号Q有关。

如果实（复）非奇异矩阵A能够化成正交（酉）矩阵Q与实（复）非奇异上三角矩阵R的乘积，即A=QR，则称其为A的QR分解。



计算QR分解的方法一共有三种：
	Gram–Schmidt Orthogonalization: 有数值不稳定的缺点，可以用改进的Gram-Schmidt方法
	Householder Triangularization: 数值稳定，适用于稠密矩阵
	Givens Rotations: 数值稳定，适用于稀疏矩阵
# https://www.zhihu.com/question/23905796/answer/528875727




用施密特正交计算方法, 只需通过Gram-Schmidt过程得到A的标准正交矩阵Q，很快速的求取出R.
https://www.jianshu.com/p/13a81c5b4b9d

例题: 对矩阵A做QR分解，A=
|0 3 1|
|0 4 -2|
|2 1 2|

(1) 记 A=[x1 x2 x3]，其中 x1=[0 0 2]T, x2=[3 4 1]T, x3=[1 -2 2]T;

(2) 做施密特正交化 
y1=x1=[0 0 2]T
y2=x2-<x2,y1>/<y1,y1> *y1=x2-1/2 *y1=[3 4 0]T
y3=x3-<x3,y2>/<y2,y2>*y2 -<x3,y1>/<y1,y1>*y1=x3+1/5*y2-y1=[8/5, -6/5, 0]T

单位化
e1=y1/||y1||=1/2*y1=[0 0 1]T
e2=1/5*y2=[3/5 4/5 0]T
e3=1/2*y3=[4/5 -3/5 0]T

(3) 再看x和e的关系
x1=y1=2e1
x2=1/2*y1+y2=e1+5e2
x3=y1-1/5*y2+y3=2e1-e2+2e3;

(4) 所以 A=Q.R 的分解为
Q=[e1 e2 e3]=
[0 3/5 4/5]
[0 4/5 -3/5]
[1 0 0]

上三角矩阵 R=
[2 1 2]
[0 5 -1]
[0 0 2]



(5) Q^T.A=Q^T.Q.R=R;
所以 R=Q^T.A




(6)R语言代码实现如下：
A=matrix(c(0,3,1,0,4,-2,2,1,2), nrow=3, byrow = T);A
solve(A) #求A^-1

eigen(A) #特征值和特征向量
# 特征根是复数...


# QR 分解
qrresult <- qr(A)
qrresult
Q=qr.Q(qrresult);Q #Q矩阵
#     [,1] [,2] [,3]
#[1,]    0 -0.6 -0.8
#[2,]    0 -0.8  0.6
#[3,]   -1  0.0  0.0
R=qr.R(qrresult);R #R矩阵
#     [,1] [,2] [,3]
#[1,]   -2   -1   -2
#[2,]    0   -5    1
#[3,]    0    0   -2
qr.X(qrresult) #还原矩阵

Q %*% R




(7) 使用QR分解求特征根
https://www.cnblogs.com/chenying99/articles/4967960.html

QR算法求矩阵全部特征值的基本思想是利用矩阵的QR分解通过迭代格式
  Ak=Qk.Rk 
  A(k+1)=Rk.Qk 
将A=A1化成相似的上三角阵，从而求出矩阵A的全部特征值。



## R 代码演示如下:
A=matrix(c(5, -3, 2, 6,-4,4,4,-4,5), nrow=3, byrow = T);A
eigen(A) #3 2 1
qrresult <- qr(A)
Q=qr.Q(qrresult);Q
R=qr.R(qrresult);R

# 反复多次反向相乘，A2=R.Q 的对角线为原矩阵A的特征值
for(i in seq(1,10)){
  A2=R %*% Q;A2
  qrresult <- qr(A2)
  Q=qr.Q(qrresult);Q
  R=qr.R(qrresult);R
};A2
#              [,1]          [,2]        [,3]
#[1,]  2.988494e+00 -1.011336e+00 -12.0220481
#[2,] -1.128289e-02  2.011544e+00  -1.8628214
#[3,]  7.440532e-06 -7.612464e-06   0.9999624

> round(A2)
#     [,1] [,2] [,3]
#[1,]    3   -1  -12
#[2,]    0    2   -2
#[3,]    0    0    1







2. QR 分解的应用

QR 分解经常用来解线性最小二乘法问题。

QR分解的实际计算有很多方法，例如 ivens旋转，Householder变换以及Gram-Schmidt正交化等等。每一种方法都有其优点和不足。

对于非方阵的mxn(m≥n)阶矩阵A也可能存在QR分解。这时Q为m*m阶的正交矩阵，R为m*n阶上三角矩阵。这时的QR分解不是完整的(方阵)，因此称为约化QR分解(对于列满秩矩阵A必存在约化QR分解)。同时也可以通过扩充矩阵A为方阵或者对矩阵R补零，可以得到完全QR分解。














ref:
https://blog.csdn.net/Jakob_Hu/article/details/90901054




========================================
|-- 奇异值分解(singular value decomposition, SVD) [奇异值的平方==特征值]
----------------------------------------
1.将矩阵分解为奇异向量(singular vector)和 奇异值(singular value).

特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法。


2.
通过奇异值分解，我们会得到一些与特征分解相同类型的信息。然而，奇异值分解有更广泛的应用。
>> 每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。
例如，非方阵的矩阵没有特征分解，这时我们只能使用奇异值分解。

在PCA中，使用奇异值分解SVD来代替特征分解，得到特征值和特征向量。




3. 先回顾特征值分解：针对方阵
(1)已知方阵Anxn，根据特征值的定义 
A.x1=L1.x1; 
A.x2=L2.x2; 
...
A.xn=Ln.xn; 
相加得
A.(x1,x2,...,xn)=L1.x1+L2.x2+...+Ln.xn=(x1,x2,...,xn)*diag(L1,...,Ln)
记 W=(x1, x2, ..., xn)，则
A.W=W*diag(L1,...,Ln)

如果W可逆，则 A=W*diag(L1,...,Ln)*W^-1;

(2) 对矩阵W做施密特正交化，归一化，得到正交矩阵W, 
||W||=1, 
W^T.W=E，所以 W^-1=W^T;

所以，上述(1)中的等式可以写成 A=W*diag(L1,...,Ln)*W^T;







4. 对于普通矩阵 Amxn , 假设能写成 Amxn=U.B.V^T的形式，
B是除了主对角线外都是0的矩阵MxN，我们把B的主对角线上的元素叫做奇异值。

其中 U是MxM的，V是NxN的，而且U和V都是酉矩阵。也就是U^T.U=E, V^T.V=E;

(0) 补充 酉矩阵
https://blog.csdn.net/qq_38048756/article/details/102710075

1)酉矩阵（unitary matrix）定义
若n阶复矩阵A满足 AH.A=A.AH=E, 则称A为酉矩阵，记之为A∈UN×N。其中，AH是A的共轭转置。

2)性质
如果A是酉矩阵
i)A^−1=A^H
i)A^−1也是酉矩阵；
i)det(A)=1; （det表示矩阵的行列式）
i)充分条件是它的n个列向量是两两正交的单位向量。

3)共轭转置
首先将A中的每个元素aij取共轭得bij，将新得到的由bij组成的新m*n型矩阵记为矩阵B，再对矩阵B作普通转置得到B^T，即为A的共轭转置矩阵：B^T=A^H

对于矩阵部分的内容在深度学习（花书）第二章线性代数中有一些介绍，如果遇到问题可以在第二章进行寻找。






下面我们将求出U和V，并给出B是对角阵的证明。

(1) 求出左侧的U矩阵
A.A^T 是一个MxM的方矩，求其特征值: (A.A^T).Ui=Lambdai.Ui, 把其M个特征值张成向量U=(u1, ..., uM)。
就是我们SVD公式里面的U矩阵了。一般我们将U中的每个特征向量叫做A的左奇异向量。

(2) 求出右侧的V矩阵
A^T.A 是一个NxN的方矩，求其特征值: (A^T.A).Vi=Lambdai.Vi, 把其N个特征值张成向量V=(v1, ..., vM)。
就是我们SVD公式里面的V矩阵了。一般我们将V中的每个特征向量叫做A的右奇异向量。

(3) 接着求对角矩阵B
U和V我们都求出来了，现在就剩下奇异值矩阵B没有求出了。由于B除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值σ就可以了。

由 A=U.B.V^T，左乘向量V，由V^T.V=E得 A.V=U.B.V^T.V=U.B，
把U和V按向量展开，记作对角矩阵B=diag(sigma1, sigma2, ...)
A.(v1,...,vi)=(u1,...,ui).diag(sigma1,...,sigmai)
A.vi=ui.sigmai, 所以 sigmai=A.vi/ui;

这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵B。

(4) 补充证明:
上面还有一个问题没有讲，就是我们说AT.A的特征向量组成的就是我们SVD中的V矩阵，而AAT的特征向量组成的就是我们SVD中的U矩阵，这有什么根据吗？这个其实很容易证明，我们以V矩阵的证明为例。

A=U.B.VT 两边取转置，A^T=V.B^T.U^T，

1)左乘原始式子，使用 UT.U=E，
A^T.A=V.B^T.U^T.U.B.VT=V.B^T.E.B.VT=V.(B^T.B).VT
而之前证明了对于正交矩阵V有: A^T.A=V.diag(L1,...,La).V^T
所以 V.(B^T.B).VT=V.diag(L1,...,La).V^T，也就是 B^T.B=diag(L1,...,Lv)

可以看出AT.A的特征向量组成的的确就是我们SVD中的V矩阵。
可见B^T.B是对角矩阵，且奇异值的平方等于特征值: diag(sigma1^2,...,sigmav^2)=diag(L1,...,Lv)


2) 同理右乘原始式子，可以得到
A.A^T=U.B.VT.V.B^T.U^T=U.B.E.B^T.U^T=U.(B.B^T).U^T
由特征值的性质得 A.A^T=U.diag(L1,...,Lu).U^T
所以 U.(B.B^T).U^T=U.diag(L1,...,Lu).U^T，也就是 B.B^T=diag(L1,...,Lu)

可以看出A.AT的特征向量组成的的确就是我们SVD中的U矩阵。
可见B.B^T是对角矩阵，且奇异值的平方等于特征值: diag(sigma1^2,...,sigmau^2)=diag(L1,...,Lu)

3) 综合1)和2) 的结论，我们知道 diag(L1,...Lv)和diag(L1,...,Lu)的非零部分是一一相等的，不等的部分都是0。

B=
[1 0 0]
[0 2 0]

B^T=
[1 0]
[0 2]
[0 0]

乘积
B^T.B=diag(1,4,0)
B.B^T=diag(1,4)

我们的特征值矩阵等于奇异值矩阵的平方:
	sigmai^2=Lambdai
	sigmai=sqrt(Lambdai)

我们可以不用σi=Avi/ui来计算奇异值，也可以通过求出AT.A的特征值取平方根来求奇异值。






5. 例题

奇异值矩阵中也是按照从大到小排列。

例1: 对矩阵进行奇异值分解 A=
[0 1]
[1 1]
[1 0]

A^T=
[0 1 1]
[1 1 0]

(1)A.A^T=
[1 1 0]
[1 2 1]
[0 1 1]
|A.A^T -L.E|=0，得
L1=1, u1=[-1 0 1]T;
L2=3, u2=[1 2 1]T;
L3=0, u3=[-1 1 -1]T;
归一化得
U=(u1/|u1|, u2/|u2|, u3/|u3|)=
[-1/sqrt(2), 1/sqrt(6), -1/sqrt(3)]
[0, 2/sqrt(6), 1/sqrt(3)]
[1/sqrt(2), 1/sqrt(6), -1/sqrt(3)]

(2) A^T.A=
[2 1]
[1 2]
|A^T.A-L.E|=0，得
L1=1, v1=[-1 1];
L2=3, v2=[1 1];
归一化得
V=(v1/|v1|, v2/|v2|)=
[-1/sqrt(2), 1/sqrt(2)]
[1/sqrt(2), 1/sqrt(2)]

(3) 用σi=sqrt(λi)直接求出奇异值为1和sqrt(3)
奇异矩阵B=
[1 0]
[0 sqrt(3)]
[0 0]

(4) 最终得到A的奇异值分解为：

A=U.B.V^T=
[-1/sqrt(2), 1/sqrt(6), -1/sqrt(3)]
[0, 2/sqrt(6), 1/sqrt(3)]
[1/sqrt(2), 1/sqrt(6), -1/sqrt(3)]
*
[1 0]
[0 sqrt(3)]
[0 0]
*
[-1/sqrt(2), 1/sqrt(2)]
[1/sqrt(2), 1/sqrt(2)]



(5) 使用R验证一下
U=matrix( c(-1/sqrt(2), 1/sqrt(6), -1/sqrt(3),
0, 2/sqrt(6), 1/sqrt(3),
1/sqrt(2), 1/sqrt(6), -1/sqrt(3)), byrow = T, nrow = 3);U
#            [,1]      [,2]       [,3]
#[1,] -0.7071068 0.4082483 -0.5773503
#[2,]  0.0000000 0.8164966  0.5773503
#[3,]  0.7071068 0.4082483 -0.5773503

V=matrix(c(-1/sqrt(2), 1/sqrt(2),
           1/sqrt(2), 1/sqrt(2)), byrow = T, nrow=2);V
#           [,1]      [,2]
#[1,] -0.7071068 0.7071068
#[2,]  0.7071068 0.7071068

B=diag(x=c(1,sqrt(3)), nrow=3, ncol=2);B
#           [,1]      [,2]
#[1,] -0.7071068 0.7071068
#[2,]  0.7071068 0.7071068

A=round(U %*% B %*% t(V) ); A
#      [,1] [,2]
# [1,]    1    0
# [2,]    1    1
# [3,]    0    1





而用R直接进行SVD分解呢？
> A2=svd(A);A2
$d
[1] 1.732051 1.000000

$u
           [,1]          [,2]
[1,] -0.4082483  7.071068e-01
[2,] -0.8164966 -1.110223e-16
[3,] -0.4082483 -7.071068e-01

$v
           [,1]       [,2]
[1,] -0.7071068  0.7071068
[2,] -0.7071068 -0.7071068

## 这里T是list，注意这里的U和V是矩阵，D是向量，想要恢复原矩阵，需要：
> attach(A2)
> round(u %*% diag(d) %*% t(v))
     [,1] [,2]
[1,]    1    0
[2,]    1    1
[3,]    0    1
> detach(A2)



#---> 感觉不对，u为什么不是方阵?
> A2=svd(A, nu=3);A2$u
           [,1]          [,2]       [,3]
[1,] -0.4082483  7.071068e-01  0.5773503
[2,] -0.8164966 -1.110223e-16 -0.5773503
[3,] -0.4082483 -7.071068e-01  0.5773503





6. SVD的意义
对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。

Am×n=Um×m . Bm×n . VTn×n ≈ Um×k . Bk×k . VTk×n

其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵Um×k,Bk×k,VTk×n来表示。
如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。

由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。
也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。
同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。
下面我们就对SVD用于PCA降维做一个介绍。


1)SVD用于PCA
在主成分分析（PCA）原理总结中，我们讲到要用PCA降维，需要找到样本协方差矩阵XTX的最大的d个特征向量，然后用这最大的d个特征向量张成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵XTX，当样本数多样本特征数也多的时候，这个计算量是很大的。

注意到我们的SVD也可以得到协方差矩阵XTX最大的d个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵XTX，也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是我们我们认为的暴力特征分解。


2)另一方面，注意到PCA仅仅使用了我们SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？

假设我们的样本是m×n的矩阵X，如果我们通过SVD找到了矩阵XXT最大的d个特征向量张成的m×d维矩阵U，则我们如果进行如下处理：

X′d×n=UTd×mXm×n

可以得到一个d×n的矩阵X‘,这个矩阵和我们原来的m×n维样本矩阵X相比，行数从m减到了d，可见对行数进行了压缩。也就是说，左奇异矩阵可以用于行数的压缩。相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降维。　




7.SVD小结　
SVD作为一个很基本的算法，在很多机器学习算法中都有它的身影，特别是在现在的大数据时代，由于SVD可以实现并行化，因此更是大展身手。SVD的原理不难，只要有基本的线性代数知识就可以理解，实现也很简单因此值得仔细的研究。

当然，SVD的缺点是分解出的矩阵解释性往往不强，有点黑盒子的味道，不过这不影响它的使用。


(2) SVD分解的应用

1).降维
通过上面的式子很容易看出，原来矩阵A的特征有n维。而经过SVD分解之后，完全可以用前r个非零奇异值对应的奇异向量表示矩阵A的主要特征。这样，就天然起到了降维的作用。

2).压缩
还是看上面的式子，再结合第三部分的图，也很容易看出，经过SVD分解以后，要表示原来的大矩阵A，我们只需要存U，Σ，V三个较小的矩阵的即可。而这三个较小矩阵的规模，加起来也远远小于原有矩阵A。这样，就天然起到了压缩的作用。






8. 问题
(1)我想问下现在的降维技术我知道的有，奇异值分解，非负矩阵分解，典型关联分析，这三种在使用中哪种更有优势，运行速度更快？
首先，典型关联分析主要是做关联分析的，降维的话不算它最重要的目的。

当特征维度很高的时候，非负矩阵分解一般比奇异值分解快一些，毕竟只有两个矩阵要求。

当特征维度不高的时候，速度差不多，这时推荐奇异值分解，因为非负矩阵分解是基于损失函数得到的近似矩阵。而奇异值分解是完全确定的矩阵。









ref:
1. https://www.cnblogs.com/pinard/p/6251584.html
	https://zhuanlan.zhihu.com/p/31386807
2. https://blog.csdn.net/Jakob_Hu/article/details/91841729




========================================
|-- 矩阵的LU分解 [特征值与特征向量] //todo
----------------------------------------

LU分解的主要用途包括求解矩阵的逆，求解线性方程组等


1. LU分解

(1) 
假定我们能把矩阵A写成下列两个矩阵相乘的形式：A=LU，其中L为下三角矩阵，U为上三角矩阵。

这样我们可以把线性方程组Ax= b写成 Ax= (LU)x = L(Ux) = b。
令Ux = y，则原线性方程组Ax = b可首先求解向量y，使Ly = b，然后求解 Ux = y，从而达到求解线性方程组Ax= b的目的。


LU分解的用处: 分解为 L 和 U 之后，先解 Ly=b 再解 Ux=y 会变得简单许多。
Solving Ax=b becomes LUx=b :
1). find A=LU
2). solve Ly=b
3). solve Ux=y


(2) 高斯消元法: 不允许行交换
回顾我们手工求解这个线性方程组的做法，首先将矩阵 A 行之间进行加减，将 A 矩阵转化为一个上三角矩阵U，然后从下往上将未知数一个一个求解出来, 这就是高斯消元法。
Gaussian Elimination transform a linear system into an upper triangular one by applying linear transformations on the left. It is triangular triangularization.

换个角度看：将 A 做高斯消元得到 U 可以看成是对 A 连续做基础行变换: Lm−1.…L2.L1.A=U

L is unit lower-triangular: L 的对角线上的元素全为1！

实际上，矩阵行变换等价于左乘一个单位矩阵，（因为行变换操作可以直接体现在左乘的单位矩阵上），我们将左乘的单位矩阵最终等价变换后得到的矩阵命名为 M ,显然， M 是一个下三角矩阵，U是一个上三角矩阵。
M.A=U
M和L互为逆矩阵，L=M^-1, 则 A=L.U



意义:https://blog.csdn.net/weixin_30748995/article/details/96204183

LU分解在本质上是高斯消元法的一种表达形式，我们只需要将消元过程中的消元乘数写在相应的位置就可以得到L，使用这种方式可以减少消元的操作步骤，且使得消元思路清晰




(3) 高斯消元法: 允许行交换
由于要确保 A 转化后的第一行的第一项不为 0, 第二行的第二项不为 0, 第三行…… 因此， A 前面应该再加上一个行与行之间进行交换的矩阵 P。
因此，LU 分解的公式又可以写成：PA = LU

P 是 permutation matrix, L 是 lower triangular matrix, U 是 upper triangular matrix. 
有些书将置换矩阵 P 放置在等号右边。
比如这个矩阵在线计算器( https://matrixcalc.org/en/ )默认的就是将 P 放置在矩阵 L 的前面。不妨利用这个工具测试一下 LU 分解的正确性。


R语言中就是A = PLU( 而不是PA = LU ): http://www.cocoachina.com/articles/112034
library(Matrix)
A <- matrix(c(4, 3, -2, 5, 2, -4, 6, 1, -1, 2, -5, 6, 3, 5, -2, -3), nrow = 4);A
B <- matrix(c(16.9, -14, 25, 9.4), nrow = 4);B

luA <- lu(A)
elu <- expand(luA) # 这个就是对A的LU分解， A=P.L.U #其中P是的作用是行交换
(L <- elu$L)
(U <- elu$U)
(P <- elu$P)
#[1,] . . | .
#[2,] . . . |
#[3,] . | . .
#[4,] | . . .

#
# A=P.L.U
# A.x=B, P.L.Ux=B
# 记y=U.x, P.L.y=B, y=(P.L)^-1.B=L^-1 . P^-1 .B
# 然后求解 x=u^-1.y
(Y <- solve(L) %*% solve(P) %*% B)
(X <- solve(U) %*% Y)
#4 x 1 Matrix of class "dgeMatrix"
#      [,1]
#[1,]  4.5
#[2,]  1.6
#[3,] -3.8
#[4,] -2.7

# check if A.x=b;
A %*% X -B








(4)
如果 A 是对称正定矩阵，L 和 U 正好是互为转置，差别在于对应的行存在倍数关系，这种那个倍数关系可以用对角线矩阵来实现，所以， LU 分解就可以写成：A=L.D.L^T

将对角线矩阵均摊到两边，公式可以转化为： A=L.D.L^T=(LD^0.5).(D^0.5.L^T)=U^T.U
其中，矩阵 U 是上三角矩阵，这个就是 Cholesky decomposition，这个分解方法有点像求实数的平方根。同时，该分解方法的计算量只有 LU 分解的一半。










========================================
第七部分 线性空间
----------------------------------------




========================================
|-- 向量空间
----------------------------------------
1. 定义 
设V是非空的n维向量集合，如果集合V对于向量的加法和数乘运算满足以下条件
1) 对于任意的a,b属于V, 有a+b属于V;
2) 对于任意的a属于V，Lambda属于R，有Lambda.a属于V;
则称V为向量空间。

封闭：集合中的任意两个向量经过向量的加法和数乘运算后得到的向量仍是集合中的向量。
所以向量空间的定义也可以表述为: 对向量的加法和数乘运算封闭的非空向量集合。


(2)
齐次线性方程组的解的全体 S={x| Ax=0} 是向量空间。
非齐次线性方程组的解的全体 S_bar={x| Ax=b} 不是向量空间。
证明:因为非齐次不封闭。
对于任意的ita1, ita2 属于S_bar, L属于R，有
A(ita1+ita2)=Aita1+Aita2=b+b=2*b != b, 对加法和数乘不封闭。



例1: 证明等价的向量组生成的向量空间相等。
证明: 设向量组a1,...,am与向量组b1,...,bs等价，且
V1={x=L1.a1+...+Lm.am|L1,...,Lm属于R}
V2={x=mu1.b1+...+mus.bs|mu1,...,mus属于R}
V1和V2是向量空间，证明V1=V2.

设 x属于V1，则x可由a1,...,am线性表示。
因为a1,..,am可由b1,...,bs线性表示，因而x可由b1,..,bs线性表示，所以x属于V2.
也就是说 x属于V1，则x属于V2，因此 V1属于V2。
同理可证 V2属于V1.
所以V1=V2




2. 向量空间的基和维数
(1) 定义: 设V为向量空间，如果
1) 在V中有r个向量a1,...,ar线性无关;
2) V中任意一个向量a可由向量组a1,...,ar线性表示,
则称a1,...,ar为向量空间V的一个 基， 
r称为向量空间V的 维数，
并称V是r维向量空间。


只含有一个零向量的集合{0}也是一个向量空间，这个向量空间没有基，规定它的维数为零，并称之为零维向量空间。


(2)类比概念
向量空间V  <=> 向量组 
向量空间V的基  <=> 最大无关组 
V的维数 <=> 向量组的秩
所以，向量空间V的基不唯一，但是维数是唯一确定的。

设V是r维向量空间，则V中任意r个线性无关的向量都是V的一个基。


(3) 类比方程组
齐次线性方程组 Ax=0，r(A)=r，则自由变量个数为 n-r 个。
基础解系 e1,...,e(n-r)
任意解可以表示为 e=k1.e1+...+k(n-r).e(n-r), k1,...,k(n-r)属于R;

从而基础解系e1,...,e(n-r)就是解空间S={x|Ax=0}的一个基，所以解空间S是n-r维向量空间。
解空间S可以表示为 S={x=k1.e1+...+k(n-r).e(n-r) | k1,...,k(n-r)属于R}


如果r(A)=n时，方程组Ax=0只有0解，因而没有基础解系。此时解空间S只含有一个零向量，为零维向量空间。





3. 基变换公式，坐标变换公式
(1) 某个基下的坐标

(2) 存在可逆矩阵P和Q，p^-1=Q, 使得n维向量空间V的2个基a1,...,an和b1,...,bn能互相线性表示
(b1,...,bn)=(a1,...,an).P (I)
及 
(a1,...,an)=(b1,...,bn).Q (II)
则称以上2个公式为基变换公式。
P称为由基a1,...,an到基b1,...,bn的 过渡矩阵。
Q称为由基b1,...,bn到基a1,...,an的 过渡矩阵。



(3) 设 n维向量空间V的向量a，
在基a1,...,an下的坐标是[x1 ... xn]T, 
在基b1,...,bn下的坐标是[y1 ... yn]T, 
如果这两个基满足关系(I)和(II)，则有
a=(a1, a2, ..., an).[x1 x2 ... xn]T; 
及
a=(b1, b2, ..., bn).[y1 y2 ... yn]T= (a1,...,an).P.[y1 y2 ... yn]T

由于a在基下的表示是唯一的，所以的到 坐标变换公式:
[x1 x2 ... xn]T = P.[y1 y2 ... yn]T
或 
P^-1.[x1 x2 ... xn]T = [y1 y2 ... yn]T = Q.[x1 x2 ... xn]T (III)





========================================
|-- 基、维数与坐标 //todo
----------------------------------------









========================================
|-- 线性空间的同构 //todo
----------------------------------------






========================================
|-- 线性变换 //todo
----------------------------------------







========================================
***************
----------------------------------------



========================================
线性代数与R语言
----------------------------------------
https://blog.csdn.net/hnu_lb/article/details/38419405

1. 矩阵的运算
(1)矩阵的相乘
A %*% B



(2) 求行列式
> A=matrix( c(2,1, 1,
1, 2, 1,
1, 1, 2), nrow=3, byrow = T);A

     [,1] [,2] [,3]
[1,]    2    1    1
[2,]    1    2    1
[3,]    1    1    2

> det(A) #求矩阵的行列式
[1] 4




2.矩阵的逆
> set.seed(1)
> A=matrix(rnorm(9),nrow=3,ncol=3)
> A
           [,1]       [,2]      [,3]
[1,] -0.6264538  1.5952808 0.4874291
[2,]  0.1836433  0.3295078 0.7383247
[3,] -0.8356286 -0.8204684 0.5757814

> solve(A)
            [,1]        [,2]       [,3]
[1,] -0.50015875  0.82896132 -0.6395669
[2,]  0.45439113 -0.02930499 -0.3470881
[3,] -0.07838637  1.16130885  0.3139817

# 验证
> round(A %*% solve(A))
> round( solve(A) %*% A )
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1



solve()函数也可以用来求解方程组ax=b。
> A=matrix( c(1, 1, 3, 12,
            1, 1, -5, -12,
            2, 1, -1, 1), nrow=3, byrow = T);A

     [,1] [,2] [,3] [,4]
[1,]    1    1    3   12
[2,]    1    1   -5  -12
[3,]    2    1   -1    1

> solve(A[,1:3], A[,4])
[1] 1 2 3




3. 矩阵的特征值和特征向量
矩阵A的谱分解如下：A=UΛU’，其中U的列为A的特征值所对应的特征向量，在R中可以用eigen()函数得到U和Λ。
> A=matrix( c(2,1, 1,
1, 2, 1,
1, 1, 2), nrow=3, byrow = T);A

     [,1] [,2] [,3]
[1,]    2    1    1
[2,]    1    2    1
[3,]    1    1    2

> det(A) #求矩阵的行列式
[1] 4

> Aeigen=eigen(A)
> Aeigen
eigen() decomposition
$values # 特征值
[1] 4 1 1

$vectors
           [,1]       [,2]       [,3]
[1,] -0.5773503  0.0000000  0.8164966
[2,] -0.5773503 -0.7071068 -0.4082483
[3,] -0.5773503  0.7071068 -0.4082483


> Aeigen$vectors %*% diag(Aeigen$values) %*% solve(Aeigen$vectors)
     [,1] [,2] [,3]
[1,]    2    1    1
[2,]    1    2    1
[3,]    1    1    2




## 奇异值分解
> A2=svd(A);A2
$d
[1] 4 1 1

$u
           [,1]       [,2]          [,3]
[1,] -0.5773503  0.8164966 -5.041791e-17
[2,] -0.5773503 -0.4082483 -7.071068e-01
[3,] -0.5773503 -0.4082483  7.071068e-01

$v
           [,1]       [,2]       [,3]
[1,] -0.5773503  0.8164966  0.0000000
[2,] -0.5773503 -0.4082483 -0.7071068
[3,] -0.5773503 -0.4082483  0.7071068



> A2$u%*%diag(A2$d)%*%t(A2$v)
     [,1] [,2] [,3]
[1,]    2    1    1
[2,]    1    2    1
[3,]    1    1    2








4. 矩阵如何转为对角矩阵? 
# 矩阵A
A=matrix(c(0,-1,1,-1,0,1,1,1,0), nrow=3)
A
# 特征值和特征向量
eigen(A)

# P^-1.A.P=diag(L1,L2,L3),则P，我算出来的和eigen(A)不很一致:
P=matrix( c( c(-1,-1,1)/sqrt(3), c(1,0,1)/sqrt(2), c(-0.5,1,0.5)/sqrt(0.5**2*2+1) ),nrow=3 )
P
solve(P) %*% A %*% P

# 该转换矩阵本身是正交矩阵，正交矩阵
# 正交矩阵 P^T.P=E，且都是规范化的。
t(P) %*% P
apply(P, 1, function(x){sum(x^2)}) #行向量是单位向量
apply(P, 2, function(x){sum(x^2)}) #列向量也是单位向量

# 正交矩阵的转置和逆相等
solve(P) - t(P)




========================================
左乘旋转矩阵的几何意义：旋转坐标轴
----------------------------------------
1. 2D 坐标的旋转
一个坐标A1(x,y)逆时针旋转alpha角度到A2(x',y'), 满足[x',y']T=A.[x,y]T时，坐标变换矩阵 A=
|cosA, -sinA|
|sinA, cosA|

推导方式:
使用极坐标辅助推导。A和B到原点的距离为r。
假设A1和原点角度为beta，则x=r*cos(b), y=r*sin(a);
则A2的角度为alpha+beta;
x'=r*cos(a+b)=r*(cos(a)*cos(b)-sin(a)*sin(b))=x*cos(a)-y*sin(a);
y'=r*sin(a+b)=r*(sin(a)*cos(b)+cos(a)*sin(b))=x*sin(a)+y*cos(a);
写成矩阵形式，[x',y']T=A.[x,y]T，可知矩阵A 如上。


只要给出旋转角度，计算出A矩阵，然后使用A矩阵分别左乘每一个点，就能计算出这个点旋转后的点坐标 这样我们就可以通过矩阵变换坐标了。
而旋转坐标系alpha度，相当于每个点旋转-alpha度。


查看A矩阵，如果alpha=90度，则A的形式为
|0, -1|
|1, 0|




(1) 而交换坐标轴和上文并不一样，不能简单的说是旋转角度，需要区别对待。
[y,x]T=A.[x,y]T 时，矩阵A=
|0 1|
|1 0|




2. 3D坐标的旋转






ref:
坐标轴的旋转矩阵: https://blog.csdn.net/TOM_00001/article/details/62054572







========================================
矩阵求导
----------------------------------------
见 ML in action, P308 附录B，有半页。

矩阵求导（Matrix Derivative）也称作矩阵微分（Matrix Differential），在机器学习、图像处理、最优化等领域的公式推导中经常用到。


布局约定（Layout conventions）
布局（Layout）：在矩阵求导中有两种布局，分别为分母布局(denominator layout)和分子布局(numerator layout)。这两种不同布局的求导规则是不一样的。

通过观察，发现分子布局和分母布局刚好是转置关系。
分子布局下与原来的分子相同，而分母布局下差一个转置。
本文采用分子布局。



1. 对向量和矩阵的求导，并不比常规的求导更难，只是要清楚这里的概念和定义。

向量函数(也就是函数组成的向量) y=[y1 y2 ... yn]T, 关于向量 x=[x1 x2 ... xn]T 的导数，记作 dy/dx=
[dy1/dx1 dy2/dx2 ... dyn/dxn]
...
[dyn/dx1 dyn/dx2 ... dyn/dxn]
这个矩阵叫做 Jacobian 矩阵。



例: 列向量 A= [ sin(x)-y,  sin(3*x)-4*y]T，
(1) A对标量x求导，得到另一个向量 [cos(x), 3*cos(3*x)]T 
(2) 如果A要对另一个向量求导，会得到一个矩阵，比如B=[x y z]T
dA/dB=
[cosx 3*cos(3x)]
[-1 -4]
[0 0]





2. 矩阵关于向量 x 的导数
很复杂，没看懂

一个 m × n 的矩阵 Y 对 一个 p × 1 的向量x求导，结果应该是一个 m × n × p 的矩阵.




//todo
维度分析




ref:
https://www.jianshu.com/p/6b64b7ee6ec2
https://blog.csdn.net/xidianliutingting/article/details/51673207





========================================
----------------------------------------







========================================
----------------------------------------







========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------
