webSpider


========================================
python爬虫大纲与教程
----------------------------------------
1.爬虫主要由以下几个功能部分组成
(1).访问网址，获取数据: 使用requests包、
(2).解析数据(难点): 正则re、xpath等
(3).存储数据: 文件存储、数据库存储



难点：
多线程抓取
保存到数据库
XPath解析数据 https://blog.csdn.net/u013332124/article/details/80621638
	lxml.etree 
	lxml.html
高级解析器： BeautifulSoup
cookie模拟登陆


requests 比 urllib 人性化。
使用 selenium  吧 ，好用多了。




2.工具
(1)jupyter notebook
(2)Colab: https://colab.research.google.com
https://www.jianshu.com/p/81eae79ee78b


爬虫框架更好用：




3.爬虫教程
32个Python爬虫项目
https://www.jianshu.com/p/39d4b15c05ee

http://www.runoob.com/w3cnote/python-spider-intro.html

Python小爬虫——贴吧图片的爬取
https://www.cnblogs.com/Axi8/p/5757270.html

Python爬虫原理-简单版、复杂版、并发版
https://www.cnblogs.com/sss4/p/7809821.html



4.XPath精要
https://www.cnblogs.com/giserliu/p/4399778.html
附录：基本XPATH语法介绍，详细请参考XPath的官方文档

XPATH基本上是用一种类似目录树的方法来描述在XML文档中的路径。比如用“/”来作为上下层级间的分隔。第一个“/”表示文档的根节点（注意，不是指文档最外层的tag节点，而是指文档本身）。比如对于一个HTML文件来说，最外层的节点应该是”/html”。

同样的，“..”和“.”分别被用来表示父节点和本节点。

XPATH返回的不一定就是唯一的节点，而是符合条件的所有节点。比如在HTML文档里使用“/html/head/scrpt”就会把head里的所有script节点都取出来。

为了缩小定位范围，往往还需要增加过滤条件。过滤的方法就是用[]把过滤条件加上。比如在HTML文档里使用“/html/body/div[@id='main']”，即可取出body里id为main的div节点。

其中@id表示属性id，类似的还可以使用如@name, @value, @href, @src, @class….处理属性判断。

而 函数text()的意思则是取得节点包含的文本。比如：<div>hello<p>world</p>< /div>中，用"div[text()='hello']"即可取得这个div，而world则是p的text()。

函数position()的意思是取得节点的位置。比如"li[position()=2]"表示取得第二个li节点，它也可以被省略为"li[2]"。

不过要注意的是数字定位和过滤 条件的顺序。比如"ul/li[5][@name='hello']"表示取ul下第五项li，并且其name必须是hello，否则返回空。而如果用 "ul/li[@name='hello'][5]"的意思就不同，它表示寻找ul下第五个name为"hello"的li节点。

此外，*可以代替所有的节点名，比如用"/html/body/*/span"可以取出body下第二级的所有span，而不管它上一级是div还是p或是其它什么东东。

而 "descendant::"前缀可以指代任意多层的中间节点，它也可以被省略成一个“/”。比如在整个HTML文档中查找id为“leftmenu”的 div，可以用"/descendant::div[@id='leftmenu']"，也可以简单地使用"//div[@id='leftmenu']"。

至于"following-sibling::"前缀就如其名所说，表示同一层的下一个节点。"following-sibling::*"就是任意下一个节点，而"following-sibling::ul"就是下一个ul节点。








========================================
实例: 下载视频的简单爬虫：[requests,re,file]
----------------------------------------
#下载视频

import re
import requests

#发出请求
respose=requests.get('http://www.xiaohuar.com/v/')
#print(respose.status_code)# 响应的状态码 200
#print(respose.content)  #返回字节信息
#print(respose.text)  #返回文本内容

#正则解析内容
urls=re.findall(r'class="items".*?href="(.*?)"',respose.text,re.S)  #re.S 把文本信息转换成1行匹配
#print(urls)
url=urls[5]

#发送请求2
result=requests.get(url)
#解析出视频链接
mp4_url=re.findall(r'id="media".*?src="(.*?)"',result.text,re.S)[0]
print('mp4_url:',mp4_url)

#发送请求3
video=requests.get(mp4_url)
#写入二进制文件
with open('a.mp4','wb') as f:
    f.write(video.content)
 
print("end")





========================================
实例: 解析html元素 [lxml.etree,xpath,print]
----------------------------------------
from lxml import etree

html = '<table id="table1" cellspacing="0px"> <tr><th>编号</th><th>姓名</th><th>年龄</th></tr><tr><td>1</td><td>张三</td><td>11</td></tr><tr><td>2</td><td>李四</td><td>12</td></tr><tr><td>3</td><td>王五</td><td>13</td></tr><tr><td>4</td><td>马六</td><td>14</td></tr></table>'
content2 = etree.HTML(html)

#第一次解析
rows = content2.xpath('//table[@id="table1"]/tr')[1:]
for row in rows:
	#第二次解析
    id = row.xpath('./td[1]/text()')[0]
    name = row.xpath('./td[2]/text()')[0]
    age = row.xpath('./td[3]/text()')[0]
    print(id, name, age)

#1 张三 11
#2 李四 12
#3 王五 13
#4 马六 14







========================================
实例: 下载微信文章的简单爬虫: [requests,re,file]
----------------------------------------
#下载微信文章

import re
import requests

#发出请求
respose=requests.get('https://mp.weixin.qq.com/s?__biz=MzI0NDcxNzc5Mg==&mid=2247485084&idx=1&sn=70239065aaa1f59610dab9894df7f745')
assert respose.status_code==200,'Error: requests.get'

#print(respose.text)

title=re.findall(r'msg_title = "(.*?)"',respose.text,re.S)[0]
print('title=',title)

msg_desc=re.findall(r'msg_desc = "(.*?)"',respose.text,re.S)[0]
print('msg_desc=',msg_desc)

rs=re.findall(r'id="js_content"\>(.*?)\<\/div\>',respose.text,re.S)[0]
#print('rs=', rs)

#写入文件
with open('/home/wangjl/web/en.html','w') as f:
    f.write('<meta http-equiv="Content-Type" content="text/html; charset=utf-8">')
    f.write(rs)

print("end===")






========================================
实例: 获取自己的外网IP: [requests, re, print]
----------------------------------------
import requests
import re

def get_ip_by_ip138():
    response = requests.get("http://2018.ip138.com/ic.asp")
    ip = re.search(r"\[\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\]",response.content.decode(errors='ignore')).group(0)
    return ip

print("本机的ip地址为:",get_ip_by_ip138())

# 本机的ip地址为: [116.7.234.239]





========================================
【难点】 网页内容解析: XPath
----------------------------------------
对 XPath 的理解是很多高级 XML 应用的基础。


1.使用场景
在运行豆瓣爬虫时，由于页面太复杂，用正则解析太繁琐，而使用xpath则很简洁。

#豆瓣电影
import requests
from lxml import html
url='https://movie.douban.com/' #需要爬数据的网址
page=requests.Session().get(url)

tree=html.fromstring(page.text)
#print(page.text)

# xpath:  https://www.cnblogs.com/giserliu/p/4399778.html
#result=tree.xpath('//td[@class="title"]//a/text()') #获取需要的数据[失败，可能是豆瓣改版了]
result=tree.xpath('//ul/li[@class="ui-slide-item"]') #该怎么获取电影名字？

#输出结果
print('len=',len(result))
print(result[0])

#end
print("==end==")





2.学习 xpath 
XPath 就是一个用来查找xml节点的路径语言，一个路径字符串语法
XPath和re的关系，其实跟jQuery和原生js的关系差不多的，前者更简洁。

http://www.w3school.com.cn/xpath/index.asp

什么是 XPath?
	XPath 使用路径表达式在 XML 文档中进行导航
	XPath 包含一个标准函数库
	XPath 是 XSLT 中的主要元素
	XPath 是一个 W3C 标准





(1)Node节点：
在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档节点（或称为根节点）。

请看下面这个 XML 文档：
<?xml version="1.0" encoding="ISO-8859-1"?>
<bookstore>

<book>
  <title lang="en">Harry Potter</title>
  <author>J K. Rowling</author> 
  <year>2005</year>
  <price>29.99</price>
</book>

</bookstore>

上面的XML文档中的节点例子：
	<bookstore> （文档节点）
	<author>J K. Rowling</author> （元素节点）
	lang="en" （属性节点） 


基本值（或称原子值，Atomic value）是无父或无子的节点。
基本值的例子：
J K. Rowling
"en"

项目（Item）是基本值或者节点。

同胞（Sibling）,拥有相同的父的节点

先辈（Ancestor）,某节点的父、父的父，等等。
	上例中，title 元素的先辈是 book 元素和 bookstore 元素。

后代（Descendant），某个节点的子，子的子，等等。
	上例中，bookstore 的后代是 book、title、author、year 以及 price 元素：






(2)XPath 语法
XPath 使用路径表达式来选取 XML 文档中的节点或节点集。节点是通过沿着路径 (path) 或者步 (steps) 来选取的。

$ cat bookstore.xml
<?xml version="1.0" encoding="ISO-8859-1"?>

<bookstore>

<book>
  <title lang="eng">Harry Potter</title>
  <price>29.99</price>
</book>

<book>
  <title lang="eng">Learning XML</title>
  <price>39.95</price>
</book>

</bookstore>


Python解析xml的框架 XML_Demo.py
import requests
from lxml import html

#1.请求数据
url='http://y.biomooc.com/wangjl/bookstore.xml' #需要爬数据的网址
page=requests.Session().get(url)

#2.解析数据，与匹配
# https://blog.csdn.net/liaoningxinmin/article/details/80377252
tree=html.fromstring(page.content)
#################
#XPath start
#################
tree.xpath('//@lang') #['eng', 'eng'] 

#后面就模仿最后一句话，开始写XPath





下面列出了最有用的路径表达式：
表达式	描述
nodename	选取此节点的所有子节点。
/	从根节点选取。
//	从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。
.	选取当前节点。
..	选取当前节点的父节点。
@	选取属性。





谓语（Predicates）
谓语用来查找某个特定的节点或者包含某个指定的值的节点。

谓语被嵌在方括号中。
实例
在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：

路径表达式	结果
/bookstore/book[1]	选取属于 bookstore 子元素的第一个 book 元素。
/bookstore/book[last()]	选取属于 bookstore 子元素的最后一个 book 元素。
/bookstore/book[last()-1]	选取属于 bookstore 子元素的倒数第二个 book 元素。
/bookstore/book[position()<3]	选取最前面的两个属于 bookstore 元素的子元素的 book 元素。
//title[@lang]	选取所有拥有名为 lang 的属性的 title 元素。
//title[@lang='eng']	选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。


在python中还是要用//开头的，注意单双引号的交替。
>>tree.xpath('//title[@lang="eng"]') #2个元素

选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。
>>tree.xpath('//bookstore/book[price>35]') #1个元素

选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。
>>tree.xpath('//bookstore/book[price>35]/title')





选取未知节点
XPath 通配符可用来选取未知的 XML 元素。

通配符	描述
*	匹配任何元素节点。
@*	匹配任何属性节点。
node()	匹配任何类型的节点。


路径表达式	结果
/bookstore/*	选取 bookstore 元素的所有子元素。
//*	选取文档中的所有元素。
//title[@*]	选取所有带有属性的 title 元素。





选取若干路径
通过在路径表达式中使用“|”运算符，您可以选取若干个路径。相当于 或 。

实例
在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

路径表达式	结果
//book/title | //book/price	选取 book 元素的所有 title 和 price 元素。
//title | //price	选取文档中的所有 title 和 price 元素。
/bookstore/book/title | //price	选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。






(3)XPath 轴
轴可定义相对于当前节点的节点集。

轴名称	结果
ancestor	选取当前节点的所有先辈（父、祖父等）。
ancestor-or-self	选取当前节点的所有先辈（父、祖父等）以及当前节点本身。
attribute	选取当前节点的所有属性。
child	选取当前节点的所有子元素。
descendant	选取当前节点的所有后代元素（子、孙等）。
descendant-or-self	选取当前节点的所有后代元素（子、孙等）以及当前节点本身。
following	选取文档中当前节点的结束标签之后的所有节点。
namespace	选取当前节点的所有命名空间节点。
parent	选取当前节点的父节点。
preceding	选取文档中当前节点的开始标签之前的所有节点。
preceding-sibling	选取当前节点之前的所有同级节点。
self	选取当前节点。





路径表达式是从一个XML节点（当前的上下文节点）到另一个节点、或一组节点的书面步骤顺序。
这些步骤以“/”字符分开，每一步有三个构成成分：  /step1/step2/step3/
 - 轴描述（用最直接的方式接近目标节点）
 - 节点测试（用于筛选节点位置和名称）
 - 节点描述（用于筛选节点的属性和子节点特征）
一般情况下，我们使用简写后的语法。虽然完整的轴描述是一种更加贴近人类语言，利用自然语言的单词和语法来书写的描述方式，但是相比之下也更加啰嗦。


[换个描述再说一遍] 步（step）包括：
 - 轴（axis）: 定义所选节点与当前节点之间的树关系
 - 节点测试（node-test）:识别某个轴内部的节点
 - 零个或者更多谓语（predicate）:更深入地提炼所选的节点集
步的语法：
	轴名称::节点测试[谓语]


实例
例子	结果
child::book	选取所有属于当前节点的子元素的 book 节点。
attribute::lang	选取当前节点的 lang 属性。
child::*	选取当前节点的所有子元素。
attribute::*	选取当前节点的所有属性。
child::text()	选取当前节点的所有文本子节点。
child::node()	选取当前节点的所有子节点。
descendant::book	选取当前节点的所有 book 后代。
ancestor::book	选择当前节点的所有 book 先辈。
ancestor-or-self::book	选取当前节点的所有 book 先辈以及当前节点（如果此节点是 book 节点）
child::*/child::price	选取当前节点的所有 price 孙节点。

没搞懂，什么意思？







(4)XPath 运算符
XPath 表达式可返回节点集、字符串、逻辑值以及数字。
http://www.w3school.com.cn/xpath/xpath_operators.asp

运算符	描述	实例	返回值
|	计算两个节点集	//book | //cd	返回所有拥有 book 和 cd 元素的节点集
+	加法	6 + 4	10
-	减法	6 - 4	2
*	乘法	6 * 4	24
div	除法	8 div 4	2
=	等于	price=9.80	如果 price 是 9.80，则返回 true。否则返回 false。







(5)XPath 标准函数
XPath 含有超过 100 个内建的函数。这些函数用于字符串值、数值，日期和时间比较、节点和 QName 处理、序列处理、逻辑值等等。
http://www.w3school.com.cn/xpath/xpath_functions.asp

. starts-with函数
获取以xxx开头的元素 
例子：xpath('//div[starts-with(@class,"test")]')

contains函数
获取包含xxx的元素 
例子：xpath('//div[contains(@id,"test")]')

and
与的关系 
例子：xpath('//div[contains(@id,"test") and contains(@id,"title")]')

text()函数
例子1：xpath('//div[contains(text(),"test")]') 
例子2：xpath('//div[@id="test"]/text()')









3.实例学习 XPath
(0)抓取原文 test.html

<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
    <title>xpath test</title>
</head>
<body>
<div price="99.8">
    <div>
        <ul>
            <li>时间</li>
            <li>地点</li>
            <li>任务</li>
        </ul>
    </div>
    <div id='testid' data-h="first">
        <h2>这里是个小标题</h2>
        <ol>
            <li data="one">1</li>
            <li data="two">2</li>
            <li data="three">3</li>
        </ol>
        <ul>
            <li code="84">84</li>
            <li code="104">104</li>
            <li code="223">223</li>
        </ul>
    </div>
    <div>
        <h3>这里是H3的内容
            <a href="http://www.baidu.com">百度一下</a>
            <ul>
                <li>test1</li>
                <li>test2</li>
            </ul>
        </h3>
    </div>
    <div id="go">
        <ul>
            <li>1</li>
            <li>2</li>
            <li>3</li>
            <li>4</li>
            <li>5</li>
            <li>6</li>
            <li>7</li>
            <li>8</li>
            <li>9</li>
            <li>10</li>
        </ul>
    </div>
</div>
</body>
</html>


(1)爬虫代码主体
version1.1
import requests
from lxml import html

#1.请求数据
url='http://y.biomooc.com/wangjl/test.html' #需要爬数据的网址
page=requests.Session().get(url)

#2.解析数据
page2=page.text
page3=page2.encode('ISO-8859-1').decode(page.apparent_encoding)
#print(page3) #不乱码了

#3.匹配
tree=html.fromstring(page3)
#################
#XPath start
#################




version1.2
import requests
from lxml import html

#1.请求数据
url='http://y.biomooc.com/wangjl/test.html' #需要爬数据的网址
page=requests.Session().get(url)

#2.解析数据，与匹配
tree=html.fromstring(page.content)
#################
#XPath start
#################
tree.xpath('//@code') #['84', '104', '223']




(2)xpath抓取实例
1)# 匹配包含某属性的所有的属性值//@lang
>>tree.xpath('//@code') #['84', '104', '223']


2)使用|或，设置多个条件
>>tree.xpath('//div[@id="testid"]/h2/text() | //li[@data]/text()') #多个匹配条件
#['这里是个小标题', '1', '2', '3']



3)轴Axes: child 选取当前节点的所有子元素
>>tree.xpath('//div[@id="testid"]/child::ul/li/text()') #child子节点定位
# ['84', '104', '223']

>>tree.xpath('//div[@id="testid"]/child::*') #child::*当前节点的所有子元素
#[<Element h2 at 0x7f1014644958>, <Element ol at 0x7f1014644f48>, <Element ul at 0x7f101464c1d8>]

#定位某节点下为ol的子节点下的所有节点
>>tree.xpath('//div[@id="testid"]/child::ol/child::*/text()') 
#['1', '2', '3']



轴Axes: attribute 选取当前节点的所有属性
>>tree.xpath('//div/attribute::id') #attribute定位id属性值
# ['testid', 'go']

>>tree.xpath('//div[@id="testid"]/attribute::*') #定位当前节点的所有属性
#['testid', 'first']



轴Axes: ancestor 父辈元素 / ancestor-or-self：父辈元素及当前元素
>>tree.xpath('//div[@id="testid"]/ancestor::div/@price') #定位父辈div元素的price属性
#['99.8']

>>tree.xpath('//div[@id="testid"]/ancestor::div') #所有父辈div元素
#[<Element div at 0x7f101464c278>]
>>tree.xpath('//div[@id="testid"]/ancestor-or-self::div') #所有父辈及当前节点div元素
#[<Element div at 0x7f101464c278>, <Element div at 0x7f10146cc5e8>]



轴Axes: descendant 后代 / descendant-or-self：后代及当前节点本身
同上。



轴Axes: following 选取文档中当前节点的结束标签之后的所有节点
#定位testid之后不包含id属性的div标签下所有的li中第一个li的text属性
>>tree.xpath('//div[@id="testid"]/following::div[not(@id)]/.//li[1]/text()')
#['test1']


namespace：选取当前节点的所有命名空间节点
>>tree.xpath('//div[@id="testid"]/namespace::*') #选取命名空间节点
#[('xml', 'http://www.w3.org/XML/1998/namespace')]


parent：选取当前节点的父节点
#选取data值为one的父节点的子节点中最后一个节点的值
>>tree.xpath('//li[@data="one"]/parent::ol/li[last()]/text()') 
#['3']
#注意这里的用法，parent::父节点的名字，也就是从父节点、父父节点等中找ol。




preceding：选取文档中当前节点的开始标签之前的所有节点
>>tree.xpath('//div[@id="testid"]/preceding::div/ul/li[1]/text()')[0] 
#'时间'

##下面这两条可以看到其顺序是靠近testid节点的优先
>>tree.xpath('//div[@id="testid"]/preceding::li[1]/text()')[0]
#'任务'
>>tree.xpath('//div[@id="testid"]/preceding::li[3]/text()')[0]
#'时间'




preceding-sibling：选取当前节点之前的所有同级节点
#记住只能是同级节点
>>tree.xpath('//div[@id="testid"]/preceding-sibling::div/ul/li[2]/text()')[0]
#'地点'
>>tree.xpath('//div[@id="testid"]/preceding-sibling::li') #这里返回的就是空的了
#[] 因为没有同级别的li




self：选取当前节点
#选取带id属性值的div中包含data-h属性的标签的所有属性值
>>tree.xpath('//div[@id]/self::div[@data-h]/attribute::*') 
#['testid', 'first']



组合拳
#定位id值为testid下的ol下的li属性值data为two的父元素ol的兄弟前节点h2的text值
>>tree.xpath('//*[@id="testid"]/ol/li[@data="two"]/parent::ol/preceding-sibling::h2/text()')[0] 
#这里是个小标题





4)position定位
>>tree.xpath('//*[@id="testid"]/ol/li[position()=2]/text()')[0] 
#'2'



5)条件
#定位所有h2标签中text值为`这里是个小标题`
>>tree.xpath(u'//h2[text()="这里是个小标题"]/text()')[0]
#这里是个小标题




6)函数
count：统计
>>tree.xpath('count(//li[@data])') #节点统计
#3.0


concat：字符串连接
>>tree.xpath('concat(//li[@data="one"]/text(),//li[@data="three"]/text())')
#13


string：解析当前节点下的字符
#string只能解析匹配到的第一个节点下的值，也就是作用于list时只匹配第一个
>>tree.xpath('string(//li)') 
#时间


local-name：解析节点名称
>>tree.xpath('local-name(//*[@id="testid"])') #local-name解析节点名称
#div



contains(string1,string2)：如果 string1 包含 string2，则返回 true，否则返回 false
>>tree.xpath('//h3[contains(text(),"H3")]/a/text()')[0] #使用字符内容来辅助定位
#百度一下

##一记组合拳
#匹配带有href属性的a标签的先辈节点中的div，其兄弟节点中前一个div节点下ul下li中text属性包含“务”字的节点的值
>>tree.xpath(u'//a[@href]/ancestor::div/preceding::div/ul/li[contains(text(),"务")]/text()')[0] 
#任务




not：布尔值（否）
>>tree.xpath('count(//li[not(@data)])') #不包含data属性的li标签统计
#18.0


string-length：返回指定字符串的长度
#string-length函数+local-name函数定位节点名长度小于2的元素
>>tree.xpath('//*[string-length(local-name())<2]/text()')[0] 
#百度一下




组合拳2
#contains函数+local-name函数定位节点名包含di的元素
>>tree.xpath('//div[@id="testid"]/following::div[contains(local-name(),"di")]') 
#[<Element div at 0x7f101464cbd8>, <Element div at 0x7f101464c688>]



or：多条件匹配
>>tree.xpath('//li[@data="one" or @code="84"]/text()') #or匹配多个条件
#['1', '84']
#也可使用|
>>tree.xpath('//li[@data="one"]/text() | //li[@code="84"]/text()') #|匹配多个条件
#['1', '84']


组合拳3：floor + div除法 + ceiling
#position定位+last+div除法，选取中间两个
>>tree.xpath('//div[@id="go"]/ul/li[position()=floor(last() div 2+0.5) or position()=ceiling(last() div 2+0.5)]/text()') 
#['5', '6']
######//todo 没看懂？？


组合拳4隔行定位：position+mod取余
#position+取余运算隔行定位
>>tree.xpath('//div[@id="go"]/ul/li[position()=((position() mod 2)=0)]/text()') 
#['2', '4', '6', '8', '10']


starts-with：以。。开始
#starts-with定位属性值以8开头的li元素
>>tree.xpath('//li[starts-with(@code,"8")]/text()')[0]
#'84'






7)数值比较
<：小于
#所有li的code属性小于200的节点
>>tree.xpath('//li[@code<200]/text()')
#['84', '104']


div：对某两个节点的属性值做除法
>>tree.xpath('//div[@id="testid"]/ul/li[3]/@code div //div[@id="testid"]/ul/li[1]/@code')
#2.6547619047619047


组合拳4：根据节点下的某一节点数量定位
#选取所有ul下li节点数大于5的ul节点
>>tree.xpath('//ul[count(li)>5]/li/text()')
# ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']



8)将对象还原为字符串
s = tree.xpath('//*[@id="testid"]')[0] #使用xpath定位一个节点
#<Element div at 0x7f10146cc5e8>

from lxml import etree
etree.tostring(s)  #还原这个对象为html字符串
#'<div id="testid">\n\t\t<h2>&#213;&#226;&#192;&#239;&#202;&#199;&#184;&#246;&#208;&#161;&#177;&#234;&#204;&#226;</h2>\n\t\t<ol>\n\t\t\t<li data="one">1</li>\n\t\t\t<li data="two">2</li>\n\t\t\t<li data="three">3</li>\n\t\t</ol>\n\t\t<ul>\n\t\t\t<li code="84">84</li>\n\t\t\t<li code="104">104</li>\n\t\t\t<li code="223">223</li>\n\t\t</ul>\n\t</div>\n\t'



9)选取一个属性中的多个值
举例：<div class="mp-city-list-container mp-privince-city" mp-role="provinceCityList">
选择这个div的方案网上有说用and的，但是似乎只能针对不同的属性的单个值
本次使用contains

from lxml import etree
html = '<div class="mp-city-list-container mp-privince-city" mp-role="provinceCityList"></div>'
content2 = etree.HTML(html)

#解析
rs=content2.xpath('//div[contains(@class,"mp-city-list-container mp-privince-city")]')
etree.tostring(rs[0])

#当然也可以直接选取其属性的第二个值
content2.xpath('//div[contains(@class,"mp-privince-city")]')
#重点是class需要添加一个@符号






















========================================
爬虫实例：获取豆瓣电影 正在热映电影 名字和时长[requests, XPath, print]
----------------------------------------
import requests
from lxml import html


#1.请求数据
url='https://movie.douban.com/' #需要爬数据的网址
page=requests.Session().get(url)


#2.解析数据
tree=html.fromstring(page.text)
#print(page.text) #网页内容

#该怎么获取电影名字？
# xpath教程:  https://www.cnblogs.com/giserliu/p/4399778.html
#result=tree.xpath('//td[@class="title"]//a/text()') #获取需要的数据[失败，可能是豆瓣改版了]
#result=tree.xpath('//ul/li[@class="ui-slide-item"]/attribute::data-title') #hit25 class="a",but no class="a b"
result=tree.xpath('//ul/li[contains(@class,"ui-slide-item")]/attribute::data-title')#hit 32 items 

duration=tree.xpath('//ul/li[contains(@class,"ui-slide-item")]/attribute::data-duration')#时长



#3.输出结果
print('正在热映电影 ',len(result),'部：')
for i in range(len(result)):
    print(i+1, result[i],duration[i],sep=" / ")

#4. end
print("==end==")



#输出结果
#正在热映电影  32 部：
#1 / 宝贝儿 / 96分钟
#2 / 无双 無雙 / 130分钟
#...


========================================
【大坑】Python解决抓取内容乱码问题（decode和encode解码）
----------------------------------------

1.字符串在Python内部的表示是unicode编码，在做编码转换时，通常需要以unicode作为中间编码，即先将其他编码的字符串解码（decode）成unicode，再从unicode编码（encode）成另一种编码。

decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(‘gb2312’)，表示将gb2312编码的字符串str1转换成unicode编码。
encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘utf-8’)，表示将unicode编码的字符串str2转换成utf-8编码。

decode中写的就是想抓取的网页的编码，encode即自己想设置的编码



Python3的 默认编码 为Unicode，测试如下：
# Python3
import sys
sys.getdefaultencoding() #'utf-8'



怎么知道原始网页用的什么编码方式呢？
(1)查看网页源代码，找到：
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
(2)F12检查元素，查看Response Headers中的：
	Content-Type: text/plain;charset=UTF-8
(3)代码中直接查询，见本文 实例
	print(2,req.encoding) #ISO-8859-1 #response内容的编码






2.比如网页源文件中发现是 gb2312 编码的。

2.1 用 urllib 库
#!/usr/bin/env python
# -*- coding:utf-8 -*-
import urllib

page=urllib.request.urlopen(r'http://nhxy.zjxu.edu.cn/')
res=page.read()

#write to file/screen
#print(res.decode("utf-8")) 
print(res.decode("gb2312"))#查网页源代码知道的编码




2.2 用 requests 库
import requests
from lxml import html

#1.请求数据
url='http://nhxy.zjxu.edu.cn/' #需要爬数据的网址
page=requests.Session().get(url)
print(req.encoding) #ISO-8859-1
print(page.apparent_encoding) #'GB2312'

#2.解析数据
page2=page.text
page3=page2.encode('ISO-8859-1').decode(page.apparent_encoding)
print(page3) #不乱码了







3.乱码实例（文件是vim保存到主机上的utf8编码的）
#爬虫，应对乱码
import requests
from lxml import html

#1.请求数据
url='http://y.biomooc.com/wangjl/test.html' #需要爬数据的网址
page=requests.Session().get(url)

############### 解决编码问题
#req=page
#print(1,req.headers['content-type']) #text/html
#print(2,req.encoding) #ISO-8859-1 #response内容的编码
#print(3,req.apparent_encoding) #utf-8 #response headers里设置的编码
#print(4,requests.utils.get_encodings_from_content(req.text)) #['utf-8'] #response返回的html header标签里设置的编码
#返回的内容是采用‘ISO-8859-1’，所以出现了乱码，而实际上我们应该采用‘utf-8’编码
#总结：当response编码是‘ISO-8859-1’，我们应该首先查找response header设置的编码；
#text=req.text
# 转换编码,否则会导致输出乱码
#text2 = text.encode('ISO-8859-1').decode(req.apparent_encoding)
#print(text2)
################
#print(page.text)

#2.解析数据
page2=page.text
page3=page2.encode('ISO-8859-1').decode(page.apparent_encoding)
print(page3) #不乱码了

#可以匹配了
#tree=html.fromstring(page3)
#rs=tree.xpath('//html')

print("==End==")




原文：https://blog.csdn.net/w_linux/article/details/78370218 


========================================
实例: python抓取百度贴吧的图片并保存(遇到过乱码) [urllib.request, re, urllib.request.urlretrieve]
----------------------------------------
https://www.cnblogs.com/to-creat/p/6438288.html

1.简易版
import urllib.request

def getHtml(url):
    page=urllib.request.urlopen(url)
    html=page.read()
    return html


html=getHtml('http://y.biomooc.com/wangjl/test.html')
print(html.decode("utf-8")) #为什么？
#getHtml('https://www.baidu.com').decode("utf-8")

#如果"utf-8"报错：UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc8 in position 0
#则改为"gb18030"
print(html.decode("gb18030"))



其中：
Urllib 模块提供了读取web页面数据的接口，我们可以像读取本地文件一样读取www和ftp上的数据。首先，我们定义了一个getHtml()函数:
urllib.urlopen()方法用于打开一个URL地址。
read()方法用于读取URL上的数据，向getHtml()函数传递一个网址，并把整个页面下载下来。执行程序就会把整个网页打印输出。





2.下载图片到本地版本
#coding=utf-8
import urllib
import re

#fun:获取html代码
def getHtml(url):
    page = urllib.request.urlopen(url)
    html = page.read()
    return html

#fun:用正则解析出图片url
def getImg(html):
    reg = r'src="(.+?\.jpg)" pic_ext' #匹配图片的文件名
    #reg = r'src="(.+?\.jpg)" pic_ext="(.+?)"' #"(.*?)"
    #imgre = re.compile(reg)
    return re.findall(reg,html)

#1.获取html
html = getHtml("http://tieba.baidu.com/p/2460150866")
#print(html.decode('utf-8'))

#2.解析出图片url
#python3这句代码 .decode('utf-8') ，解决报错问题 TypeError: cannot use a string pattern on a bytes-like object
imglist=getImg(html.decode('utf-8'))
print(len(imglist), imglist[0])
#print(imglist)

#3.保存图片到文件夹中
i=0
for img in imglist:
    #这里的核心是用到了urllib.urlretrieve()方法，直接将远程数据下载到本地。
    urllib.request.urlretrieve(imglist[i],'/home/wangjl/web/tmp/baidu_'+str(i)+'.jpg') #request.urlretrieve(jpg_link, path) 
    i+=1
    if i%10==0: #进度条
        print(i, 'done.')

print('==end==')




========================================
实例：10行代码爬取全国所有A股/港股/新三板上市公司信息
----------------------------------------
https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&mid=2247487728&idx=1&sn=f019272798059f475c98990f62d9e562

本文知识点：  
	Table型表格抓取
	DataFrame.read_html函数使用
	MySQL数据库存储
	Navicat数据库的使用


1. table格式的表格，可以利用pandas模块里的read_html函数方便快捷地抓取下来。
$ conda install pandas #jupyter中引入失败
$ pip install pandas -i http://pypi.douban.com/simple --trusted-host pypi.douban.com #重新安装
## https://blog.csdn.net/u012421976/article/details/78329843
这些源安装超级慢，所以我推荐大家使用豆瓣的源

pip install matplotlib -i http://pypi.douban.com/simple --trusted-host pypi.douban.com
pip install numpy -i http://pypi.douban.com/simple --trusted-host pypi.douban.com
pip install pandas -i http://pypi.douban.com/simple --trusted-host pypi.douban.com
pip install seaborn scipy  -i http://pypi.douban.com/simple --trusted-host pypi.douban.com



2.getStockInfo.py
import pandas as pd
import csv

for i in range(1,178):  # 爬取全部177页数据
    url = 'http://s.askci.com/stock/a/?reportTime=2017-12-31&pageNum=%s' % (str(i))
    tb = pd.read_html(url)[3] #经观察发现所需表格是网页中第4个表格，故为[3]
    tb.to_csv(r'1.csv', mode='a', encoding='utf_8_sig', header=1, index=0)
    print('第'+str(i)+'页抓取完成')


不到1min结束。比采用正则表达式、xpath这类常规方法要省心省力地多。
比人工复制粘贴也快的多。



========================================
【难点】多线程、多线程之间的通信
----------------------------------------
1.队列 Queue 能实现子进程之间的通信



2.主进程和子进程之间怎么通信？



========================================
实例：多线程下载ZINC小分子数据库(防止直接用wget并发太大被屏蔽)[requests,shell,file]
----------------------------------------

#1.下载csv文件：
#http://zinc15.docking.org/catalogs/fda/substances/
# (1) 关键词搜索 fda/specsnp/molportnp等   http://zinc15.docking.org/catalogs/
# (2) 点击右侧的 Browse Substances， 
# (3) 点击中间的下载按钮，选择 csv格式的。      
#        
#2.shell 语句处理成单列，并去掉第一行表头
#$ awk -F"," '{print $1}' fda-substances.csv >fda.id
#$ sed -i '1d' fda.id
#
#3. 用python多线程下载，同时下载好的id记录到done文件中
#


################
# on linux only.
# v0.2
################ 

## 任务：从文件读取一行小分子 id，作为参数传给下载进程们。
# 多线程下载 多文件
## 一个进程记录数据，一个进程池 众多线程下载和保存数据。
## 读、写进程之间用queue通信。

#问题： 为什么不能在子进程中完整保存呢？因为没有flush文件。

import os
import re
import requests


############################
##part 1 配置文件:
############################
os.chdir('/home/wangjl/Download/py/') #设置项目目录
print(os.getcwd())

#需要新建文件夹 xx，并在文件夹同级放xx.id文件
pname="molportnp";
#"fda"  #"specsnp" #项目名字project name




############################
#part 2 工作函数
############################
#fun: 根据id下载文件
def download(id):
    #发出请求
    # "http://zinc15.docking.org/substances/ZINC000000000018.sdf"
    rs=requests.get("http://zinc15.docking.org/substances/"+id+".sdf")
    #写入二进制文件
    with open(pname+"/"+id+'.sdf','wb') as f:
        f.write(rs.content)
    return id;
#download("ZINC000000001368")



############################
#part 3 多线程
############################
import time,multiprocessing,os,random,re,sys
from multiprocessing import Queue
from multiprocessing import Process

#start time
start=time.time()
print('='*10, "Begin of main process", os.getpid(), "[child pid by parent ppid]")

# 读和处理数据
def worker(id):
    #一个很耗时的计算
    rs=download(id) #该函数在上一个cell定义的
    q.put(rs) #结果输出到管道
    #print(id+" done.")

#保存的线程1个
def worker_out():
    i=0
    with open(pname+'.done', 'w') as f:
        while True:
            i+=1
            if i%200==0: #进度条
                #pass 
                print(str(i)+" files have been done.  Elapse = "+ str(time.time()-start) +'s' )
            
            rs=q.get() #waite while q is empty
            #print(rs)
            f.write(rs+"\n") #写入文件
            f.flush() #刷新缓存，一次性输出到文件


# 主进程
if __name__ == '__main__':
    q=Queue(30) #会超标，但是不会超出太多
    
    # 声明进程池对象
    pool = multiprocessing.Pool(processes = 12)

    #文件读取，分配任务给进程
    fr=open(pname+".id",'r')

    # 向进程池中提交任务
    i=0
    for lineR in fr.readlines():
        i+=1
        if i>1000:
            pass;
            #break; #测试用语句

        line=lineR.strip()
        #print('>>>>>>to worker:',line)
        #arr=re.split(' ',line)
        #print("start new process", line) #任务是一次发送完的
        pool.apply_async( worker,args=(line,) )
    fr.close() #关闭文件

    #分完任务，开始启动保存进程
    pOut = Process(target=worker_out)
    pOut.start()

    #等待读进程结束
    pool.close()
    pool.join()

    #主线程查看队列，决定是否关掉写循环
    while not q.empty():
        time.sleep(1)#每一秒检查一次队列是否为空

    pOut.terminate(); #终止死循环

print(time.time()-start,'s', '='*10, "End of main process", os.getpid())


========================================
实例：多线程下载沪深股市行情信息（上市公司/csv）[urllib.request.urlopen, shell, file]
----------------------------------------

1. 多线程下载信息
# 多线程，下载股票数据

################
# on linux only.
# v0.2
################ 

## 任务：从爬虫获取股票id，作为参数传给下载进程们。
# 多线程下载 多文件
## 一个进程记录数据，一个进程池 众多线程下载和保存数据。
## 读、写进程之间用queue通信。


import urllib.request
import re
 
##def downback(a,b,c):
##    ''''
##    a:已经下载的数据块
##    b:数据块的大小
##    c:远程文件的大小
##   '''
##    per = 100.0 * a * b / c
##    if per > 100 :
##        per = 100
##    print('%.2f%%' % per)
stock_CodeUrl = 'http://quote.eastmoney.com/stocklist.html'

 
# 获取股票代码列表
def urlTolist(url):
    allCodeList = []
    html = urllib.request.urlopen(url).read()
    html = html.decode('gbk')
    s = r'<li><a target="_blank" href="http://quote.eastmoney.com/\S\S(.*?).html">'
    pat = re.compile(s)
    code = pat.findall(html)
    for item in code:
        if item[0] == '6' or item[0] == '3' or item[0] == '0':
            allCodeList.append(item)
    return allCodeList
 
 
allCodelist = urlTolist(stock_CodeUrl)
len(allCodelist) #3689


import os
import re
import requests


############################
##part 1 配置文件:
############################
os.chdir('/home/wangjl/web/all_stock_data') #设置项目目录
print(os.getcwd())

#需要新建文件夹 xx
pname="20181210";


with open(pname+'.id','w') as f:
    for code in allCodelist:
        f.write(code+"\n")

############################
#part 2 工作函数
############################
def getHtml(url):
    page=urllib.request.urlopen(url)
    html=page.read().decode("gb18030")
    return html
#fun: 根据id下载文件
def download(code):
    if code[0] == '6':
        url = 'http://quotes.money.163.com/service/chddata.html?code=0' + code + \
              '&end=20181210&fields=TCLOSE;HIGH;LOW;TOPEN;LCLOSE;CHG;PCHG;TURNOVER;VOTURNOVER;VATURNOVER;TCAP;MCAP'
    else:
        url = 'http://quotes.money.163.com/service/chddata.html?code=1' + code + \
              '&end=20181210&fields=TCLOSE;HIGH;LOW;TOPEN;LCLOSE;CHG;PCHG;TURNOVER;VOTURNOVER;VATURNOVER;TCAP;MCAP'
    #发出请求,写入二进制文件
    html=getHtml(url)
    with open( pname+"/"+code + '.csv','w') as f:
        f.write(html)
    return code;

download("600000")

print("==2end=============")

############################
#part 3 多线程
############################
import time,multiprocessing,os,random,re,sys
from multiprocessing import Queue
from multiprocessing import Process

#start time
start=time.time()
print('='*10, "Begin of main process", os.getpid(), "[child pid by parent ppid]")

# 读和处理数据
def worker(id):
    #一个很耗时的计算
    rs=download(id) #该函数在上一个cell定义的
    q.put(rs) #结果输出到管道
    #print(id+" done.")

#保存的线程1个
def worker_out():
    i=0
    with open(pname+'.done', 'w') as f:
        while True:
            i+=1
            if i%200==0: #进度条
                #pass 
                print(str(i)+" files have been done.  Elapse = "+ str(time.time()-start) +'s' )
            
            rs=q.get() #waite while q is empty
            #print(rs)
            f.write(rs+"\n") #写入文件
            f.flush() #刷新缓存，一次性输出到文件


# 主进程
if __name__ == '__main__':
    q=Queue(200) #会超标，但是不会超出太多
    
    # 声明进程池对象
    pool = multiprocessing.Pool(processes = 100)

    #文件读取，分配任务给进程
    fr=open(pname+".id",'r')

    # 向进程池中提交任务
    i=0
    for lineR in fr.readlines():
        i+=1
        if i>500:
            pass;
            #break; #测试用语句

        line=lineR.strip()
        pool.apply_async( worker,args=(line,) )
    fr.close() #关闭文件

    #分完任务，开始启动保存进程
    pOut = Process(target=worker_out)
    pOut.start()

    #等待读进程结束
    pool.close()
    pool.join()

    #主线程查看队列，决定是否关掉写循环
    while not q.empty():
        time.sleep(1)#每一秒检查一次队列是否为空

    pOut.terminate(); #终止死循环

print(time.time()-start,'s', '='*10, "End of main process", os.getpid())



2.检查出下载失败的id
# 检测下载个数和结果的差别。
fda1=[]
fda2=[]
#文件读取
def file2Arr(fname):
    fr1=open(fname,'r')

    i=0
    arr=[]
    for lineR in fr1.readlines():
        i+=1
        if i>10:
            pass;
            #break; #测试用语句

        line=lineR.strip()
        arr.append(line)
    fr1.close();
    return arr;

fda1=file2Arr("/home/wangjl/web/all_stock_data/20181210.id")
fda2=file2Arr("/home/wangjl/web/all_stock_data/file")

for id in fda1:
    if id not in fda2:
        print(id)



3.整合出来某一天的所有上市公司的信息
import pandas as pd

# 从文件获取某一天的信息
id="600001"

def getByID_Date(id,date):
    #使用pandas读取txt文件，默认分隔符是逗号。
    df=pd.read_csv("20181210/"+id+'.csv',sep=",", index_col="日期")
    
    #pandas 如果不存在某一行
    if date in df.index:
        return df.loc[[date]] #某一行
    else:
        return df[1:1]

#getByID_Date(id,"2018-12-07")

#合并两条信息
#t1=getByID_Date('000001',"2018-12-07")
#t2=getByID_Date('000002',"2018-12-07")
#t1.append(t2)

#读取code列表
ids=[]
f=open("/home/wangjl/web/all_stock_data/20181210.id",'r')
for code in f.readlines():
    ids.append( code.strip() )
f.close()
print( len(ids) )


#同过循环读取所有的code的第一行
today=""
i=0
for code in ids:
    i+=1
    if i%300==0:
        print(i, ' processing ', code )

    row=getByID_Date(code,"2018-12-07")
    if i==1:
        today=row
    else:
        today=today.append(row)
print("==end==")

pring( today )

data2=today[["股票代码","名称","总市值"]]
data2.to_csv("data2.csv")

import os
os.getcwd()
#'/home/wangjl/web/all_stock_data' 到R中用ggplot2画图。





========================================
### 附录 ###
----------------------------------------


========================================
[收集]爬虫好用的函数
----------------------------------------

#1.获取名单
import requests, time 

def getHTMLText(url): #获得所需的网页源代码
    try:
        user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'
        headers = {'User-Agent': user_agent}
        r = requests.get(url, headers=headers, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""
    
def getFileName():
    dirname = time.strftime('%Y%m%d',time.localtime(time.time()))
    dirname+='sh'
    return  dirname



2. 为了做到随机时间段后执行，做成随机函数

import time,random
for i in range(10): #随机时间段后[2s,12s]执行
    pause=2+10*random.random()
    print(str(i)+' '+str(pause))
    time.sleep(pause)


-








========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------

