sklearn 笔记



========================================
sklearn 简介
----------------------------------------
1. 网站
https://scikit-learn.org/stable/
https://github.com/scikit-learn/scikit-learn

scikit-learn: machine learning in Python


Simple and efficient tools for predictive data analysis
Accessible to everybody, and reusable in various contexts
Built on NumPy, SciPy, and matplotlib
Open source, commercially usable - BSD license


六大类基本功能：
分类
回归
聚类
数据降维
模型选择
数据预处理





2. 十分钟上手sklearn：特征提取，常用模型，

1.PCA算法：主成分分析
2.LDA算法：线性评价分析
3.线性回归
4.逻辑回归
5.朴素贝叶斯
6.决策树
7.SVM
8.神经网络
9.KNN算法


(1) 安装

$ pip3 install scikit-learn -i https://pypi.douban.com/simple/

sklearn 与 scikit-learn 是同一个东西嘛？前置是后者的缩写。安装 假包sklearn 时会默认安装其 scikit-learn。




(2) 内置/玩具 数据集简介

UCI 数据集: 
	https://archive.ics.uci.edu/ml/index.php
	新版 https://archive-beta.ics.uci.edu/
	http://archive.ics.uci.edu/ml/machine-learning-databases/




查看数据集列表
import sklearn.datasets
dir(sklearn.datasets)

'get_data_home',
'load_boston',
'load_breast_cancer',
'load_diabetes',
'load_digits',
'load_files',
'load_iris',
'load_linnerud',
'load_sample_image',
'load_sample_images',
'load_svmlight_file',
'load_svmlight_files',
'load_wine',


某个数据集的详情
from sklearn.datasets import load_boston
load_boston(return_X_y=False)
print(load_boston().DESCR)


载入某个数据集
X, y = load_boston(return_X_y=True)
print(X.shape) #(506, 13)


波士顿房价数据集
# 载入数据
from sklearn.datasets import load_boston
X,y=load_boston().data, load_boston().target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)







========================================
torch, tensorflow, keras, sklearn, 的侧重点、使用习惯与发展趋势
----------------------------------------
业界偏爱tf，学界偏爱pytorch，后者的内存管理为人诟病，真到实际部署还得tf
	只要能用到gpu的我都会用pytorch
	TF做部署还是挺方便的
	TF有种一子落错，满盘皆输的趋势，这种趋势已经比较难改变了
keras现在又分离出来了，都在瞎折腾，还是pytorch稳定

tf与其说是一个框架不如说是dsl了[飙泪笑]学习曲线和python本身完全分离
小模型用keras，大模型用torch，总归不会错[机智]

学pytorch吧，tensorflow学习成本太高。
tensorflow难用是公认的，PyTorch的好用也是公认的，学习PyTorch可以让你尽情施展算法能力。
Tensorflow2 不难了。我大概花了2个月学习并整理了一份TF2的教程。《30天吃掉那只TensorFlow2.0》，有需要的小伙伴可以参考一下哦。
	https://github.com/lyhue1991/eat_tensorflow2_in_30_days



(2)
比如说盖木头房子。想盖什么房子要先选木料，然后加工成需要的形状，最后组合钉装成想要的房子形状。

tensorflow 好比是木头，Keras 好比是拿 tensorflow 做好的木板。如果你盖的房子简单，形状大众，Keras 调用起来会很方便。但如果想设计特殊的房子，那就要从木料开始。

初学和入门的话建议用 keras。但是想要深入或者做自己的APP的话建议用 tensorflow。


(3)
新手且不打算对内部原理进行深入了解：推荐keras快速搭建、训练、测试。

新手且打算研究内部的结构，并且想要自己创造性的生成一些新的结构：keras先拿来跑一下baseline，知道一下机器学习基本流程和大概样子，然后去学tensorflow。（虽然不知道为什么大家都说tensorflow很难学然后选择放弃，但tensorflow的学习资源也是相比其他框架多得多的，所以有那么多资料和攻略，就更不怕难和麻烦了，看一下资料，读一下代码然后自己去实现一下，对自己的能力是很大的提高，而且现在tensorflow的API也很成熟了，各种类型的网络都有现成函数可以调用，所以不用害怕）习惯之后，你会发现其实只要你思路清晰，无非就是以下几个问题：
程序的规范（dtype的匹配，张量shape的匹配）
数据结构、数据接口
并行的处理
其他等等

如果后面有一些终端部署需求的，tensorflow也是再合适不过的，tensorflow.js以及相关的方案都是很棒的。








========================================
KNN 最近邻算法: K-Nearest Neighbor
----------------------------------------

1. 原理

计算新点p0到已知分类的点pn的距离，最近的k个点中哪一类最多，那么新点p0就属于哪一类。

KNN也可用于回归：取最近的几个点的y坐标的平均值作为该点的预测值。


2. 优缺点 

KNN的优点: 简单，容易理解。
KNN的缺点：
	需要对数据认真的预处理
	对规模超大的数据集拟合的时间较长
	对高维数据集拟合欠佳
	对于稀疏数据集束手无策




========================================
|-- KNN 二分类
----------------------------------------
2. 最简单实例: 已知二分类，预测一个新点的分类


(1) 训练集：生成已知标签的数据集

# 导入数据生成器
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

data=make_blobs(n_samples=200, centers=2, random_state=8)
X,y=data

plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k')
plt.show()

# 图略：上下两部分散点，上面紫色，下面黄色。


(2) 画出分类器
import numpy as np

# 导入KNN分类器
from sklearn.neighbors import KNeighborsClassifier

clf=KNeighborsClassifier()
clf.fit(X, y)

# 画网格背景
x_min, x_max = X[:, 0].min() -1, X[:,0].max()+1
y_min, y_max = X[:, 1].min() -1, X[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
# 对区域的每一点进行预测，作为颜色值
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)

# 画散点图
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto') # 带颜色的网格, 第三个参数是颜色
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k') #散点图
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.show()

图略:
# 创建分类模型，由上部粉色区域和下部灰色区域组成。
# 如果有新数据，落到哪里就是哪个分类了。



(3) 画出新数据点 6.75, 4.82
# 在分类模型上画出该点：在 plt.show() 前加入这句: plt.scatter(6.75, 4.82, marker="*", c="red", s=200)

# 画网格背景
x_min, x_max = X[:, 0].min() -1, X[:,0].max()+1
y_min, y_max = X[:, 1].min() -1, X[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
# 对区域的每一点进行预测，作为颜色值
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)

# 画散点图
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto') # 带颜色的网格, 第三个参数是颜色
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k') #散点图
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.scatter(6.75, 4.82, marker="*", c="red", s=200) #画出新的数据点
plt.show()

图略：新点是一个红色五角星。
# 从图中看，新点落在下方灰色区域中。



(4) 带入模型验证一次。
clf.predict([ [ 6.75, 4.82] ]) #确实归为第1类
# array([1])


print(clf.predict([ [6, 10] ])) #上方点 0
print(clf.predict([ [6, -0.29] ])) #下方点 1



========================================
|-- KNN 多分类
----------------------------------------

(1) 生成已知标签的数据集：500个点，分为5类
# 导入数据生成器
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# 修改 make_blobs 的 center 参数，分类数提高到5个
# 修改 n_samples 参数，把样本量也增加到 500个
data2=make_blobs(n_samples=500, centers=5, random_state=8)
X2,y2=data2

# 画散点图
plt.scatter(X2[:,0], X2[:,1], c=y2, cmap=plt.cm.spring, edgecolor='k')
plt.show()

#图略：5类中的2类有重叠部分。重叠部分一般不好区分，是分类错误最多的区域，。



(2) 使用KNN建模
import numpy as np

# 导入KNN分类器
from sklearn.neighbors import KNeighborsClassifier

clf=KNeighborsClassifier()
clf.fit(X2, y2)


# 画图
x_min, x_max = X2[:, 0].min() -1, X2[:,0].max()+1
y_min, y_max = X2[:, 1].min() -1, X2[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto')

plt.scatter(X2[:,0], X2[:,1], c=y2, cmap=plt.cm.spring, edgecolor='k')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.show()

# 建立5个分区，大部分是正确分类的，重合区域、边界附近有少部分点是错误分类的。
#耗时比上一次多了很多。


(3) 输出在训练集中的正确率
clf.score(X2, y2) #0.956







========================================
|-- KNN 用于回归分析
----------------------------------------
KNN回归的原理: 对x轴进行遍历，取距离最近的几个点的y坐标的平均值作为预测值。

(1) 导入 make_regression 回归数据生成器
from sklearn.datasets import make_regression
# 生成特征数量为1，噪音为50的数据集
X,y=make_regression(n_features=1, n_informative=1, noise=50, random_state=8)

# 散点图
import matplotlib.pyplot as plt
plt.scatter(X,y, c="orange", edgecolor="k")
plt.show()

#图略： x范围+-3， y范围+-250，倾斜45度角的散点



(2) 建立KNN回归模型
# 导入用于回归分析的KNN模型
from sklearn.neighbors import KNeighborsRegressor
reg=KNeighborsRegressor()
# 用KNN模型拟合数据
reg.fit(X,y)

# 可视化
import numpy as np
z=np.linspace(-3,3, 200).reshape(-1,1)
plt.scatter(X,y,c="orange", edgecolor="k")
plt.plot(z, reg.predict(z), c="k", linewidth=3)
plt.title("KNN Regressor")
plt.show()
# 图略: 黑色表示KNN回归生成的模型。直观看，效果不好，大量的数据点没有被模型覆盖。


# 给模型评分
reg.score(X,y) #0.772


(3) 怎么提高模型打分？

# 调整 n_neighbors，默认5，我们减少该值
reg2=KNeighborsRegressor(n_neighbors=2)
reg2.fit(X,y)

# 再次可视化
import numpy as np
z=np.linspace(-3,3, 200).reshape(-1,1)
plt.scatter(X,y,c="orange", edgecolor="k")
plt.plot(z, reg2.predict(z), c="k", linewidth=3)
plt.title("KNN Regressor: n_neighbors=2")
plt.show()
# 图略：黑色曲线覆盖了更多的点，也就是说，模型变复杂了。


# 再次给模型评分
reg2.score(X,y) #0.858
# 打分确实提高了，0.77->0.86








========================================
|-- KNN 分类真实案例：酒的分级
----------------------------------------
假设我们对酒的品质一无所知，现在已知一个酒的各项参数，让给出分级，怎么做？

(1) 载入酒的数据
from sklearn.datasets import load_wine
wine_dataset=load_wine()
print(type(wine_dataset)) #这是一个很复杂的格式 <class 'sklearn.utils.Bunch'>


# 尝试了解该数据集
wine_dataset
# 略

wine_dataset.keys() #dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])
# 数据 data，分类目标 target, 分类名字 target_names，数据描述 DESCR，特征变量的名字 feature_names

wine_dataset.data.shape #(178, 13)  178行 样本，13列 特征变量

print(wine_dataset.DESCR) # 略
#可见共3个分类，class_0-2。
# 13个变量分别是: 酒精含量、苹果酸、色彩表合度等。




(2) 分拆成训练集和测试集
# train_test_split 函数，默认随机分组，75%的归为训练集，25%归为测试集。

# 一般使用X表示数据特征，y表示对应的标签。因为X是二维的数组，也称为矩阵，y是一维数组，也叫向量。

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split

# 拆分数据。random_state 随机数种子。当设置为0或者缺省时，每次生成的随机数都不同。
X_train, X_test, y_train, y_test=train_test_split(
    wine_dataset["data"], wine_dataset["target"], random_state=8
)
# 检查数据 行列 数
print("X_train", X_train.shape)
print("X_test", X_test.shape)
print("y_train", y_train.shape)
print("y_test", y_test.shape)

输出 
X_train (133, 13)
X_test (45, 13)
y_train (133,)
y_test (45,)


(3) 建模
# 导入 KNN 分类模型
from sklearn.neighbors import KNeighborsClassifier
# 指定模型的 n_neighbors 参数为1
knn=KNeighborsClassifier(n_neighbors=1) #最近的k=1个已知点

# fit: 用模型对数据进行拟合
knn.fit(X_train, y_train)
# KNeighborsClassifier(n_neighbors=1)

# 使用测试集检验模型
print(knn.score(X_train, y_train)) #1.0 对训练集全对
knn.score(X_test, y_test) #0.7111 测试集 71% 正确率


(4) 使用模型对新数据进行预测
import numpy as np
X_new=np.array([ [ 13.2, 2.77, 2.51, 18.5,96.6,1.04,2.55,0.57,1.47,6.2,1.05,3.33,820] ] )
print(X_new.shape) #(1, 13)

#预测
prediction=knn.predict(X_new)
print(prediction, wine_dataset["target_names"][prediction] )

输出：
(1, 13)
[2] ['class_2']



小结：测试集上的正确率确实有点低。尝试看看怎么优化。
不过，作为入门级的方法，主要用于理解整体建模套路。






========================================
glm 广义线性模型: 适合于高维数据
----------------------------------------

1. 线性模型是一类广泛应用于机器学习领域的预测模型。
使用输入数据集的特征的线性函数进行建模，并对结果进行预测。

(2). 概要
线性模型的基本概念
线性回归模型
岭回归模型
套索回归模型 Lasso
二元分类器中的 逻辑回归
和线性SVC模型



2. 线性模型的概念

公式 y_hat = w*x + b

对于只有一个特征变量的数据集，w是直线的斜率，b是截距。
如果特征值增加，每个w值就对应每个特征直线的斜率。

另一种角度看，模型的预测可以看做输入特征的加权和，而w代表每个特征的权重，当然w也可以为负数。


(1) 画一条直线

import numpy as np
import matplotlib.pyplot as plt

# 令x为-5到5之间，元素数为100的等差数列
x=np.linspace(-5,5,100)
# 输入直线方程
y=0.5*x + 3

plt.plot(x,y, c="orange")
plt.title("Straight line")
plt.show()


线性模型，就是通过训练数据确定自身斜率和截距。



(2) 通过2点确定一条直线

# 2点(1,3) (4,5) 确定一条直线

# 导入线性回归模型
from sklearn.linear_model import LinearRegression

# 输入2个点的横坐标
X=[[1], [4]]
# 输入2个点的纵坐标
y=[3,5]

# 用线性模型拟合这2个点
lr=LinearRegression().fit(X, y)
# 画出2个点和直线
z=np.linspace(0,5, 20)
plt.scatter(X,y,s=80) #画2个点
plt.plot(z, lr.predict(z.reshape(-1,1)), c='k')

plt.title("Straight line")
plt.show()

# 输出该直线的方程
w=lr.coef_[0]
b=lr.intercept_
print( "y = {:.3f}".format(w), "x", " + {:.3f}".format(b)   )




(3) 如果是3个点呢？
# 2点(1,3) (4,5) (3,3)确定一条直线
from sklearn.linear_model import LinearRegression

X=[[1], [4], [3]]
y=[3, 5, 3]

# 拟合
lr=LinearRegression().fit(X, y)

# 画图
z=np.linspace(0,5, 20)
plt.scatter(X,y,s=80) #画2个点
plt.plot(z, lr.predict(z.reshape(-1,1)), c='k')
plt.title("Straight line")
plt.show()

# 输出该直线的方程
w=lr.coef_[0]
b=lr.intercept_
print( "y = {:.3f}".format(w), "x", " + {:.3f}".format(b)   )


直线没有穿过任何一点，而是位于和3个点的距离和最小的位置。



(4) 生成更多点，做线性拟合

from sklearn.datasets import make_regression
#生成用于回归分析的数据
X,y=make_regression(n_samples=50, n_features=1, n_informative=1, noise=50, random_state=1)

# 线性拟合
reg=LinearRegression()
reg.fit(X,y)
# 生成等差数列z作为横轴，画线性模型的图形
z=np.linspace(-3,3, 200).reshape(-1,1)
plt.scatter(X, y, c='b', s=60)
plt.plot(z, reg.predict(z), c='k') #预测每个x对应的y
plt.title("Linear regression")
plt.show()

# 输出该直线的方程
w=reg.coef_[0]
b=reg.intercept_
print( "y = {:.3f}".format(w), "x", " + {:.3f}".format(b)   )


注意： sklearn 把下划线作为训练数据集的结尾，比如 coef_, intercept_, 以便和用户自定义参数区分开。

特征数只有1，用一条直线进行预测分析。
特征变量为2时，是一个平面。
更多，是一个超平面。

如果训练数据集的特征变量大于数据点的数量的时候，线性模型可以对训练数据做出近乎完美的预测。







3. 最基本的线性模型 - 线性回归

线性回归，也称为 普通最小二乘法(OLS)。
线性回归没有可供用户调节的参数。

实例: 使用 make_regression 函数 生成一个样本数量为100，特征数为2的数据集，
使用 train_test_split 函数分割训练集和验证集，
用线性回归模型计算出w值和b值。

(1) 无噪音模拟数据

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
X,y=make_regression(n_samples=100, n_features=2, n_informative=2, random_state=38)
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)
lr=LinearRegression().fit(X_train, y_train)

# 打印出模型
print("coef:", lr.coef_[:])
print("intercept:", lr.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print("training set:", lr.score(X_train, y_train))
print("tesing set:", lr.score(X_test, y_test))

# 完全对的原因，是因为没有添加noise！真实世界的数据，噪音是很大的。


(2) 载入真实数据 - 糖尿病数据

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# 载入数据
from sklearn.datasets import load_diabetes
X,y=load_diabetes().data, load_diabetes().target

X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)
lr=LinearRegression().fit(X_train, y_train)

# 打印出模型
print("coef:", lr.coef_[:])
print("intercept:", lr.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print()
print("training set:", lr.score(X_train, y_train))
print("tesing set:", lr.score(X_test, y_test))

# 分别是0.53 和0.46，打分降低了很多！

线性回归很容易过拟合。
在训练集和测试集之间打分差异过大，是过拟合的一个明确信号。

我们怎么控制模型的复杂度呢？岭回归是 标准线性回归的一个常用的替代模型。







========================================
|-- 使用L2正则化的线性模型 - 岭回归
----------------------------------------

保留全部特征变量，只是降低特征变量的系数来避免过拟合的方法，称为L2正则化。

(1) 糖尿病模型，岭回归

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split

# 载入数据
from sklearn.datasets import load_diabetes
X,y=load_diabetes().data, load_diabetes().target
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)

# 导入岭回归
from sklearn.linear_model import Ridge

# 使用岭回归对数据进行拟合
ridge=Ridge().fit(X_train, y_train)

# 打印出模型
print("coef:", ridge.coef_[:])
print("intercept:", ridge.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print()
print("training set:", ridge.score(X_train, y_train))
print("tesing set:", ridge.score(X_test, y_test))

# 分别是 0.43 和 0.43，打分接近


可以说，复杂度越低的模型，在训练集上表现越差，但是其泛化能力会更好。

如果在意泛化能力，则应该选择岭回归，而不是线性回归模型



(2) 岭回归的参数调节

岭回归是在模型的简单性（使系数趋近于零）和它在训练集上的性能之间取的平衡的一种模型。
用户可以使用alpha参数控制模型更加简单，还是在训练集上性能更高。

上例中使用的默认alpha=1.

注意: alpha的取值并没有一定的规定。取决于特定的数据集。
增加alpha 会降低特征变量的系数，使其趋于零，从而降低在训练集的性能，但更有助于泛化。

1) 本例子使用 alpha =10.
from sklearn.linear_model import Ridge

# 使用岭回归对数据进行拟合，设置 alpha=10
ridge=Ridge(alpha=10).fit(X_train, y_train)

# 打印出模型
print("coef:", ridge.coef_[:])
print("intercept:", ridge.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print()
print("training set:", ridge.score(X_train, y_train))
print("tesing set:", ridge.score(X_test, y_test))

# 分别是 0.15 和 0.16，测试集打分超过训练集了
# 也就是说，如果模型过拟合，可以通过提高 alpha 值来降低过拟合现象。


2) 降低 alpha 值会让系数的限制变得不那么严格。当alpha很小时，限制可以忽略不计，非常接近线性回归。
from sklearn.linear_model import Ridge

# 使用岭回归对数据进行拟合，设置 alpha=0.1
ridge=Ridge(alpha=0.1).fit(X_train, y_train)

# 打印出模型
print("coef:", ridge.coef_[:])
print("intercept:", ridge.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print()
print("training set:", ridge.score(X_train, y_train))
print("tesing set:", ridge.score(X_test, y_test))

# 分别是 0.52 和 0.47
# 相比线性模型，alpha很小时，训练集打分略降低，而测试集打分略提高。




(3) alpha值对模型的影响

画图展示 不同 alpha 值对应的模型的 coef_ 属性。
较高的 alpha 值表示模型的限制更加严格。
所以我们认为，alpha值越高，coef_属性的数值会更小，反之 coef_ 属性的值更大。



import matplotlib.pyplot as plt

# alpha=0.1 时的模型系数
plt.plot(Ridge(alpha=0.1).fit(X_train, y_train).coef_, 'o', label="Ridge alpha=0.1")

# alpha=1 时的模型系数
plt.plot(Ridge(alpha=1).fit(X_train, y_train).coef_, 's', label="Ridge alpha=1")

# alpha=10 时的模型系数
plt.plot(Ridge(alpha=10).fit(X_train, y_train).coef_, '^', label="Ridge alpha=10")

# 绘制线性回归的系数作为对比
from sklearn.linear_model import LinearRegression
lr=LinearRegression().fit(X_train, y_train)
plt.plot(lr.coef_, "o", label="linear regression")
#
plt.xlabel("coefficient index")
plt.ylabel("coefficent magnitude")
plt.hlines(0,0,len(lr.coef_)) #水平直线，过原点
plt.legend()
plt.show()



横轴代表 coef_ 属性：
x=0 显示第一个特征变量的系数，
x=1 显示的是第二个特征变量的系数，
以此类推，直到 x=10时。
纵轴显示特征变量的系数数量级。

当 alpha=10 时，特征变量系数大多在0附近；
当 alpha=1时，岭模型的特征便阿玲系数普遍增大了；
当 alpha=0.1 时，岭回归系数更大了，已经接近线性回归。

而线性回归模型没有经过正则化处理，系数非常大，已经快跑到图表之外了。



(4) 数据集大小对岭回归的影响 - 学习曲线
另一个理解正则化对模型影响的方法，就是固定alpha值，该不安训练集的数据量。

x轴是数据集大小，y轴是学习打分，这样的曲线叫学习曲线。


import numpy as np

from sklearn.model_selection import learning_curve, KFold
# 定义一个绘制学习曲线的函数
def plot_learning_curve(est, X, y):
    # 对数据进行20次拆分用来对模型进行评分
    training_set_size, train_scores, test_scores=learning_curve(
        est, X, y, train_sizes=np.linspace(0.1, 1, 20), cv=KFold(20, shuffle=True,random_state=1))
    
    estimator_name=est.__class__.__name__
    line=plt.plot(training_set_size, train_scores.mean(axis=1), '--', label="training "+estimator_name)
    plt.plot(training_set_size, test_scores.mean(axis=1), '-',
            label="test "+estimator_name, c=line[0].get_color())
    plt.xlabel("Training set size")
    plt.ylabel("Score")
    plt.ylim(0, 1.1)    

plot_learning_curve(Ridge(alpha=1), X, y)
plot_learning_curve(LinearRegression(), X, y)
plt.legend(loc=(0, 1.05), ncol=2, fontsize=11)


- 可见，数据量小的时候，岭回归的训练集和测试集表现差不多，而普通线性回归则差异很大。
- 当数据量很大时，正则化就没那么重要了，两者表现一致。
- 随着数据量的增大，线性回归在训练集上的得分是下降的；说明数据量越大，线性回归越不容易过拟合，或者越难记住已知数据。





(5) 岭迹图 x=alpha, y=coef


# 创建 alpha 集合
alphas = np.logspace(-10,2,100)  # -10 到 2 取100份
# 计算对应的 coef
coefs = []
for alpha in alphas:
    # 获取模型 设置参数
	# 通过修改Ridge(fit_intercept=False)，来让岭回归模型来关闭差值，不让差值调整结果值，这样我们获得的斜率就不是0了。
    rr = Ridge(alpha=alpha, fit_intercept=False)
    rr.fit(X_train, y_train)
    coefs.append(rr.coef_)
# 绘图
plt.plot(alphas,coefs)
# 设置坐标轴 不是以均匀的方式展示 设置x轴线 而是 以10的倍数来显示
plt.xscale('log')
plt.show()





========================================
|-- 使用 L1 正则化的线性模型 - 套索回归 lasso
----------------------------------------


通俗解释
线性回归时使 权重的绝对值相加 让其结果不能大于某个值

一般在特征很多的时候，很多特征对结果的影响几乎为0，我们就可以限制 拉姆达的值，让加和小于某个值，其中权重最小的就会被归零，加和的值还大于拉姆达的话，再让最小的归零，知道小于拉姆达为止。





L1 正则化和L2正则化一样，也会让系数限制在很接近0的范围内。
但是 L1 正则化还会让一部分系数正好等于0，这可以看做是对特征进行自动筛选。
相当于忽略不重要的特征，突出重要的特征。


(1) 套索回归 - 默认参数
# 载入 糖尿病模型
import numpy as np
import matplotlib.pyplot as plt

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split

# 载入数据
from sklearn.datasets import load_diabetes
X,y=load_diabetes().data, load_diabetes().target
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)


# 载入套索回归
from sklearn.linear_model import Lasso
# 使用套索回归拟合
lasso=Lasso().fit(X_train, y_train)

#输出打分
print("training score:", lasso.score(X_train, y_train))
print("testing score:", lasso.score(X_test, y_test))
print("特征数:", np.sum(lasso.coef_ !=0 ))

# 打分只有 0.36， 0.37，只使用了3个特征。
# 训练集结果也很糟糕，说明fasjeng欠拟合。



(2) 套索回归的参数调节

套索回归也有一个正则化参数 alpha，用来控制变量系数被约束到0的强度。
默认是1.

为了降低欠拟合的程度，尝试降低 alpha。
还需要增大 最大迭代次数(max_iter) 的默认设置。



# 使用套索回归拟合
lasso=Lasso(alpha=0.1, max_iter=100000).fit(X_train, y_train)

#输出打分
print("training score:", lasso.score(X_train, y_train))
print("testing score:", lasso.score(X_test, y_test))
print("特征数:", np.sum(lasso.coef_ !=0 ))


输出: 
training score: 0.519480608218357
testing score: 0.47994757514558173
特征数: 7


- 降低alpha值可以拟合出更复杂的模型，从而在训练集和测试集都能获得良好的表现。
- 该结果比 岭回归 稍好，且只用了10个特征中的7个特征。
- 但是，alpha 设置的太低，就去掉了正则化效果，模型就会像线性回归一样，出现过拟合现象。


# 设置 lasso 回归alpha=0.0001
lasso=Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)
#输出打分
print("training score:", lasso.score(X_train, y_train))
print("testing score:", lasso.score(X_test, y_test))
print("特征数:", np.sum(lasso.coef_ !=0 ))

输出
training score: 0.5303811330981303
testing score: 0.4594509683706016
特征数: 10

# alpha太小时，所有变量都用上了。且测试集打分低了10个百分点，说明有过拟合现象。




(3) 套索回归与岭回归的对比

画出不同alpha值的套索回归与岭回归的系数。


# 绘制 alpha=1, 0.1, 0.001 是的模型系数
plt.plot( Lasso(alpha=1).fit(X_train, y_train).coef_, 's', label="Lasso alpha=1")
plt.plot( Lasso(alpha=0.1).fit(X_train, y_train).coef_, '^', label="Lasso alpha=0.1")
plt.plot( Lasso(alpha=0.001).fit(X_train, y_train).coef_, 'v', label="Lasso alpha=0.001")

# 绘制 alpha=0.1 时的岭回归模型
from sklearn.linear_model import Ridge
plt.plot(Ridge(alpha=0.1).fit(X_train, y_train).coef_, "o", label="Ridge alpha=0.1")

plt.legend(ncol=2, loc=(0, 1.05))
#plt.ylim(-25, 25)
plt.xlabel("Coefficient index")
plt.ylabel("Coefficient magnitude")
plt.show()



alpha=1 时，不仅大部分系数为0，且不为0的几个点绝对值也很小。
当alpha=0.1时，大部分系数也为0，但是等于0的个数比alpha=1时少了很多。
而alpha=0.001时，整个模型几乎没有被正则化，大部分系数是非零的，且绝对值较大。

alpha=0.1 的岭回归和 alpha=0.1 的套索回归，预测能力类似，但是岭回归几乎所有的系数都非零。

同等条件选择岭回归。
但是如果特征过多，选择套索回归，因为变量少了更容易解释和理解。




(4) 系数收缩图 x=alpha, y=coef
# 创建 alpha 集合
alphas = np.logspace(-3, 2, 100)  # -3 到 2 取100份
# 计算对应的 coef
coefs = []
for alpha in alphas:
    # 获取模型 设置参数
	# 通过修改Ridge(fit_intercept=False)，来让岭回归模型来关闭差值，不让差值调整结果值，这样我们获得的斜率就不是0了。
    rr = Ridge(alpha=alpha, fit_intercept=False)
    rr.fit(X_train, y_train)
    coefs.append(rr.coef_)
# 绘图
plt.plot(alphas,coefs)
# 设置坐标轴 不是以均匀的方式展示 设置x轴线 而是 以10的倍数来显示
plt.xscale('log')
plt.xlabel("Alpha")
plt.ylabel("Coef")
plt.show()








========================================
|-- 弹性网模型 Elastic net: 套索回归 + 岭回归 // todo
----------------------------------------
通过设置系数，达到在一个模型中组合使用套索回归和岭回归的目的。

用户需要调整2个参数，一个是L1正则化参数，另一个是L2正则化参数。


正则化项L1和L2的直观理解
https://blog.csdn.net/jinping_shi/article/details/52433975







========================================
|-- 其他线性模型
----------------------------------------

logistic regression
线性支持向量机 (Linear SVM)


对于线性模型来说，最主要的桉树就是 正则化参数(Regularization Parameter).
- 在线性回归、岭回归、套索回归中使用alpha参数调节；
- 在logistic regression和 Linear SVM 中通过C参数调节。
- 如果特征太多，使用L1正则化，如lasso回归。
- 如果特征不多，而且每一个都很重要，则使用L2正则化建模，如岭回归。








========================================
|-- 模拟对权重的预测能力：普通线性回归、岭回归与lasso回归比较
----------------------------------------

import numpy as np
import matplotlib.pyplot as plt

# (1)创建数据
np.random.seed(10)  # 随机数种子
samples = 50  # 有几个样本就有几行
features = 100  # 有几个特征就有几列
X = np.random.randn(samples,features)  # 以0为中心，标准差为1的数 参数为形状
# X 作为特征值

# 随机生成权重
w = 10*np.random.randn(features)  # 有几个特征就有几个权重的值  给每个权重扩大10倍

# 随机将一些权重归零
index = np.random.permutation(features)  # 打乱的 各个权重的索引
index[:90]  # 找出前九十个索引
w[index[:90]] = 0  # 把前九十个打乱顺序的所对应的权重值 归零

# 根据现有的特征值与权重值求目标值
y = np.dot(X,w)


# (2) 比较各回归方式 预测权重的效果
from sklearn.linear_model import LinearRegression,Ridge,Lasso
lr = LinearRegression()
rr = Ridge(alpha=1, fit_intercept=False)  #这里主要研究 w 的值，所以为了不受影响，不使用偏差值
lasso = Lasso(alpha=0.8)  #alpha 用来设置权重的上限，不过alpha 的值为0-1的小数 用来表示有用的特征的比例

# 注意：Lasso中的alpha 表示有用特征的比例
# 例如 共有 5 个特征， 有用的 只有一个，那么 alpha = 0.2

# 训练数据
lr.fit(X,y)
rr.fit(X,y)
lasso.fit(X,y)


# 查看各个模型对coef的预测是否正确
# plt.figure(figsize=(12,8))  #设置画布大小
axes1 = plt.subplot(2,2,1)  # 先绘制真实的权重
axes1.plot(w)
axes1.set_title('real')

# 普通线性回归
axes2 = plt.subplot(2,2,2)
axes2.plot(lr.coef_)
axes2.set_title('lr')

# 岭回归
axes3 = plt.subplot(2,2,3)
axes3.plot(rr.coef_)
axes3.set_title('rr')

# 拉索回归
axes4 = plt.subplot(2,2,4)
axes4.plot(lasso.coef_)
axes4.set_title('lasso')

plt.show()








========================================
朴素贝叶斯 Naive Bayes: 基于先验知识进行分类
----------------------------------------

朴素贝叶斯算法是一种基于贝叶斯理论的有监督学习算法。
朴素，是因为设个算法假设样本特征之间是相互独立的，这个“朴素”假设。
因为该假设，导致NB模型很高效。


概要:
贝叶斯定理简介
朴素贝叶斯的简单应用
伯努利朴素贝叶斯、高斯朴素贝叶斯和多项式朴素贝叶斯
朴素贝叶斯实例 - 判断肿瘤是良性还是恶性





========================================
|-- NB 基本概念: 贝叶斯定理 和 朴素贝叶斯分类器
----------------------------------------

1. 条件概率的定义

(1) A和B同时发生的概率，
	就是A发生的情况下，B发生的概率；
	或者
	就是B发生的情况下，A发生的概率；

写成公式，就是 
P(AB)=P(A)*P(B|A) = P(B)*P(A|B)
右边的等式做一下变换，就是贝叶斯公式 P(A|B)=P(A)*P(B|A) /P(B)


2. 简单应用 
用 0 代表没有下雨，而1代表下雨。

import numpy as np

# 过去7天是否下雨可以用数组表示 
y=np.array( [0,1,1,0,1,0,0] )

# 其他气象信息: 北风、闷热、多云、天气预报是否下雨
X=np.array([
	[0, 1, 0, 1],
	[1, 1, 1, 0],
	[0, 1, 1, 0],
	[0, 0, 0, 1],
	[0, 1, 1, 0],
	[0, 1, 0, 1],
	[1, 0, 0, 1],
])

# 分析下雨、不下雨时每个气象条件的频数
counts={}
for label in np.unique(y):
	counts[label]=X[ y==label ].sum(axis=0)

print("Feature counts:\n{}".format(counts))

# 没下雨时(y=0), 4天天气预报都说下雨，1天北风，2天闷热，0天多云。
# 下雨时(y=1)，天气预报都说没有下雨，1天北风，3天闷热，3天多云。

Feature counts:
{0: array([1, 2, 0, 4]), 1: array([1, 3, 3, 0])}




(2) 使用 伯努利贝叶斯分类器
from sklearn.naive_bayes import BernoulliNB
# 拟合数据
clf=BernoulliNB()
clf.fit(X,y)

# 预测一下训练集
print( clf.predict(X) )
# 打分
print( clf.score(X, y) )

输出：
[0 1 1 0 1 0 0]
1.0


还可以使用 clf.predict_proba() 给出下雨/不下雨的概率。


2) 如果天气预报说没有下雨，且出现多云，倾向于归类到“下雨”

clf.predict( [[0,0,1,0]] ) # array([1])
clf.predict_proba( [[0,0,1,0]] ) #array([[0.13848881, 0.86151119]]) #下雨的概率更大


3) 如果天气预报说下雨，且北风，闷热，无云，倾向于归类到“不下雨”
clf.predict( [[1,1,0,1]] ) #array([0])
clf.predict_proba( [[1,1,0,1]] ) #array([[0.92340878, 0.07659122]]) #不下雨的概率更大


警告：scikit-learn 官网文档给出一段很搞笑的描述：
https://scikit-learn.org/stable/modules/naive_bayes.html
On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.

	虽然朴素贝叶斯是相当好的分类器，但是对于预测具体数值并不擅长，
	因此 predict_proba() 给出的预测概率，不要太当真。




========================================
|-- 朴素贝叶斯的不同方法
----------------------------------------

sklearn 中的朴素贝叶斯有3种方法，分别是：
伯努利朴素贝叶斯 Bernoulli Naive Bayes;
高斯朴素贝叶斯 Gaussian Naive Bayes;
多项式贝叶斯 Multinomial Naive Bayes;


1. 伯努利分布也被称为 “二项分布” 或者 “0-1分布”。
比如我们进行抛硬币的游戏，结果就是正面、反面，我们称抛硬币的结果是符合伯努利分布的。

(1) 
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split

# 生成样本，数量500，分类数5
X,y=make_blobs(n_samples=500, centers=5, random_state=8)

# 拆分数据
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)

# 使用伯努利贝叶斯拟合数据
nb=BernoulliNB()
nb.fit(X_train, y_train)

# 打分
print("trainning score: {:0.3f}".format( nb.score(X_train, y_train) ) )
print("testing score: {:0.3f}".format( nb.score(X_test, y_test) ) )

# 只有一半分类是正确的，很糟糕！
输出 
trainning score: 0.499
testing score: 0.544


2) # 可视化，为什么这么糟糕
import matplotlib.pyplot as plt
x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5
y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5

# 用不同背景色表示不同分类
xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02),
                  np.arange(y_min, y_max, 0.02))
z=nb.predict(np.c_[(xx.ravel(), yy.ravel())]).reshape(xx.shape)
plt.pcolormesh(xx, yy, z, cmap=plt.cm.Pastel1, shading='auto')

# 画训练集和测试集散点图
plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.cool, edgecolor='k')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.cool, edgecolors='k', marker='*')
plt.xlim( xx.min(), xx.max())
plt.ylim( yy.min(), yy.max())
plt.title("Classifier: BernoulliNB")
plt.show()


# 这就是简单把2条线，分为4个象限，注意有3个颜色。
# 因为使用了伯努利朴素贝叶斯的默认参数 binarize=0.0，所以模型对于数据的判断是
#  如果特征1大于或等于0，且特征2大于或等于0，归为一类；
#  如果特征1小于0，且特征2小于0，归为一类；
#  其余归为一类。
# 所以分类效果很烂。

# 对于多分类，不能使用伯努利朴素贝叶斯模型了。可以使用高斯朴素贝叶斯模型。








========================================
|-- 高斯朴素贝叶斯 (假设特征符合高斯分布)
----------------------------------------

就是假设样本的特征符合高斯分布/正态分布。

from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
gnb.fit(X_train, y_train)

# 打分
print("trainning score: {:0.3f}".format( gnb.score(X_train, y_train) ) )
print("testing score: {:0.3f}".format( gnb.score(X_test, y_test) ) )


输出: 
trainning score: 0.939
testing score: 0.968



# 可视化，为什么分类效果这么好

# 用不同背景色表示不同分类
xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02),
                  np.arange(y_min, y_max, 0.02))
z=gnb.predict(np.c_[(xx.ravel(), yy.ravel())]).reshape(xx.shape)
plt.pcolormesh(xx, yy, z, cmap=plt.cm.Pastel1, shading='auto')

# 画训练集和测试集散点图
plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.cool, edgecolor='k')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.cool, edgecolors='k', marker='*')
plt.xlim( xx.min(), xx.max())
plt.ylim( yy.min(), yy.max())
plt.title("Classifier: GaussianNB")
plt.show()


# 可见，高斯NB的分类边界比伯努利NB复杂的多，且基本分类正确。
# 最常用，因为自然科学和社会科学，大量现象都符合正态分布。




========================================
|-- 多项式朴素贝叶斯分布
----------------------------------------

二项分布通过抛硬币来理解，
多项式分布可以通过掷骰子来理解。

均匀的6面骰子，每次投掷后朝上的一面是1-6这6个数字。如果投掷n次，则每个面朝上的次数的分布，符合多项式分布。



from sklearn.naive_bayes import MultinomialNB

mnb=MultinomialNB()
#mnb.fit(X_train, y_train) # 报错 ValueError: Negative values in data passed to MultinomialNB (input X)
#mnb.score(X_test, y_test)


# 只能传入非负数
# 导入数据预处理工具 MinMaxScaler，作用是把特征值全部转为0-1之间。
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit(X_train)
X_train_scaled=scaler.transform(X_train)
X_test_scaled=scaler.transform(X_test)

# 使用多项式朴素贝叶斯拟合经过预处理的数据
mnb.fit(X_train_scaled, y_train)
mnb.score(X_test_scaled, y_test) #0.32




# 这个打分很糟糕，比伯努利NB还差。可视化

# 用不同背景色表示不同分类
xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02),
                  np.arange(y_min, y_max, 0.02))
z=mnb.predict(np.c_[(xx.ravel(), yy.ravel())]).reshape(xx.shape)
plt.pcolormesh(xx, yy, z, cmap=plt.cm.Pastel1, shading='auto')

# 画训练集和测试集散点图
plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.cool, edgecolor='k')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.cool, edgecolors='k', marker='*')
plt.xlim( xx.min(), xx.max())
plt.ylim( yy.min(), yy.max())
plt.title("Classifier: MultinomialNB")
plt.show()

# 大部分数据放到了错误的分类中。

# 多项式NB只适合对非负离散数值特征进行分类。典型例子是转化为向量后的文本数据进行分类。





========================================
|-- 真实数据 - 判断中流是良性还是恶性
----------------------------------------

1. 了解数据
from sklearn.datasets import load_breast_cancer
cancer=load_breast_cancer()

print( cancer.keys() )
# dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])

print(cancer.DESCR)

# 共  569 个病例，每个病人 30 个数值特征，
# 2分类的结果(恶性/良性): 212 - Malignant, 357 - Benign

print( cancer.target_names) #['malignant' 'benign']
print( cancer["feature_names"]) #30个特征的名字



2. 建模 

X, y=cancer.data, cancer.target

# 拆分数据
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=38)
print("train set size:", X_train.shape)
print("test set size:", X_test.shape)

# 建模
from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
gnb.fit(X_train, y_train)

# 打分
print("trainning score: {:0.3f}".format( gnb.score(X_train, y_train) ) )
print("testing score: {:0.3f}".format( gnb.score(X_test, y_test) ) )

输出 
train: (426, 30)
test: (143, 30)
trainning score: 0.948
testing score: 0.944


(2) 随便预测一个
print("predict:", gnb.predict( [X[312]] ))
print("real:", y[312])

输出：
predict: [1]
real: 1




3. 学习曲线 x=样本量 y=打分
随着样本量的增加，模型的打分变化情况。

import numpy as np
import matplotlib.pyplot as plt

# 导入学习曲线库
from sklearn.model_selection import learning_curve
# 导入随机拆分工具
from sklearn.model_selection import ShuffleSplit

# 定义一个函数绘制学习曲线
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, 
                       n_jobs=1, train_sizes=np.linspace(0.1, 1.0, 5)):
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    # xlab
    plt.xlabel("Traning examples")
    # ylab
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean=np.mean(train_scores, axis=1)
    test_scores_mean=np.mean(test_scores, axis=1)
    plt.grid()
    
    plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label="Cross-validation score")
    
    plt.legend(loc="lower right")
    return plt

# setting
title="Learning Curves (Naive Bayes)"
cv=ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
estimator=GaussianNB()
plot_learning_curve(estimator, title, X, y, ylim=(0.9, 1.01), cv=cv, n_jobs=4)
plt.show()


# 可见，随着样本量的增大，训练集打分逐渐降低，因为要拟合的信息越来越多。
# 而测试集打分基本不变，说明高斯NB在预测方面，对样本量的要求没那么苛刻。如果样本量少，可以考虑NB建模。










========================================
Tree & RF 决策树 与 随机森林:
----------------------------------------

概要 
- 决定册数的与基本原理和构造
- 决策树的优势与不足
- 随机森林的基本原理和构造
- 随机森林的优势和不足
- 实战







========================================
|-- 决策树
----------------------------------------

1. 决策树的构建（最大决策深度=1）

import numpy as np
# 画图工具
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# 导入 tree 模型和数据集加载工具
from sklearn import tree, datasets
# 导入拆分工具
from sklearn.model_selection import train_test_split

wine=datasets.load_wine()
# 只选取数据集的前2个特征,为了图形方便展示
X=wine.data[:, :2]
y=wine.target
# 将数据集拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)

# 设定决策树分类器的最大深度为1
clf=tree.DecisionTreeClassifier(max_depth=1)
# 拟合
clf.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(clf.score(X_train, y_train)) )
print("testing score: {:.3f}".format(clf.score(X_test, y_test)) )

# 最关键的参数就是 max_depth，就是问问题的数量，只能回答yes / no.
# 问的问题越多，表示决策树的深度越深。

输出
training score: 0.692
testing score: 0.622


# 可视化

# 定义图像中分区的颜色和散点的颜色
cmap_light=ListedColormap(["#FFAAAA", "#AAFFAA", "#AAAAFF"])
cmap_bold=ListedColormap(["#FF0000", "#00FF00", "#0000FF"])

# 分别用样本的2个特征创建图形和x/y轴
x_min, x_max= X_train[:, 0].min()-1,  X_train[:, 0].max()+1
y_min, y_max= X_train[:, 1].min()-1,  X_train[:, 1].max()+1

xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02), 
                  np.arange(y_min, y_max, 0.02))
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])

# 给每个分类中的样本分配不同的颜色
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 样本散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: tree( max_depth = 1)")
plt.show()


只分2类，分类效果不好，不到 70%。





2. max_depth=3

# 尝试加大深度 3
clf2=tree.DecisionTreeClassifier(max_depth=3)
# 拟合
clf2.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(clf2.score(X_train, y_train)) )
print("testing score: {:.3f}".format(clf2.score(X_test, y_test)) )

输出：
training score: 0.887
testing score: 0.822


# 可视化
Z=clf2.predict(np.c_[xx.ravel(), yy.ravel()])

# 给每个分类中的样本分配不同的颜色
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 样本散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: tree( max_depth = 3)")
plt.show()





3. max_depth=5

# 尝试加大深度 5
clf3=tree.DecisionTreeClassifier(max_depth=5)
# 拟合
clf3.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(clf3.score(X_train, y_train)) )
print("testing score: {:.3f}".format(clf3.score(X_test, y_test)) )

# 出现过拟合倾向了，就是训练集效果远好于测试集。
输出 
training score: 0.925
testing score: 0.778




# 可视化
Z=clf3.predict(np.c_[xx.ravel(), yy.ravel()])

# 给每个分类中的样本分配不同的颜色
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 样本散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: tree( max_depth = 5)")
plt.show()





4. 决策树的可视化演示
$ pip3 install graphviz -i https://pypi.douban.com/simple/


import graphviz
from sklearn.tree import export_graphviz

# 选择 max_depth=3 的决策树进行可视化
# 输出到文件
export_graphviz(clf2, out_file="wine.dot", class_names=wine.target_names,
               feature_names=wine.feature_names[:2], impurity=False, filled=True)

# 读文件
with open("wine.dot") as f:
    dot_graph=f.read()
# 可视化
graphviz.Source(dot_graph)

# 这种层级关系非常方便向非专业人士解释算法是如何工作的。





5. max_depth与打分曲线

def getScore(depth):
    # 尝试加大深度 5
    clf=tree.DecisionTreeClassifier(max_depth=depth)
    # 拟合
    clf.fit(X_train, y_train)

    # 输出打分
    return [clf.score(X_train, y_train), clf.score(X_test, y_test)]

scores=[]
for i in range(1, 10):
    scores.append( getScore(i) )

scores=np.array(scores)
scores


# 画图
plt.plot(scores[:,0], label="trainning score")
plt.plot(scores[:,1], label="testing score")
plt.xlabel("Max_depth")
plt.ylabel("Score")
plt.legend()
plt.show()

# 可见max_demth超过2就过拟合了。



决策树的优点是：直观，方便解释；不需要对数据预处理；
缺点是：即使使用 max_depth 或者 max_leaf_nodes 等参数对决策树进行预剪枝处理，还时不可避免会过拟合，导致模型的泛化能力大打折扣。





========================================
|-- 随机森林Random Forests (集成学习方法：避免过拟合问题)
----------------------------------------

随机森林也称为 随机决策森林，是一种集合学习方法，既可用于分类，也可用于回归。

集合算法，包括 随机森林(Random Forests)和梯度上升决策树(Gradient Boosted Decision Tree, GBDT)。

随机森林是把不同的几棵决策树打包到一起，每棵树的参数都不相同，然后我们取每棵树预测结果的平均值。
这样既可以保留决策树们的工作成效，又可以降低过拟合的风险。
可以用数学公式推导。略。

1. 继续使用 wine 数据集 

# 导入随机森林分类器
from sklearn.ensemble import RandomForestClassifier
# 导入拆分工具
from sklearn.model_selection import train_test_split

from sklearn import tree, datasets
wine=datasets.load_wine()
# 只选取数据集的前2个特征,为了图形方便展示
X=wine.data[:, :2]
y=wine.target
# 将数据集拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)

# 设定随机森林有6棵树
forest=RandomForestClassifier(n_estimators=6, random_state=3)
# 拟合
forest.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(forest.score(X_train, y_train)) )
print("testing score: {:.3f}".format(forest.score(X_test, y_test)) )

# 测试集的结果比训练集明显差，已经过拟合了。

输出

training score: 0.977
testing score: 0.778


# help(RandomForestClassifier) 
# bootstrap=True 是一个重要的参数，也是默认值。
#   每棵树都是随机的样本，而每棵树也会选择不同的特征，保证每棵树都是不同的。

# max_feature 也是一个重要的参数，默认是 auto=sqrt(特征数量)。太少则每棵树差异太大，太大则每棵树基本都一样。
# n_estimators 是决策树的数量。这些树的概率投票决定着随机森林的输出。




(2). 可视化
## 可视化随机森林
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# 定义图像中分区的颜色和散点的颜色
cmap_light=ListedColormap(["#FFAAAA", "#AAFFAA", "#AAAAFF"])
cmap_bold=ListedColormap(["#FF0000", "#00FF00", "#0000FF"])

# 分别用样本的2个特征创建图形和x/y轴
x_min, x_max= X_train[:, 0].min()-1,  X_train[:, 0].max()+1
y_min, y_max= X_train[:, 1].min()-1,  X_train[:, 1].max()+1

xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02), 
                  np.arange(y_min, y_max, 0.02))
Z=forest.predict(np.c_[xx.ravel(), yy.ravel()])

# 给每个分类中的样本分配不同的颜色
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 样本散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: RandomForestClassifier")
plt.show()

# 结果更细腻了。
# 可以调节 n_estimator 参数和 random_state 参数，看分类器的表现怎么变化。





2. n_estimator与打分曲线

def getScore(n_est):
    forest=RandomForestClassifier(n_estimators=n_est, random_state=3)
    # 拟合
    forest.fit(X_train, y_train)
    # 输出打分
    return [forest.score(X_train, y_train), forest.score(X_test, y_test)]

scores=[]
for i in range(1, 30):
    scores.append( getScore(i) )

scores=np.array(scores)

# 画图
plt.plot(scores[:,0], label="trainning score")
plt.plot(scores[:,1], label="testing score")
plt.xlabel("n_estimator")
plt.ylabel("Score")
plt.legend()
plt.show()

# n_estimator=17 就是打分极限了





3. 随机森林的优缺点

优点
	应用广泛；
	不需要用户在意参数的调节；
	不需要对数据预处理；
	支持并行处理，就是把 n_jobs 参数设置的<=CPU 核心数；如果设置为-1，则使用全部CPU核心。

缺点：
	向非专业人士展示不方便，优先使用决策树来展示
	对于超高维数据、稀疏数据集等捉襟见肘。这种情况下，线性模型要比随机森林的表现更好一些。
	消耗内存，速度也比线性模型慢。





========================================
|-- 决策树实战 - 预测收入
----------------------------------------

1. 数据集 
简介
https://archive.ics.uci.edu/ml/datasets/Adult

下载 csv文件 
https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data
https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names

(1) 下载数据
import os
os.getcwd()


import pandas as pd
data=pd.read_csv("data/adult.data", header=None, index_col=False,
                names=["age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", 
                       "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"])
# 为了方便展示，选取部分列
data_lite=data[ ["age", "workclass","education","sex", "hours-per-week", "occupation", "income"] ]

print(data.shape)
print(data_lite.shape)

data_lite.head()

输出 
(32561, 15)
(32561, 7)



(2) 数据预处理: 分类变量 to 哑变量

import numpy as np
data_lite.loc[:,"workclass"].unique()
# ？ 表示缺失值

#array([' State-gov', ' Self-emp-not-inc', ' Private', ' Federal-gov',
#       ' Local-gov', ' ?', ' Self-emp-inc', ' Without-pay',
#       ' Never-worked'], dtype=object)



# 使用 get_dummies 处理数据，把分类变量变为 0/1 数值型的。
data_dummies=pd.get_dummies(data_lite)

print("样本原始特征:\n", list(data_lite.columns), "\n" )
print("虚拟变量特征:\n", list(data_dummies.columns) )
print( data_dummies.shape)

data_dummies.head()
#可见原来的7列已经扩展成 46 列了。
# 输出略。




## 把数据分配给X和y
features=data_dummies.loc[:, 'age':'occupation_ Transport-moving']
X=features.values
# 将收入大于50k作为预测目标
y=data_dummies["income_ >50K"].values

# 维度
print(X.shape, y.shape)


(3) 建模
# 导入拆分工具
from sklearn.model_selection import train_test_split
# 将数据集拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

############
from sklearn import tree
# 设定决策树分类器的最大深度为5
clf=tree.DecisionTreeClassifier(max_depth=5)
# 拟合
clf.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(clf.score(X_train, y_train)) )
print("testing score: {:.3f}".format(clf.score(X_test, y_test)) )
print()


############
# 导入随机森林分类器
from sklearn.ensemble import RandomForestClassifier
# 设定随机森林有6棵树
rfc=RandomForestClassifier(n_estimators=7, random_state=3)
# 拟合
rfc.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(rfc.score(X_train, y_train)) )
print("testing score: {:.3f}".format(rfc.score(X_test, y_test)) )

输出：
training score: 0.803
testing score: 0.796

training score: 0.928
testing score: 0.784

决策树打分0.80，随机森林打分 0.78. 凑合能用吧。做出一个预测，有 80% 的正确率。





========================================
----------------------------------------





========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------





========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------



