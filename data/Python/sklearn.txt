sklearn 笔记



========================================
sklearn 简介
----------------------------------------
1. 网站
https://scikit-learn.org/stable/
https://github.com/scikit-learn/scikit-learn

scikit-learn: machine learning in Python


Simple and efficient tools for predictive data analysis
Accessible to everybody, and reusable in various contexts
Built on NumPy, SciPy, and matplotlib
Open source, commercially usable - BSD license


六大类基本功能：
分类
回归
聚类
数据降维
模型选择
数据预处理





2. 十分钟上手sklearn：特征提取，常用模型，

1.PCA算法：主成分分析
2.LDA算法：线性评价分析
3.线性回归
4.逻辑回归
5.朴素贝叶斯
6.决策树
7.SVM
8.神经网络
9.KNN算法


(1) 安装

$ pip3 install scikit-learn -i https://pypi.douban.com/simple/

sklearn 与 scikit-learn 是同一个东西嘛？前置是后者的缩写。安装 假包sklearn 时会默认安装其 scikit-learn。




(2) 内置/玩具 数据集简介

UCI 数据集: 
	https://archive.ics.uci.edu/ml/index.php
	新版 https://archive-beta.ics.uci.edu/
	http://archive.ics.uci.edu/ml/machine-learning-databases/










========================================
torch, tensorflow, keras, sklearn, 的侧重点、使用习惯与发展趋势
----------------------------------------
业界偏爱tf，学界偏爱pytorch，后者的内存管理为人诟病，真到实际部署还得tf
	只要能用到gpu的我都会用pytorch
	TF做部署还是挺方便的
	TF有种一子落错，满盘皆输的趋势，这种趋势已经比较难改变了
keras现在又分离出来了，都在瞎折腾，还是pytorch稳定

tf与其说是一个框架不如说是dsl了[飙泪笑]学习曲线和python本身完全分离
小模型用keras，大模型用torch，总归不会错[机智]





========================================
KNN 最近邻算法: K-Nearest Neighbor
----------------------------------------

1. 原理

计算新点p0到已知分类的点pn的距离，最近的k个点中哪一类最多，那么新点p0就属于哪一类。

KNN也可用于回归：取最近的几个点的y坐标的平均值作为该点的预测值。


2. 优缺点 

KNN的优点: 简单，容易理解。
KNN的缺点：
	需要对数据认真的预处理
	对规模超大的数据集拟合的时间较长
	对高维数据集拟合欠佳
	对于稀疏数据集束手无策




========================================
|-- KNN 二分类
----------------------------------------
2. 最简单实例: 已知二分类，预测一个新点的分类


(1) 训练集：生成已知标签的数据集

# 导入数据生成器
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

data=make_blobs(n_samples=200, centers=2, random_state=8)
X,y=data

plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k')
plt.show()

# 图略：上下两部分散点，上面紫色，下面黄色。


(2) 画出分类器
import numpy as np

# 导入KNN分类器
from sklearn.neighbors import KNeighborsClassifier

clf=KNeighborsClassifier()
clf.fit(X, y)

# 画网格背景
x_min, x_max = X[:, 0].min() -1, X[:,0].max()+1
y_min, y_max = X[:, 1].min() -1, X[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
# 对区域的每一点进行预测，作为颜色值
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)

# 画散点图
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto') # 带颜色的网格, 第三个参数是颜色
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k') #散点图
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.show()

图略:
# 创建分类模型，由上部粉色区域和下部灰色区域组成。
# 如果有新数据，落到哪里就是哪个分类了。



(3) 画出新数据点 6.75, 4.82
# 在分类模型上画出该点：在 plt.show() 前加入这句: plt.scatter(6.75, 4.82, marker="*", c="red", s=200)

# 画网格背景
x_min, x_max = X[:, 0].min() -1, X[:,0].max()+1
y_min, y_max = X[:, 1].min() -1, X[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
# 对区域的每一点进行预测，作为颜色值
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)

# 画散点图
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto') # 带颜色的网格, 第三个参数是颜色
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k') #散点图
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.scatter(6.75, 4.82, marker="*", c="red", s=200) #画出新的数据点
plt.show()

图略：新点是一个红色五角星。
# 从图中看，新点落在下方灰色区域中。



(4) 带入模型验证一次。
clf.predict([ [ 6.75, 4.82] ]) #确实归为第1类
# array([1])


print(clf.predict([ [6, 10] ])) #上方点 0
print(clf.predict([ [6, -0.29] ])) #下方点 1



========================================
|-- KNN 多分类
----------------------------------------

(1) 生成已知标签的数据集：500个点，分为5类
# 导入数据生成器
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# 修改 make_blobs 的 center 参数，分类数提高到5个
# 修改 n_samples 参数，把样本量也增加到 500个
data2=make_blobs(n_samples=500, centers=5, random_state=8)
X2,y2=data2

# 画散点图
plt.scatter(X2[:,0], X2[:,1], c=y2, cmap=plt.cm.spring, edgecolor='k')
plt.show()

#图略：5类中的2类有重叠部分。重叠部分一般不好区分，是分类错误最多的区域，。



(2) 使用KNN建模
import numpy as np

# 导入KNN分类器
from sklearn.neighbors import KNeighborsClassifier

clf=KNeighborsClassifier()
clf.fit(X2, y2)


# 画图
x_min, x_max = X2[:, 0].min() -1, X2[:,0].max()+1
y_min, y_max = X2[:, 1].min() -1, X2[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto')

plt.scatter(X2[:,0], X2[:,1], c=y2, cmap=plt.cm.spring, edgecolor='k')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.show()

# 建立5个分区，大部分是正确分类的，重合区域、边界附近有少部分点是错误分类的。
#耗时比上一次多了很多。


(3) 输出在训练集中的正确率
clf.score(X2, y2) #0.956







========================================
|-- KNN 用于回归分析
----------------------------------------
KNN回归的原理: 对x轴进行遍历，取距离最近的几个点的y坐标的平均值作为预测值。

(1) 导入 make_regression 回归数据生成器
from sklearn.datasets import make_regression
# 生成特征数量为1，噪音为50的数据集
X,y=make_regression(n_features=1, n_informative=1, noise=50, random_state=8)

# 散点图
import matplotlib.pyplot as plt
plt.scatter(X,y, c="orange", edgecolor="k")
plt.show()

#图略： x范围+-3， y范围+-250，倾斜45度角的散点



(2) 建立KNN回归模型
# 导入用于回归分析的KNN模型
from sklearn.neighbors import KNeighborsRegressor
reg=KNeighborsRegressor()
# 用KNN模型拟合数据
reg.fit(X,y)

# 可视化
import numpy as np
z=np.linspace(-3,3, 200).reshape(-1,1)
plt.scatter(X,y,c="orange", edgecolor="k")
plt.plot(z, reg.predict(z), c="k", linewidth=3)
plt.title("KNN Regressor")
plt.show()
# 图略: 黑色表示KNN回归生成的模型。直观看，效果不好，大量的数据点没有被模型覆盖。


# 给模型评分
reg.score(X,y) #0.772


(3) 怎么提高模型打分？

# 调整 n_neighbors，默认5，我们减少该值
reg2=KNeighborsRegressor(n_neighbors=2)
reg2.fit(X,y)

# 再次可视化
import numpy as np
z=np.linspace(-3,3, 200).reshape(-1,1)
plt.scatter(X,y,c="orange", edgecolor="k")
plt.plot(z, reg2.predict(z), c="k", linewidth=3)
plt.title("KNN Regressor: n_neighbors=2")
plt.show()
# 图略：黑色曲线覆盖了更多的点，也就是说，模型变复杂了。


# 再次给模型评分
reg2.score(X,y) #0.858
# 打分确实提高了，0.77->0.86








========================================
|-- KNN 分类真实案例：酒的分级
----------------------------------------
假设我们对酒的品质一无所知，现在已知一个酒的各项参数，让给出分级，怎么做？

(1) 载入酒的数据
from sklearn.datasets import load_wine
wine_dataset=load_wine()
print(type(wine_dataset)) #这是一个很复杂的格式 <class 'sklearn.utils.Bunch'>


# 尝试了解该数据集
wine_dataset
# 略

wine_dataset.keys() #dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])
# 数据 data，分类目标 target, 分类名字 target_names，数据描述 DESCR，特征变量的名字 feature_names

wine_dataset.data.shape #(178, 13)  178行 样本，13列 特征变量

print(wine_dataset.DESCR) # 略
#可见共3个分类，class_0-2。
# 13个变量分别是: 酒精含量、苹果酸、色彩表合度等。




(2) 分拆成训练集和测试集
# train_test_split 函数，默认随机分组，75%的归为训练集，25%归为测试集。

# 一般使用X表示数据特征，y表示对应的标签。因为X是二维的数组，也称为矩阵，y是一维数组，也叫向量。

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split

# 拆分数据。random_state 随机数种子。当设置为0或者缺省时，每次生成的随机数都不同。
X_train, X_test, y_train, y_test=train_test_split(
    wine_dataset["data"], wine_dataset["target"], random_state=8
)
# 检查数据 行列 数
print("X_train", X_train.shape)
print("X_test", X_test.shape)
print("y_train", y_train.shape)
print("y_test", y_test.shape)

输出 
X_train (133, 13)
X_test (45, 13)
y_train (133,)
y_test (45,)


(3) 建模
# 导入 KNN 分类模型
from sklearn.neighbors import KNeighborsClassifier
# 指定模型的 n_neighbors 参数为1
knn=KNeighborsClassifier(n_neighbors=1) #最近的k=1个已知点

# fit: 用模型对数据进行拟合
knn.fit(X_train, y_train)
# KNeighborsClassifier(n_neighbors=1)

# 使用测试集检验模型
print(knn.score(X_train, y_train)) #1.0 对训练集全对
knn.score(X_test, y_test) #0.7111 测试集 71% 正确率


(4) 使用模型对新数据进行预测
import numpy as np
X_new=np.array([ [ 13.2, 2.77, 2.51, 18.5,96.6,1.04,2.55,0.57,1.47,6.2,1.05,3.33,820] ] )
print(X_new.shape) #(1, 13)

#预测
prediction=knn.predict(X_new)
print(prediction, wine_dataset["target_names"][prediction] )

输出：
(1, 13)
[2] ['class_2']



小结：测试集上的正确率确实有点低。尝试看看怎么优化。
不过，作为入门级的方法，主要用于理解整体建模套路。






========================================
glm 广义线性模型: 适合于高维数据
----------------------------------------

1. 线性模型是一类广泛应用于机器学习领域的预测模型。
使用输入数据集的特征的线性函数进行建模，并对结果进行预测。


2. 概要
线性模型的基本概念
线性回归模型
岭回归模型
套索回归模型 Lasso
二元分类器中的逻辑回归和线性SVC模型







========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------



