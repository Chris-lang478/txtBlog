MachineLearning-01


TensorFlow https://www.tensorflow.org/tutorials/

Darcula主题

========================================
1.SVM, Support Vector Machine
----------------------------------------
1.Understanding Support Vector Machine algorithm from examples (along with code)
https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/

中文翻译参考 公众号：关于支持向量机相关知识汇集，by 无人机  2016-08-30



========================================
2. logistics 回归
----------------------------------------

1. 【典藏】Logistic 回归：从入门到进阶
http://www.360doc.com/content/15/1024/07/22609018_507952382.shtml

【独家】一文读懂回归分析
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml



========================================
一文读懂回归分析:概述Cox回归、岭回归、Lasso回归、ElasticNet 回归
----------------------------------------
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml

生物学中的基因很多是有共线性的，所以，后三种方法经常使用。
回归正则化方法（套索，岭和ElasticNet）在高维数据和数据集变量之间存在多重共线性的情况下运行良好。

3）Cox回归
Cox回归的因变量就有些特殊，它不经考虑结果而且考虑结果出现时间的回归模型。它用一个或多个自变量预测一个事件（死亡、失败或旧病复发）发生的时间。Cox回归的主要作用发现风险因素并用于探讨风险因素的强弱。但它的因变量必须同时有2个，一个代表状态，必须是分类变量，一个代表时间，应该是连续变量。只有同时具有这两个变量，才能用Cox回归分析。Cox回归主要用于生存资料的分析，生存资料至少有两个结局变量，一是死亡状态，是活着还是死亡；二是死亡时间，如果死亡，什么时间死亡？如果活着，从开始观察到结束时有多久了？所以有了这两个变量，就可以考虑用Cox回归分析。

9）岭回归
当数据之间存在多重共线性（自变量高度相关）时，就需要使用岭回归分析。在存在多重共线性时，尽管最小二乘法（OLS）测得的估计值不存在偏差，它们的方差也会很大，从而使得观测值与真实值相差甚远。岭回归通过给回归估计值添加一个偏差值，来降低标准误差。

岭回归要点：
1）除常数项以外，岭回归的假设与最小二乘回归相同；
2） 它收缩了相关系数的值，但没有达到零，这表明它不具有特征选择功能；
3）这是一个正则化方法，并且使用的是 L2 正则化。


13）套索回归 LASSO
与岭回归类似，套索也会对回归系数的绝对值添加一个罚值。此外，它能降低偏差并提高线性回归模型的精度。看看下面的等式：

套索回归要点：
1）除常数项以外，这种回归的假设与最小二乘回归类似；
2）它将收缩系数缩减至零（等于零），这确实有助于特征选择；
3）这是一个正则化方法，使用的是 L1 正则化；
4）如果一组预测因子是高度相关的，套索回归会选出其中一个因子并且将其它因子收缩为零。


14）ElasticNet 回归
ElasticNet 回归是套索回归和岭回归的组合体。它会事先使用 L1 和 L2 作为正则化矩阵进行训练。当存在多个相关的特征时，Elastic-net 会很有用。岭回归一般会随机选择其中一个特征，而 Elastic-net 则会选择其中的两个。同时包含岭回归和套索回归的一个切实的优点是，ElasticNet 回归可以在循环状态下继承岭回归的一些稳定性。

ElasticNet 回归要点：
1）在高度相关变量的情况下，它会产生群体效应；
2）选择变量的数目没有限制；
3）它可以承受双重收缩。







========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


