MachineLearning-01


TensorFlow https://www.tensorflow.org/tutorials/

Darcula主题

========================================
概率
----------------------------------------
1. https://github.com/DawnEve/ML_MachineLearning

2. 公众号： 
机器学习算法清单！附Python和R代码 <数据派THU  3月8日





========================================
参考资源: 知名机器学习与AI课程
----------------------------------------
1.课程
fast.ai(http://fast.ai/) ：它针对程序员提供了两个很不错的关于深度学习的课程，以及一个关于可计算线性代数的课程。是开始编写神经网络代码的好地方，随着课程深度的延伸，当你学到更多理论的时候，你可以尽快用代码实现。

neuralnetworksanddeeplearning.com（http://neuralnetworksanddeeplearning.com/chap1.html）：一本关于基本知识的很好的在线书籍。关于神经网络背后的理论。作者以一种很好的方式解释了你需要知道的数学知识。它也提供并解释了一些不使用任何深度学习框架从零开始编写神经网络架构的代码。

Andrew Ng 的深度学习课程（https://www.coursera.org/specializations/deep-learning）：coursera 上的课程，也是有关学习神经网络的。以非常简单的神经网络例子开始，逐步到卷积神经网络以及更多。

3Blue1Brown（https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw）：YouTube 上也有一些能够帮助你理解神经网络和线性代数的很好的视频。它们展示了很棒的可视化形式，以及以非常直觉的方式去理解数学和神经网络。

Stanford CS231 课程（http://cs231n.stanford.edu/）：这是关于用于视觉识别的卷积神经网络的课堂，可以学到很多关于深度学习和卷积神经网络的具体内容。



2.书
https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/model_optimization.html




========================================
1.SVM, Support Vector Machine
----------------------------------------
1.Understanding Support Vector Machine algorithm from examples (along with code)
https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/

中文翻译参考 公众号：关于支持向量机相关知识汇集，by 无人机  2016-08-30



========================================
2. logistics 回归
----------------------------------------

1. 【典藏】Logistic 回归：从入门到进阶
http://www.360doc.com/content/15/1024/07/22609018_507952382.shtml

【独家】一文读懂回归分析
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml



========================================
一文读懂回归分析:概述Cox回归、岭回归、Lasso回归、ElasticNet 回归
----------------------------------------
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml

生物学中的基因很多是有共线性的，所以，后三种方法经常使用。
回归正则化方法（套索，岭和ElasticNet）在高维数据和数据集变量之间存在多重共线性的情况下运行良好。

3）Cox回归
Cox回归的因变量就有些特殊，它不经考虑结果而且考虑结果出现时间的回归模型。它用一个或多个自变量预测一个事件（死亡、失败或旧病复发）发生的时间。Cox回归的主要作用发现风险因素并用于探讨风险因素的强弱。但它的因变量必须同时有2个，一个代表状态，必须是分类变量，一个代表时间，应该是连续变量。只有同时具有这两个变量，才能用Cox回归分析。Cox回归主要用于生存资料的分析，生存资料至少有两个结局变量，一是死亡状态，是活着还是死亡；二是死亡时间，如果死亡，什么时间死亡？如果活着，从开始观察到结束时有多久了？所以有了这两个变量，就可以考虑用Cox回归分析。

9）岭回归
当数据之间存在多重共线性（自变量高度相关）时，就需要使用岭回归分析。在存在多重共线性时，尽管最小二乘法（OLS）测得的估计值不存在偏差，它们的方差也会很大，从而使得观测值与真实值相差甚远。岭回归通过给回归估计值添加一个偏差值，来降低标准误差。

岭回归要点：
1）除常数项以外，岭回归的假设与最小二乘回归相同；
2） 它收缩了相关系数的值，但没有达到零，这表明它不具有特征选择功能；
3）这是一个正则化方法，并且使用的是 L2 正则化。


13）套索回归 LASSO
与岭回归类似，套索也会对回归系数的绝对值添加一个罚值。此外，它能降低偏差并提高线性回归模型的精度。看看下面的等式：

套索回归要点：
1）除常数项以外，这种回归的假设与最小二乘回归类似；
2）它将收缩系数缩减至零（等于零），这确实有助于特征选择；
3）这是一个正则化方法，使用的是 L1 正则化；
4）如果一组预测因子是高度相关的，套索回归会选出其中一个因子并且将其它因子收缩为零。


14）ElasticNet 回归
ElasticNet 回归是套索回归和岭回归的组合体。它会事先使用 L1 和 L2 作为正则化矩阵进行训练。当存在多个相关的特征时，Elastic-net 会很有用。岭回归一般会随机选择其中一个特征，而 Elastic-net 则会选择其中的两个。同时包含岭回归和套索回归的一个切实的优点是，ElasticNet 回归可以在循环状态下继承岭回归的一些稳定性。

ElasticNet 回归要点：
1）在高度相关变量的情况下，它会产生群体效应；
2）选择变量的数目没有限制；
3）它可以承受双重收缩。







========================================
用R对数据进行随机抽样
----------------------------------------
#去掉一类，变成两类
x=iris[which(iris$Species!="virginica"),];
x$Species=factor(x$Species)
str(x);dim(x)

#生成抽样序列
set.seed(100)
ind=sample(2,nrow(x),replace=TRUE,prob=c(0.8,0.2));ind

#取80%作为训练集，20为测试集
x_train=x[ind==1,];dim(x_train)
x_test=x[ind==2,];dim(x_test) #去掉标签



========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


