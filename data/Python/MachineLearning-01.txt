MachineLearning-01


TensorFlow https://www.tensorflow.org/tutorials/
Darcula主题


前置知识
	1.机器学习的扎实基础，熟悉深度学习。
	2.数学：熟练线性代数和概率论（很重要）。
	3.编程：Python、PyTorch 和 NumPy。
	3.很多课程中使用英语，因此学生应该熟悉技术英语。
#





========================================
代码与实例
----------------------------------------
1. https://github.com/DawnEve/ML_MachineLearning

2. 公众号： 
机器学习算法清单！附Python和R代码 <数据派THU  3月8日



3. 慕课网资源
序号	课程名称	网址
1	Python爬虫工程师养成计划（套餐923.1元）	https://coding.imooc.com/learningpath/route?pathId=23
2	spark从零开始（免费）	https://www.imooc.com/learn/814
3	大数据入门到spark信息处理（套餐1399元）	https://order.imooc.com/pay/confirm/goods_ids/6-883
4	初识机器学习-理论篇（免费）	https://www.imooc.com/learn/717
5	Python实现机器学习（免费）	https://www.imooc.com/learn/1174
6	推荐算法理论与实践（免费）	https://www.imooc.com/learn/990
7	神经网络简介（免费）	https://www.imooc.com/learn/930
8	人工智能学习路线5门课（Step3-5: 1159.4元）	https://coding.imooc.com/learningpath/route?pathId=28





========================================
参考资源: 知名机器学习与AI课程
----------------------------------------

本文依赖的
# 电子书
https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/model_optimization.html
Documentation on all topics that I learn on both Artificial intelligence and machine learning.
Topics Covered:
	Artificial Intelligence Concepts
	Search
	Decision Theory
	Reinforcement Learning
	Artificial Neural Networks
	Back-propagation
	Feature Extraction
	Deep Learning
	Convolutional Neural Networks
	Deep Reinforcement Learning
	Distributed Learning
	Python/Matlab deep learning library
#
进度
/linear_algebra.html



# 视频 【白板推导系列】【合集 1～23】
https://www.bilibili.com/video/BV1aE411o7qd?p=1
进度
高斯分布 3,4,


# 白话西瓜书 视频 
https://www.bilibili.com/video/BV17J411C7zZ?p=2



1. 流派
频率派 - 统计机器学习
贝叶斯派 - 概率图模型




2. 书
(1) 李航 统计学习方法
感知机
k聚类
朴素贝叶斯
决策树
逻辑回归

支持向量机SVM
提?
EM算法
隐马尔科夫
系?

(2) 周志华 西瓜书

(3) PRML《Pattern Recognition and Machine Learning》
贝叶斯角度
回 
分
神
核
稀疏矩阵

图 
混合
近似算法
采样
连续采样
随机森林

(4) MLAPP 
百科全书式的，贝叶斯派

(5) ESL
频率派

(6) Deep Learning 圣经
中译版 - 张志华团队





3. 视频
(1) 台大 林轩田 
基石，理论部分很精彩、通俗化 
	VC theory;
	正则化;
	线性模型

技法
	SVM
	
(2) 张志华
机器学习导论 (概率角度)
统计机器学习 (贝叶斯)

(3) NG: CS229 斯坦福课堂录像，有很多推导;

(4) 徐亦达 - 概率模型
深度很深，github上有note

(5) 台大 李宏毅
ML2017
MLDS 2018

https://www.bilibili.com/video/BV13x411v7US?p=1



(6) 【白板推导系列】【合集 1～23】
https://www.bilibili.com/video/BV1aE411o7qd?p=1

白板笔记 
https://www.yuque.com/books/share/f4031f65-70c1-4909-ba01-c47c31398466?#
https://github.com/zhulei227/ML_Notes








4.课程
fast.ai(http://fast.ai/) ：它针对程序员提供了两个很不错的关于深度学习的课程，以及一个关于可计算线性代数的课程。是开始编写神经网络代码的好地方，随着课程深度的延伸，当你学到更多理论的时候，你可以尽快用代码实现。

neuralnetworksanddeeplearning.com（http://neuralnetworksanddeeplearning.com/chap1.html）：一本关于基本知识的很好的在线书籍。关于神经网络背后的理论。作者以一种很好的方式解释了你需要知道的数学知识。它也提供并解释了一些不使用任何深度学习框架从零开始编写神经网络架构的代码。

Andrew Ng 的深度学习课程（https://www.coursera.org/specializations/deep-learning）：coursera 上的课程，也是有关学习神经网络的。以非常简单的神经网络例子开始，逐步到卷积神经网络以及更多。

3Blue1Brown（https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw）：YouTube 上也有一些能够帮助你理解神经网络和线性代数的很好的视频。它们展示了很棒的可视化形式，以及以非常直觉的方式去理解数学和神经网络。

Stanford CS231 课程（http://cs231n.stanford.edu/）：这是关于用于视觉识别的卷积神经网络的课堂，可以学到很多关于深度学习和卷积神经网络的具体内容。





5. 领域
Speech and natural language processing
Face Recognition
Image Classification, object detection
Car Driving
Playing complex games (Alpha Go)
Control strategies (Control engineering)





6. 分类
(1)有监督、无监督；
(2)有监督的实例 Supervised Learning
classification: 分类，if a tumour is benign or malignent

Regression: 回归，预测下个月的销量;

(3) 无监督的实例 Unsupervised Learning
Clustering: You ask the computer to separate similar data into clusters, this is essential in research and science.

High Dimension Visualisation: Use the computer to help us visualise high dimension data.

Generative Models: After a model captures the probability distribution of your input data, it will be able to generate more data. This can be very useful to make your classifier more robust.





7. 代码实例
(1) 100天机器学习代码 https://github.com/Avik-Jain/100-Days-Of-ML-Code





========================================
|-- 扫盲: 名词、知识点
----------------------------------------
(1)argmax是一种函数，是对函数求参数(集合)的函数。当我们有另一个函数y=f(x)时，若有结果x0= argmax(f(x))，则表示当函数f(x)取x=x0的时候，得到f(x)取值范围的最大值；若有多个点使得f(x)取得相同的最大值，那么argmax(f(x))的结果就是一个点集。

换句话说，argmax(f(x))是使得 f(x)取得最大值所对应的变量点x(或x的集合)。arg即argument，此处意为“自变量”。


(2) 如何通俗易懂地解释「范数」？
https://zhuanlan.zhihu.com/p/26884695
https://blog.csdn.net/a493823882/article/details/80569888

我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。

l0范数，l1范数-正则项与稀疏解;


(3) 什么是正则化、正则项？
带有L1正则化的回归模型通常被称为Lasso Regression，带有L2正则化的回归模型通常被称为Ridge Regression。
https://zhuanlan.zhihu.com/p/35707975







========================================
统计的学派: 频率派Frequentist vs 贝叶斯派Bayesian
----------------------------------------
1. 概率论引入机器学习是很自然的事情

X: data 数据, X=(x1 x2 ... xn)^T, NxP维矩阵; x1是p维行向量 x1=(x11 x12 ... x1p);
theta: parameter 参数

x~p(x|theta) 是概率模型;


频率派: 
	认为theta是未知常量，虽未知但是不会变，X是随机变量r.v;
	最常用的方法是极大似然估计, Theta_MLE=arg max(logP(X|theta))
		P(X|theta)=连乘(i=1,N, P(xi|theta)); 两边同时取log，得 log(P(X|theta))=累加(i=1,N, logP(xi|theta));

贝叶斯派:
	认为 theta也是 r.v, theta~P(theta) 是先验知识;
	P(theta|X)=P(X|theta)*P(theta)/P(X), 其中 P(theta|X)叫后验概率;
		P(X|theta)是最大似然, P(theta)是先验知识, 
		分母是 积分(对theta, P(X|theta)*P(theta) dtheta);
		
		p(theta) 叫先验概率 Prior probability是事件发生之前我们对theta事件的判断;
		P(theta|X) 叫后验概率 Posterior probability 是事件X发生后我们对theta事件的重新估算;
		系数 P(X|theta)/P(X) 称为"可能性函数"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。
		
#
	MAP(maximum a posteriori probability) 极大后验概率; 就是找到概率最大的点作为代替或估计; 
		也就是众数，P(X)和theta实际上没有关系，就是一个积分常量; P(theta|X)=~P(X|theta)*P(theta)
		Theta_MAP=arg max(P(theta|X))=arg max(P(X|theta)*P(theta));
		这其实不算太正统的贝叶斯
	贝叶斯估计, 就是实打实的求出来分母的那个积分，求出来后验概率P(theta|X)=P(X|theta)*P(theta)/P(X)
#
	贝叶斯预测BAYESIAN INFERENCE(贝叶斯推断): X, xN, 求P(xN | X)，中间引入已知条件theta作为已知数据X和新数据xN之间的桥梁，
		P(xN | X)=积分(p(xN,theta|X) dtheta)=积分( p(xN|theta)*p(theta|X) dtheta )
		可见，为了做预测，必须先求出后验概率p(theta|X)
#


2. 贝叶斯估计和极大似然估计在思想上有很大不同，代表着统计学中贝叶斯学派和频率学派对统计的不同认识。

	贝叶斯 引出 概率图模型，因为要求出所有已知条件下该事件发生的条件概率;
		本质就是求积分
		数值积分求不出来，可以使用蒙特卡罗估计MCMC(Monte Carlo Method)
		
		
	频率 引出 统计机器学习 
		本质是优化，是解loss function;
		1)先设计模型，概率模型、
		2)然后导出loss function
		3)梯度下降法等优化
#





############//补充

3.贝叶斯方法简介
(1)首先贝叶斯定理的基本形式为
	后验=似然度 x 先验/证据
	p(x|y)=p(y|x)p(x)/p(y)
#
推导: x和y事件同时发生的概率
p(xy)=p(y|x)p(x)
p(xy)=p(x|y)p(y)
也就是 p(y|x)p(x)=p(x|y)p(y)


全概率公式 
事件B发生的所有可能结果B1，B2，…，Bn，事件A发生的概率P(A)，则 p(A)=p(A,o)=累加(i=1,n, P(A,oi) )




(2)频率学派和贝叶斯学派的联系和区别：

频率学派不假设任何的先验知识，不参照过去的经验，只按照当前已有的数据进行概率推断。
而贝叶斯学派会假设先验知识的存在（猜测大象的重量），然后再用采样逐渐修改先验知识并逼近真实知识。

但实际上，在数据量趋近无穷时，频率学派和贝叶斯学派得到的结果是一样的，也就是说频率方法是贝叶斯方法的极限。



频率学派和贝叶斯学派的概率定义，频率学派认为模型是一成不变的，贝叶斯学派认为模型是随着数据的更新而不断更新，频率学派和贝叶斯学派都可以使用最大似然函数来估计模型。







例1: 抛硬币，已观测数据集为5次向上，求正面向上的概率w
1)频率派:
似然函数: P(D|w)=w^5;
最大似然函数 P(D|w)max=1;
所以 w^5=1; w=?;

硬币正面向上的概率为w，模型明显存在问题，称为过拟合

2) 贝叶斯派:
假设硬币正面向上的先验概率为p(w),根据贝叶斯定理得:
p(w|D)=p(D|w)p(w)/p(D)
最大化后验概率为 p(w|D)
w=p(w|D)max，w是正面向上的概率;
??




例2: Red盒子: 6橘子+2苹果; blue盒子: 1橘子+3苹果;
选择Red盒子的概率0.4，选择Blue盒子的概率是0.6，随机从一个盒子拿出一个水果
(1)求水果为橘子的概率;
(2)当水果为橘子时，求随机选择的是Red盒子的概率;
解:
(1)记作F为水果，F=o是橘子，F=a是苹果;
记作B是篮子，B=r是Red，B=b是Blue;
P(F=o)=P(F=o, B=r)+P(F=o, B=b) #全概率公式
=P(F=o|B=r)P(B=r) + P(F=o|B=b)P(B=b) #贝叶斯公式
=6/8*0.4+1/4*0.6=0.45;

(2)
P(B=r|F=o)=P(F=o|B=r)P(B=r)/p(F=o) #贝叶斯公式，带入上一问求到的P(F=o)
=6/8*0.4/0.45=2/3

由(2)可知，选择红色盒子概率为0.4，该概率为先验概率；
当观测数据为橘子时，选择红色盒子的概率变成0.67，该概率为后验概率。
再次证明了贝叶斯估计模型的概率是随着观测数据的变化而变化的。




例3:假设一个常规的检测结果的敏感度与可靠度均为99%，即病毒携带者每次检测呈阳性（+）的概率为99%。而非病毒携带者每次检测呈阴性（-）的概率为99%。
从检测结果的概率来看，检测结果是比较准确的，但是贝叶斯定理却可以揭示一个潜在的问题。
假设某小镇对全体住户进行病毒检测，已知0.5%的住户携带病毒。请问每位检测结果呈阳性的住户携带病毒的概率有多高？
解: 记 V为病毒携带事件{yes,no}; R为检测结果{+,-}

(1)P(R=+)=P(R=+, V=yes)+P(R=+, V=no) #全概率公式 
=P(+|yes)P(yes) + P(+|no)P(no) #贝叶斯公式
=0.99*0.5% + (1-0.99)*(1-0.5%)
=0.0149;

(2) P(V=yes|R=+)=P(R=+|V=yes)P(V=yes)/P(R=+)=0.99*0.5%/0.0149=0.332

(3)同时，我们可以计算一下假如一个人携带病毒，但他误检测成阴性的概率
P(yes|-)=P(-|yes)P(yes)/P(-)=(1-0.99)*0.5%/(1-0.0149)=0.00005075

可见，一个人带病毒但被误检测为阴性的概率只有0.005%，也就是说一个人如果检测为阴性，则基本可以判定他没有携带病毒。
但是一个人如果监测为阳性，则只有33%的概率确定他携带病毒。

很多医学监测当中的案例很相似，假阳性比假阴性更值得我们关注！









4.再次理解区别
1) 频率学派
高中数学对概率的定义：在大量重复进行同一实验事件A发生的频率总是接近某一个常数，并在它附近进行摆动，这时将这个常数叫事件A的概率，记作P(A)。

这是古典频率学派对概率的定义，定义包含了二个要点：
i)事件A发生的概率是常数。
ii)事件A发生的概率是重复多次进行同一实验得到的。

频率学派的局限性：
频率学派评估可重复实验事件发生的概率具有一定的现实意义。

但是假如评估本世纪末北极圈的冰川消失的概率，按照频率学派的思想，首先需要创造无数个平行世界，然后计算北极圈冰川消失的平行世界的频率，记该频率为冰川消失的概率。
目前，创造无数个平行世界的技术还不成熟，因此频率学派在评估不可重复实验事件发生的概率具有很大的限制性。


2) 贝叶斯学派
贝叶斯学派对概率的定义：贝叶斯学派评估事件A发生的概率带有主观性，且事件A发生的概率是当前观测数据集D下的概率，即条件概率P(A|D)，当观测数据集更新为D1时，则事件A发生的概率为P(A|D1)，不同的数据集预测A事件发生的概率不同。贝叶斯学派评估事件A发生的概率会引用先验概率和后验概率两个概念，贝叶斯定理是搭建先验概率和后验概率的桥梁。

定义包含了三个要点：
（I）、事件A发生的概率是变化的，并非常数。
（II）、事件A发生的概率是特定数据集下的条件概率。
（III）、事件A发生的概率是后验概率，且事件A发生的先验概率已给定。
贝叶斯学派的难点在于如何设置合理反映事件A发生的先验概率，不同的先验概率得到的结果不一样。






5.经典著作《人工智能：现代方法》的作者之一 Peter Norvig 曾经写过一篇介绍如何写一个拼写检查/纠正器的文章，详情戳这里。
http://norvig.com/spell-correct.html


贝叶斯推断可用于拼写纠正，实际上就是计算 P(我们猜测用户要输入的单词|用户实际输入的单词)

用T表示我们猜测用户输入的单词，用S表示用户实际输入的单词，那么就是求P(t|S) = P(S|t)P(t)/P(S) 的大小。

对于同一个单词，P(S)的概率是一样的，那么就等价于P(t|S)∝ P(S|t)×P(t)。  ∝是正比于，不是无穷大
那么要是的P(t|S)最大，就是使得P(S|t)×P(t)最大。

P(S|t)名义上是指我们猜测的单词t是用户真正想输入单词的概率，不同的单词概率不同，这就涉及到最大似然估计。例如用户输入的单词是thriw,这时throw跟thraw都有可能，但是你会想到，o跟i在键盘上很接近，用户可能要输的单词是throw的可能性比thraw的可能性大得多，根据最大似然估计找出最可能的单词。但是，有时候光有最大似然并不能完美的解决问题，我们还需要利用先验概率P(t)。

P(t)使我们猜测的单词出现的概率，这些单词t1、t2、t3....理论上有无穷种，但它是一种先验概率，对于单词来说，可能有点抽象。这里举一个分词的例子：

The girl saw the boy with a telescope.
如果仅用最大似然估计方法的话，可能会给出两种结果：1 The girl saw | the boy with a telescope    2.The girl saw the boy | with a telescope

但是根据我们的常识，一个女孩看着一个拿着望远镜的男孩？拿着望远镜有点莫名其妙，与“看”这个动作联系起来，那么最合适的解释恐怕是女孩拿着望远镜看那个男孩。那么得出这个结论，就是用到我们的先验知识，也就是P(t)。







ref:
浅谈频率学派和贝叶斯学派 https://blog.csdn.net/algorithmPro/article/details/83868827
深度学习贝叶斯，这是一份密集的6天速成课程（视频与PPT） https://baijiahao.baidu.com/s?id=1610925040333198359&wfr=spider&for=pc
拼写检查代码 http://norvig.com/spell-correct.html


//todo https://www.bilibili.com/video/BV1aE411o7qd?p=3







========================================
数学基础 概率 - 高斯分布(极大似然估计、有偏无偏、从概率密度观察)
----------------------------------------
Linear Gaussian Model;

1.
Data=(x1 x2 ... xn)^T, 每个xi是p维行向量，则data为 n行xp列 矩阵.

xi 独立同分布iid于 N(mu,sigma^2)
theta=(mu, sigma^2)
theta_MLE=arg max(theta, P(X|theat));


为了简便，简化，用p=1维的来证明
令p=1， theta=(mu, sigma^2)
p(x)=1/[sqrt(2pi)*sigma] * exp^(-(x-mu)^2/(2*sigma^2))

log(P(X|theta))=log 连乘(i=1, N, log(P(xi|theta)) );
然后求mu_MLE, 求偏导数，(过程见草稿纸)
(1)得到mu_MLE=1/N *累加(i=1,N, xi)，无偏估计;

同样方法，得到
(2)sigma^2_MLE=1/N *累加(i=1,n, (xi-mu)^2 ); 有偏估计
因为其数学期望不等于本身。



2. 估计量的数学期望是否等于真实值，等于则为无偏估计，否则为有偏估计;

方差的无偏估计量是 1/(N-1) *累加(i=1,n, (xi-mu)^2 ); 无偏的



3. 





========================================
1.SVM, Support Vector Machine
----------------------------------------
1.Understanding Support Vector Machine algorithm from examples (along with code)
https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/

中文翻译参考 公众号：关于支持向量机相关知识汇集，by 无人机  2016-08-30



========================================
2. logistics 回归
----------------------------------------

1. 【典藏】Logistic 回归：从入门到进阶
http://www.360doc.com/content/15/1024/07/22609018_507952382.shtml

【独家】一文读懂回归分析
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml




========================================
Self Organizing Maps (SOM): 一种基于神经网络的聚类算法
----------------------------------------
自组织映射神经网络， 即Self Organizing Maps (SOM)， 可以对数据进行无监督学习聚类。它的思想很简单，本质上是一种只有输入层--隐藏层的神经网络。隐藏层中的一个节点代表一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在隐藏层中找到一个和它最匹配的节点，称为它的激活节点，也叫“winning neuron”。 紧接着用随机梯度下降法更新激活节点的参数。同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。

所以，SOM的一个特点是，隐藏层的节点是有拓扑关系的。这个拓扑关系需要我们确定，如果想要一维的模型，那么隐藏节点依次连成一条线；如果想要二维的拓扑关系，那么就行成一个平面，如下图所示（也叫Kohonen Network）：

...
既然隐藏层是有拓扑关系的，所以我们也可以说，SOM可以把任意维度的输入离散化到一维或者二维(更高维度的不常见)的离散空间上。 Computation layer里面的节点与Input layer的节点是全连接的。

拓扑关系确定后，开始计算过程，大体分成几个部分：
1） 初始化：每个节点随机初始化自己的参数。每个节点的参数个数与Input的维度相同。
2）对于每一个输入数据，找到与它最相配的节点。假设输入时D维的， 即 X={x_i, i=1,...,D}，那么判别函数可以为欧几里得距离：
3)找到激活节点I(x)之后，我们也希望更新和它临近的节点。令S_ij表示节点i和j之间的距离，对于I(x)临近的节点，分配给它们一个更新权重：
简单地说，临近的节点根据距离的远近，更新程度要打折扣。
4）接着就是更新节点的参数了。按照梯度下降法更新：

迭代，直到收敛。


## 与K-Means的比较
同样是无监督的聚类方法，SOM与K-Means有什么不同呢？
（1）K-Means需要事先定下类的个数，也就是K的值。 SOM则不用，隐藏层中的某些节点可以没有任何输入数据属于它。所以，K-Means受初始化的影响要比较大。
（2）K-means为每个输入数据找到一个最相似的类后，只更新这个类的参数。SOM则会更新临近的节点。所以K-mean受noise data的影响比较大，SOM的准确性可能会比k-means低（因为也更新了临近节点）。
（3） SOM的可视化比较好。优雅的拓扑关系图 。


参考文献：http://www.cs.bham.ac.uk/~jxb/NN/l16.pdf
https://www.cnblogs.com/sylvanas2012/p/5117056.html



========================================
手写体识别与Tensorflow
----------------------------------------
1.如同所有语言的hello world一样，手写体识别就相当于深度学习里的hello world。

TensorFlow是当前最流行的机器学习框架，有了它，开发人工智能程序就像Java编程一样简单。


(1)设法使用 GPU
使用GPU能让训练加速10-100倍，已经不是秘密，这意味着你可以把迭代自己想法(是好是坏)的速度提高100倍。
实验做得越快，学到的知识就越多。


查看linux机器是否有GPU的简单命令:
$ lspci |grep -i nvidia
65:00.0 VGA compatible controller: NVIDIA Corporation GP106GL (rev a1)
65:00.1 Audio device: NVIDIA Corporation GP106 High Definition Audio Controller (rev a1)

结论: Ubuntu工作站有GPU，其他2台server没有GPU。


(2) 安装 GPU支持 //todo
https://tensorflow.google.cn/install/gpu






2. 训练集和测试集

MNIST 数据集已经是一个被”嚼烂”了的数据集, 很多教程都会对它”下手”, 几乎成为一个 “典范”. 不过有些人可能对它还不是很了解, 下面来介绍一下.

MNIST 数据集可在 http://yann.lecun.com/exdb/mnist/ 获取, 它包含了四个部分:

       Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)  
       Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签) 
       Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本) 
       Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)

MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST). 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据.

tensorflow提供一个input_data.py文件，专门用于下载mnist数据，我们直接调用就可以了，代码如下：

import tensorflow.examples.tutorials.mnist.input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
执行完成后，会在当前目录下新建一个文件夹MNIST_data

input_data文件会调用一个maybe_download函数，确保数据下载成功。这个函数还会判断数据是否已经下载，如果已经下载好了，就不再重复下载。


(1)### 报错：
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

### 环境：
Ubuntu 1804 64bit, 
$ python -V
Python 3.6.3 :: Anaconda, Inc.

$ pip list | grep flow
tensorflow-gpu                     1.10.1


2)重新安装包：
$ pip install --upgrade pip
$ pip -V
pip 19.2.1 from /home/wangjl/anaconda3/lib/python3.6/site-packages/pip (python 3.6)


#ERROR: Cannot uninstall 'wrapt'问题  https://www.cnblogs.com/xiaowei2092/p/11025155.html
$ pip install -U --ignore-installed wrapt enum34 simplejson netaddr

$ pip search tensorflow                                                                                                                           
tensorflow (1.14.0)     - TensorFlow is an open source machine learning framework for everyone.
$ pip install tensorflow  
安装成功 Successfully installed tensorflow-1.14.0

升级
$ pip install --upgrade tensorflow-gpu


3)
$ pip install --upgrade setuptools


(2) 再次尝试执行两行下载代码
还是那个报错 
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

1) 需要安装cuda？
CUDA（Compute Unified Device Architecture），是显卡厂商NVIDIA推出的运算平台。
放弃！！ 下次，换个环境继续： https://colab.research.google.com





(3)win10安装，数据加载成功
开始报错 module 'tensorflow.python.keras.backend' has no attribute 'get_graph'
重装
$ pip install keras==2.2.0 -i https://pypi.douban.com/simple/

import sys, numpy as np
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
print(len(x_train))
# 60000
ref:https://blog.csdn.net/qq_42823242/article/details/101717247


(4)CentOS上安装
https://www.cnblogs.com/conver/p/11141176.html
1).遇到了
ERROR: Cannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.
办法1：输入 pip install -U --ignore-installed wrapt enum34 simplejson netaddr -i https://pypi.douban.com/simple/

2).遇到了(我没遇到这个)
ERROR: tensorboard 1.14.0 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.
原因： setuptools 版本太低
办法：更新setuptools版本 输入 pip install --upgrade setuptools





https://www.cnblogs.com/skyme/p/8595642.html



========================================
|-- 卷积神经网络 (CNN)
----------------------------------------
图解：https://www.cnblogs.com/skyfsm/p/6790245.html
推荐书: Grokking deep learning 深度学习图解。

我的笔记： https://github.com/DawnEve/ML_MachineLearning/blob/master/ANN/Grokking_Deep_Learning_6_10.ipynb



1. 如何制作自己的手写数字图片，并使用训练模型进行识别

#Tips: 手写时，使用picpick截屏白色背景67*67，然后用6px黑色画笔写数字，然后缩放 图像大小 到28*28像素，保存为png。
# 要写到图片正中间，偏离中心可能识别出错。
#
#1.白底黑字的手写数字图片png, 缩放为 28*28 像素图片
#2.读入py，处理：取出rgb一个维度，(255-位点) /255，再转为 一行 784列
#3.使用卷积核和权重矩阵，获得对10个数字的打分，哪个打分高就是哪个数字。

from PIL import Image

def getImage2Matrix(PATH):
    im = Image.open(PATH)
    plt.imshow(im)
    plt.show()

    data=im.getdata()
    data=np.array(data)
    #print(data.shape)

    data=np.reshape(data.T[0], (784, 1)) #只取rgb的一个维度
    print(data.shape)
    input2 = ((255 - np.array(data, dtype=np.uint8)) / 255.0).reshape(1,-1)
    print(input2.shape)
    return input2;
#

PATH ="my_num_pics/pic8.png"
img_mt=getImage2Matrix(PATH)
img_mt.shape


def sayNumberByMatrix(image_mt): #(1,784)
    plt.imshow(image_mt.T.reshape(28,28))
    plt.show()
    
    layer_0=image_mt
    layer_0=layer_0.reshape(layer_0.shape[0],28,28)
    #layer_0.shape

    sects=list()
    for row_start in range(layer_0.shape[1]-kernel_rows+1):
        for col_start in range(layer_0.shape[2]-kernel_cols+1):
            sect=get_image_section(layer_0, row_start, row_start+kernel_rows,  col_start, col_start+kernel_cols)
            sects.append(sect)

    expanded_input=np.concatenate(sects,axis=1)
    es=expanded_input.shape
    flattened_input=expanded_input.reshape(es[0]*es[1], -1)

    kernel_output=flattened_input.dot(kernels)
    layer_1=tanh(kernel_output.reshape(es[0], -1))

    layer_2=np.dot(layer_1, weights_1_2) #验证集输出不再使用softmax
    print(layer_2)
    return np.argmax(layer_2)
#
sayNumberByMatrix(img_mt)


# 一行调用
sayNumberByMatrix(getImage2Matrix("my_num_pics/pic6.png"))






========================================
人脸生成技术
----------------------------------------
http://www.gwylab.com/

http://www.seeprettyface.com/information.html





========================================
深度学习 - 人工卷积神经网络(吴恩达视频课)
----------------------------------------
1. 文本资料
http://www.seeprettyface.com/research_notes.html







========================================
MCMC(一)蒙特卡罗方法及其收敛性判断: Markov Chain Monte Carlo (MCMC) simulations
----------------------------------------
MCMC(一)蒙特卡罗方法 https://www.cnblogs.com/pinard/p/6625739.html
MCMC(二)马尔科夫链
MCMC(三)MCMC采样和M-H采样
MCMC(四)Gibbs采样


1. MCMC概述
从名字我们可以看出，MCMC由两个MC组成，即蒙特卡罗方法（Monte Carlo Simulation，简称MC）和马尔科夫链（Markov Chain ，也简称MC）。要弄懂MCMC的原理我们首先得搞清楚蒙特卡罗方法和马尔科夫链的原理。我们将用三篇来完整学习MCMC。在本篇，我们关注于蒙特卡罗方法。


2. 蒙特卡罗方法引入
蒙特卡罗原来是一个赌场的名称，用它作为名字大概是因为蒙特卡罗方法是一种随机模拟的方法，这很像赌博场里面的扔骰子的过程。最早的蒙特卡罗方法都是为了求解一些不太好求解的求和或者积分问题。比如积分

theta=积分(a,b){f(x)dx}

如果我们很难求解出f(x)的原函数，那么这个积分比较难求解。当然我们可以通过蒙特卡罗方法来模拟求解近似值。如何模拟呢？假设我们函数图像如下图:

图略：x轴a到b，y轴就是曲线的值作为顶部不规则方块的面积。

则一个简单的近似求解方法是在[a,b]之间随机的采样一个点。比如x0,然后用f(x0)代表在[a,b]区间上所有的f(x)的值。那么上面的定积分的近似求解为:
(b-a)*f(x0)

虽然上面的方法可以一定程度上求解出近似的解，但是它隐含了一个假定，即x在[a,b]之间是均匀分布的，而绝大部分情况，x在[a,b]之间不是均匀分布的。如果我们用上面的方法，则模拟求出的结果很可能和真实值相差甚远。　

怎么解决这个问题呢？ 如果我们可以得到x在[a,b]的概率分布函数p(x)，那么我们的定积分求和可以这样进行：
theta=积分(a,b){f(x) dx}=积分(a,b){f(x)/p(x) * p(x) dx} ≈ 1/n *( 累加(i=0, n-1){f(xi)/p(xi)} )

上式最右边的这个形式就是蒙特卡罗方法的一般形式。当然这里是连续函数形式的蒙特卡罗方法，但是在离散时一样成立。

可以看出，最上面我们假设x在[a,b]之间是均匀分布的时候，p(xi)=1/(b−a)，带入我们有概率分布的蒙特卡罗积分的上式，可以得到：
1/n *( 累加(i=0, n-1){f(xi)/ (1/(b-a)) } ) = (b-a)/n * 累加(i=0,n-1){f(xi)}

也就是说，我们最上面的均匀分布也可以作为一般概率分布函数p(x)在均匀分布时候的特例。那么我们现在的问题转到了如何求出x的分布p(x)对应的若干个样本上来。


//todo




3. 概率分布采样


4. 接受-拒绝采样


5. 蒙特卡罗方法小结






refer:


========================================
MCMC(二)马尔科夫链
----------------------------------------
MCMC(一)蒙特卡罗方法
MCMC(二)马尔科夫链 https://www.cnblogs.com/pinard/p/6632399.html
MCMC(三)MCMC采样和M-H采样
MCMC(四)Gibbs采样


马尔科夫链定义本身比较简单，它假设某一时刻状态转移的概率只依赖于它的前一个状态。举个形象的比喻，假如每天的天气是一个状态的话，那个今天是不是晴天只依赖于昨天的天气，而和前天的天气没有任何关系。

当然这么说可能有些武断，但是这样做可以大大简化模型的复杂度，因此马尔科夫链在很多时间序列模型中得到广泛的应用，比如循环神经网络RNN，隐式马尔科夫模型HMM等，当然MCMC也需要它。


这个马尔科夫链是表示股市模型的，共有三种状态：牛市（Bull market）, 熊市（Bear market）和横盘（Stagnant market）。每一个状态都以一定的概率转化到下一个状态。比如，牛市以0.025的概率转化到横盘的状态。这个状态概率转化图可以以矩阵的形式表示。

如果我们定义矩阵阵P某一位置P(i,j)的值为P(j|i),即从状态i转化到状态j的概率，并定义牛市为状态0， 熊市为状态1, 横盘为状态2. 这样我们得到了马尔科夫链模型的状态转移矩阵为：
P={0.90,0.15,0.25; 0.075,0.8,0.25; 0.025,0.05,0.5} //竖列1;竖列2;竖列3;



也就是说我们的马尔科夫链模型的状态转移矩阵收敛到的稳定概率分布与我们的初始状态概率分布无关。这是一个非常好的性质，也就是说，如果我们得到了这个稳定概率分布对应的马尔科夫链模型的状态转移矩阵，则我们可以用任意的概率分布样本开始，带入马尔科夫链模型的状态转移矩阵，这样经过一些序列的转换，最终就可以得到符合对应稳定概率分布的样本。

同时，对于一个确定的状态转移矩阵P，它的n次幂Pn在当n大于一定的值的时候也可以发现是确定的，我们还是以上面的例子为例，计算代码如下：

matrix = np.matrix([[0.9,0.075,0.025],[0.15,0.8,0.05],[0.25,0.25,0.5]], dtype=float)
for i in range(10):
    matrix = matrix*matrix
    print "Current round:" , i+1
    print matrix
#





========================================
t-SNE原理与推导
----------------------------------------
t-SNE(t-distributed stochastic neighbor embedding)是用于降维的一种机器学习算法，由 Laurens van der Maaten 和 Geoffrey Hinton在08年提出。t-SNE 作为一种非线性降维算法，常用于流行学习(manifold learning)的降维过程中并与LLE进行类比，非常适用于高维数据降维到2维或者3维，便于进行可视化。

t-SNE是由SNE(Stochastic Neighbor Embedding, SNE; Hinton and Roweis, 2002)发展而来。首先介绍SNE的基本原理，之后再扩展到t-SNE。最后是t-SNE的实现以及一些优化。




>> 详见 R/机器学习: PCA及t-SNE;






ref:
https://blog.csdn.net/scott198510/article/details/76099700
https://yifdu.github.io/2018/11/12/t-SNE/
https://www.biaodianfu.com/t-sne.html



========================================
教学大纲: Introduction to Machine Learning Course
----------------------------------------
TIMELINE: Approx. 10 Weeks
SKILL LEVEL: Intermediate


1. Introduction to Machine Learning Course
Machine Learning is a first-class ticket to the most exciting careers in data analysis today. As data sources proliferate along with the computing power to process them, going straight to the data is one of the most straightforward ways to quickly gain insights and make predictions.

计算机和统计学结合预测。
Machine learning brings together computer science and statistics to harness that predictive power. It’s a must-have skill for all aspiring data analysts and data scientists, or anyone else who wants to wrestle all that raw data into refined trends and predictions.

This is a class that will teach you the end-to-end process of investigating data through a machine learning lens. It will teach you how to extract and identify useful features that best represent your data, a few of the most important machine learning algorithms, and how to evaluate the performance of your machine learning algorithms.


2. 大纲
LESSON 1: Welcome to Machine Learning
Learn what Machine Learning is and meet Sebastian Thrun(代课老师人名)!
Find out where Machine Learning is applied in Technology and Science.


LESSON 2: Naive Bayes
Use Naive Bayes with scikit learn in python.
Splitting data between training sets and testing sets with scikit learn.
Calculate the posterior probability and the prior probability of simple distributions.


LESSON 3: Support Vector Machines
Learn the simple intuition behind Support Vector Machines.
Implement an SVM classifier in SKLearn/scikit-learn.
Identify how to choose the right kernel for your SVM and learn about RBF and Linear Kernels.


LESSON 4: Decision Trees
Code your own decision tree in python.
Learn the formulas for entropy and information gain and how to calculate them.
Implement a mini project where you identify the authors in a body of emails using a decision tree in Python.


LESSON 5: Choose your own Algorithm
Decide how to pick the right Machine Learning Algorithm among K-Means, Adaboost, and Decision Trees.


LESSON 6: Datasets and Questions
Apply your Machine Learning knowledge by looking for patterns in the Enron Email Dataset.
You'll be investigating one of the biggest frauds in American history!


LESSON 7: Regressions
Understand how continuous supervised learning is different from discrete learning.
Code a Linear Regression in Python with scikit-learn.
Understand different error metrics such as SSE, and R Squared in the context of Linear Regressions.


LESSON 8: Outliers
Remove outliers to improve the quality of your linear regression predictions.
Apply your learning in a mini project where you remove the residuals on a real dataset and reimplement your regressor.
Apply your same understanding of outliers and residuals on the Enron Email Corpus.


LESSON 9: Clustering
Identify the difference between Unsupervised Learning and Supervised Learning.
Implement K-Means in Python and Scikit Learn to find the center of clusters.
Apply your knowledge on the Enron Finance Data to find clusters in a real dataset.


LESSON 10: Feature Scaling
Understand how to preprocess data with feature scaling to improve your algorithms.
Use a min mx scaler in sklearn.












ref: https://www.udacity.com/course/intro-to-machine-learning--ud120




========================================
分割出训练集和测试集
----------------------------------------
关键是要选出来合适的整数行号。

1.用python的random
import random
def getRandomIndex(n, x):
	# 索引范围为[0, n), 随机选x个不重复
    index = random.sample(range(n), x)
    return index

2.用numpy.random.choice
import numpy as np
def getRandomIndex(n, x):
	# 索引范围为[0, n)，随机选x个不重复，注意replace=False才是不重复，replace=True则有可能重复
    index = np.random.choice(np.arange(n), size=x, replace=False)
    return index
#


3.已经获取到测试集的索引了，那么得将其余的索引单独做一个数组作为训练集的索引，做法如下

import numpy as np
# 先根据上面的函数获取test_index
test_index = np.array(getRandomIndex(n, x))
# 把test_index从总的index中减去就得到了train_index
train_index = np.delete(np.arange(n), test_index)



4. 写成一个函数
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 读取数据
iris=pd.read_csv('../iris_data/iris.csv', index_col=0) # 将第一列作为行名字
iris.head()

# 分割
def splitData(df, test_ratio):
    # 索引范围为[0, n), 随机选x个不重复
    n=df.shape[0]
    x=round(n*test_ratio)
    index = np.random.choice(np.arange(n), size=x, replace=False)
    #
    test_index = np.array(index)
    train_index = np.delete(np.arange(n), test_index)
    return df.iloc[train_index,],df.iloc[test_index,]
np.random.seed(1)
train_set, test_set=splitData(iris, 0.2)
print(train_set.shape)
print(test_set.shape)





ref:
https://blog.csdn.net/qq_32623363/article/details/104180152




========================================
安装包 scikit-learn
----------------------------------------
1. 
pip install -U scikit-learn
pip install --user scikit-learn

pip list # (0.23.2)

文档
https://scikit-learn.org/stable/



2. 实例

X = [[0], [1], [2], [3]] # 已知数据点
y = [0, 0, 1, 1] # 已知点的标签

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X, y)
# KNeighborsClassifier(...)
print(neigh.predict([[1.1]])) #[0]

print(neigh.predict_proba([[0.9]]))
#[[0.66666667 0.33333333]]


如果导入包报错: ImportError: cannot import name 'MultiOutputMixin' from 'sklearn.base'
则需要重启一下python session，就正常了。







========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


