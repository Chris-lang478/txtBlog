MachineLearning-01


TensorFlow https://www.tensorflow.org/tutorials/

Darcula主题

========================================
概率
----------------------------------------
1. https://github.com/DawnEve/ML_MachineLearning

2. 公众号： 
机器学习算法清单！附Python和R代码 <数据派THU  3月8日





========================================
参考资源: 知名机器学习与AI课程
----------------------------------------
1.课程
fast.ai(http://fast.ai/) ：它针对程序员提供了两个很不错的关于深度学习的课程，以及一个关于可计算线性代数的课程。是开始编写神经网络代码的好地方，随着课程深度的延伸，当你学到更多理论的时候，你可以尽快用代码实现。

neuralnetworksanddeeplearning.com（http://neuralnetworksanddeeplearning.com/chap1.html）：一本关于基本知识的很好的在线书籍。关于神经网络背后的理论。作者以一种很好的方式解释了你需要知道的数学知识。它也提供并解释了一些不使用任何深度学习框架从零开始编写神经网络架构的代码。

Andrew Ng 的深度学习课程（https://www.coursera.org/specializations/deep-learning）：coursera 上的课程，也是有关学习神经网络的。以非常简单的神经网络例子开始，逐步到卷积神经网络以及更多。

3Blue1Brown（https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw）：YouTube 上也有一些能够帮助你理解神经网络和线性代数的很好的视频。它们展示了很棒的可视化形式，以及以非常直觉的方式去理解数学和神经网络。

Stanford CS231 课程（http://cs231n.stanford.edu/）：这是关于用于视觉识别的卷积神经网络的课堂，可以学到很多关于深度学习和卷积神经网络的具体内容。



2.书
https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/model_optimization.html




========================================
1.SVM, Support Vector Machine
----------------------------------------
1.Understanding Support Vector Machine algorithm from examples (along with code)
https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/

中文翻译参考 公众号：关于支持向量机相关知识汇集，by 无人机  2016-08-30



========================================
2. logistics 回归
----------------------------------------

1. 【典藏】Logistic 回归：从入门到进阶
http://www.360doc.com/content/15/1024/07/22609018_507952382.shtml

【独家】一文读懂回归分析
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml



========================================
一文读懂回归分析:概述Cox回归、岭回归、Lasso回归、ElasticNet 回归
----------------------------------------
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml

生物学中的基因很多是有共线性的，所以，后三种方法经常使用。
回归正则化方法（套索，岭和ElasticNet）在高维数据和数据集变量之间存在多重共线性的情况下运行良好。

3）Cox回归
Cox回归的因变量就有些特殊，它不经考虑结果而且考虑结果出现时间的回归模型。它用一个或多个自变量预测一个事件（死亡、失败或旧病复发）发生的时间。Cox回归的主要作用发现风险因素并用于探讨风险因素的强弱。但它的因变量必须同时有2个，一个代表状态，必须是分类变量，一个代表时间，应该是连续变量。只有同时具有这两个变量，才能用Cox回归分析。Cox回归主要用于生存资料的分析，生存资料至少有两个结局变量，一是死亡状态，是活着还是死亡；二是死亡时间，如果死亡，什么时间死亡？如果活着，从开始观察到结束时有多久了？所以有了这两个变量，就可以考虑用Cox回归分析。

9）岭回归
当数据之间存在多重共线性（自变量高度相关）时，就需要使用岭回归分析。在存在多重共线性时，尽管最小二乘法（OLS）测得的估计值不存在偏差，它们的方差也会很大，从而使得观测值与真实值相差甚远。岭回归通过给回归估计值添加一个偏差值，来降低标准误差。

岭回归要点：
1）除常数项以外，岭回归的假设与最小二乘回归相同；
2） 它收缩了相关系数的值，但没有达到零，这表明它不具有特征选择功能；
3）这是一个正则化方法，并且使用的是 L2 正则化。


13）套索回归 LASSO
与岭回归类似，套索也会对回归系数的绝对值添加一个罚值。此外，它能降低偏差并提高线性回归模型的精度。看看下面的等式：

套索回归要点：
1）除常数项以外，这种回归的假设与最小二乘回归类似；
2）它将收缩系数缩减至零（等于零），这确实有助于特征选择；
3）这是一个正则化方法，使用的是 L1 正则化；
4）如果一组预测因子是高度相关的，套索回归会选出其中一个因子并且将其它因子收缩为零。


14）ElasticNet 回归
ElasticNet 回归是套索回归和岭回归的组合体。它会事先使用 L1 和 L2 作为正则化矩阵进行训练。当存在多个相关的特征时，Elastic-net 会很有用。岭回归一般会随机选择其中一个特征，而 Elastic-net 则会选择其中的两个。同时包含岭回归和套索回归的一个切实的优点是，ElasticNet 回归可以在循环状态下继承岭回归的一些稳定性。

ElasticNet 回归要点：
1）在高度相关变量的情况下，它会产生群体效应；
2）选择变量的数目没有限制；
3）它可以承受双重收缩。







========================================
用R对数据进行随机抽样
----------------------------------------
#去掉一类，变成两类
x=iris[which(iris$Species!="virginica"),];
x$Species=factor(x$Species)
str(x);dim(x)

#生成抽样序列
set.seed(100)
ind=sample(2,nrow(x),replace=TRUE,prob=c(0.8,0.2));ind

#取80%作为训练集，20为测试集
x_train=x[ind==1,];dim(x_train)
x_test=x[ind==2,];dim(x_test) #去掉标签



========================================
Self Organizing Maps (SOM): 一种基于神经网络的聚类算法
----------------------------------------
自组织映射神经网络， 即Self Organizing Maps (SOM)， 可以对数据进行无监督学习聚类。它的思想很简单，本质上是一种只有输入层--隐藏层的神经网络。隐藏层中的一个节点代表一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在隐藏层中找到一个和它最匹配的节点，称为它的激活节点，也叫“winning neuron”。 紧接着用随机梯度下降法更新激活节点的参数。同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。

所以，SOM的一个特点是，隐藏层的节点是有拓扑关系的。这个拓扑关系需要我们确定，如果想要一维的模型，那么隐藏节点依次连成一条线；如果想要二维的拓扑关系，那么就行成一个平面，如下图所示（也叫Kohonen Network）：

...
既然隐藏层是有拓扑关系的，所以我们也可以说，SOM可以把任意维度的输入离散化到一维或者二维(更高维度的不常见)的离散空间上。 Computation layer里面的节点与Input layer的节点是全连接的。

拓扑关系确定后，开始计算过程，大体分成几个部分：
1） 初始化：每个节点随机初始化自己的参数。每个节点的参数个数与Input的维度相同。
2）对于每一个输入数据，找到与它最相配的节点。假设输入时D维的， 即 X={x_i, i=1,...,D}，那么判别函数可以为欧几里得距离：
3)找到激活节点I(x)之后，我们也希望更新和它临近的节点。令S_ij表示节点i和j之间的距离，对于I(x)临近的节点，分配给它们一个更新权重：
简单地说，临近的节点根据距离的远近，更新程度要打折扣。
4）接着就是更新节点的参数了。按照梯度下降法更新：

迭代，直到收敛。


## 与K-Means的比较
同样是无监督的聚类方法，SOM与K-Means有什么不同呢？
（1）K-Means需要事先定下类的个数，也就是K的值。 SOM则不用，隐藏层中的某些节点可以没有任何输入数据属于它。所以，K-Means受初始化的影响要比较大。
（2）K-means为每个输入数据找到一个最相似的类后，只更新这个类的参数。SOM则会更新临近的节点。所以K-mean受noise data的影响比较大，SOM的准确性可能会比k-means低（因为也更新了临近节点）。
（3） SOM的可视化比较好。优雅的拓扑关系图 。


参考文献：http://www.cs.bham.ac.uk/~jxb/NN/l16.pdf
https://www.cnblogs.com/sylvanas2012/p/5117056.html



========================================
手写体识别与Tensorflow (没入门就放弃)
----------------------------------------
1.如同所有语言的hello world一样，手写体识别就相当于深度学习里的hello world。

TensorFlow是当前最流行的机器学习框架，有了它，开发人工智能程序就像Java编程一样简单。




2. 训练集和测试集

MNIST 数据集已经是一个被”嚼烂”了的数据集, 很多教程都会对它”下手”, 几乎成为一个 “典范”. 不过有些人可能对它还不是很了解, 下面来介绍一下.

MNIST 数据集可在 http://yann.lecun.com/exdb/mnist/ 获取, 它包含了四个部分:

       Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)  
       Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签) 
       Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本) 
       Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)

MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST). 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据.

tensorflow提供一个input_data.py文件，专门用于下载mnist数据，我们直接调用就可以了，代码如下：

import tensorflow.examples.tutorials.mnist.input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
执行完成后，会在当前目录下新建一个文件夹MNIST_data

input_data文件会调用一个maybe_download函数，确保数据下载成功。这个函数还会判断数据是否已经下载，如果已经下载好了，就不再重复下载。


(1)### 报错：
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

### 环境：
Ubuntu 1804 64bit, 
$ python -V
Python 3.6.3 :: Anaconda, Inc.

$ pip list | grep flow
tensorflow-gpu                     1.10.1


2)重新安装包：
$ pip install --upgrade pip
$ pip -V
pip 19.2.1 from /home/wangjl/anaconda3/lib/python3.6/site-packages/pip (python 3.6)


#ERROR: Cannot uninstall 'wrapt'问题  https://www.cnblogs.com/xiaowei2092/p/11025155.html
$ pip install -U --ignore-installed wrapt enum34 simplejson netaddr

$ pip search tensorflow                                                                                                                           
tensorflow (1.14.0)     - TensorFlow is an open source machine learning framework for everyone.
$ pip install tensorflow  
安装成功 Successfully installed tensorflow-1.14.0

升级
$ pip install --upgrade tensorflow-gpu


3)
$ pip install --upgrade setuptools


(2) 再次尝试执行两行下载代码
还是那个报错 
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

1) 需要安装cuda？
CUDA（Compute Unified Device Architecture），是显卡厂商NVIDIA推出的运算平台。


放弃！！ 下次，换个环境继续： https://colab.research.google.com



https://www.cnblogs.com/skyme/p/8595642.html



========================================
MCMC(一)蒙特卡罗方法及其收敛性判断: Markov Chain Monte Carlo (MCMC) simulations
----------------------------------------
MCMC(一)蒙特卡罗方法 https://www.cnblogs.com/pinard/p/6625739.html
MCMC(二)马尔科夫链
MCMC(三)MCMC采样和M-H采样
MCMC(四)Gibbs采样


1. MCMC概述
从名字我们可以看出，MCMC由两个MC组成，即蒙特卡罗方法（Monte Carlo Simulation，简称MC）和马尔科夫链（Markov Chain ，也简称MC）。要弄懂MCMC的原理我们首先得搞清楚蒙特卡罗方法和马尔科夫链的原理。我们将用三篇来完整学习MCMC。在本篇，我们关注于蒙特卡罗方法。


2. 蒙特卡罗方法引入
蒙特卡罗原来是一个赌场的名称，用它作为名字大概是因为蒙特卡罗方法是一种随机模拟的方法，这很像赌博场里面的扔骰子的过程。最早的蒙特卡罗方法都是为了求解一些不太好求解的求和或者积分问题。比如积分

theta=积分(a,b){f(x)dx}

如果我们很难求解出f(x)的原函数，那么这个积分比较难求解。当然我们可以通过蒙特卡罗方法来模拟求解近似值。如何模拟呢？假设我们函数图像如下图:

图略：x轴a到b，y轴就是曲线的值作为顶部不规则方块的面积。

则一个简单的近似求解方法是在[a,b]之间随机的采样一个点。比如x0,然后用f(x0)代表在[a,b]区间上所有的f(x)的值。那么上面的定积分的近似求解为:
(b-a)*f(x0)

虽然上面的方法可以一定程度上求解出近似的解，但是它隐含了一个假定，即x在[a,b]之间是均匀分布的，而绝大部分情况，x在[a,b]之间不是均匀分布的。如果我们用上面的方法，则模拟求出的结果很可能和真实值相差甚远。　

怎么解决这个问题呢？ 如果我们可以得到x在[a,b]的概率分布函数p(x)，那么我们的定积分求和可以这样进行：
theta=积分(a,b){f(x) dx}=积分(a,b){f(x)/p(x) * p(x) dx} ≈ 1/n *( 累加(i=0, n-1){f(xi)/p(xi)} )

上式最右边的这个形式就是蒙特卡罗方法的一般形式。当然这里是连续函数形式的蒙特卡罗方法，但是在离散时一样成立。

可以看出，最上面我们假设x在[a,b]之间是均匀分布的时候，p(xi)=1/(b−a)，带入我们有概率分布的蒙特卡罗积分的上式，可以得到：
1/n *( 累加(i=0, n-1){f(xi)/ (1/(b-a)) } ) = (b-a)/n * 累加(i=0,n-1){f(xi)}

也就是说，我们最上面的均匀分布也可以作为一般概率分布函数p(x)在均匀分布时候的特例。那么我们现在的问题转到了如何求出x的分布p(x)对应的若干个样本上来。


//todo




3. 概率分布采样


4. 接受-拒绝采样


5. 蒙特卡罗方法小结






refer:


========================================
MCMC(二)马尔科夫链
----------------------------------------
MCMC(一)蒙特卡罗方法
MCMC(二)马尔科夫链 https://www.cnblogs.com/pinard/p/6632399.html
MCMC(三)MCMC采样和M-H采样
MCMC(四)Gibbs采样


马尔科夫链定义本身比较简单，它假设某一时刻状态转移的概率只依赖于它的前一个状态。举个形象的比喻，假如每天的天气是一个状态的话，那个今天是不是晴天只依赖于昨天的天气，而和前天的天气没有任何关系。

当然这么说可能有些武断，但是这样做可以大大简化模型的复杂度，因此马尔科夫链在很多时间序列模型中得到广泛的应用，比如循环神经网络RNN，隐式马尔科夫模型HMM等，当然MCMC也需要它。


这个马尔科夫链是表示股市模型的，共有三种状态：牛市（Bull market）, 熊市（Bear market）和横盘（Stagnant market）。每一个状态都以一定的概率转化到下一个状态。比如，牛市以0.025的概率转化到横盘的状态。这个状态概率转化图可以以矩阵的形式表示。

如果我们定义矩阵阵P某一位置P(i,j)的值为P(j|i),即从状态i转化到状态j的概率，并定义牛市为状态0， 熊市为状态1, 横盘为状态2. 这样我们得到了马尔科夫链模型的状态转移矩阵为：
P={0.90,0.15,0.25; 0.075,0.8,0.25; 0.025,0.05,0.5} //竖列1;竖列2;竖列3;



也就是说我们的马尔科夫链模型的状态转移矩阵收敛到的稳定概率分布与我们的初始状态概率分布无关。这是一个非常好的性质，也就是说，如果我们得到了这个稳定概率分布对应的马尔科夫链模型的状态转移矩阵，则我们可以用任意的概率分布样本开始，带入马尔科夫链模型的状态转移矩阵，这样经过一些序列的转换，最终就可以得到符合对应稳定概率分布的样本。

同时，对于一个确定的状态转移矩阵P，它的n次幂Pn在当n大于一定的值的时候也可以发现是确定的，我们还是以上面的例子为例，计算代码如下：

matrix = np.matrix([[0.9,0.075,0.025],[0.15,0.8,0.05],[0.25,0.25,0.5]], dtype=float)
for i in range(10):
    matrix = matrix*matrix
    print "Current round:" , i+1
    print matrix
#





========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


